{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a75bb6ad",
      "metadata": {},
      "source": [
        "# **Soft voting**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "900b0225",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enviroment\n",
        "isColab = False\n",
        "colab_dir = \"/gdrive/My Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "\n",
        "isKaggle = False\n",
        "isWsl = True\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4cb00e",
      "metadata": {},
      "source": [
        "## **Loading Enviroment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "e09b8c7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esecuzione su WSL. Directory corrente impostata a: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
            "Changed directory to: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
            "Dataset directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset\n",
            "Train set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_data\n",
            "Test set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/test_data\n",
            "Label file: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_labels.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Directory di default\n",
        "current_dir = os.getcwd()   \n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive # type: ignore\n",
        "    drive.mount(\"/gdrive\")\n",
        "    current_dir = colab_dir\n",
        "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
        "    %cd $current_dir\n",
        "elif isKaggle:\n",
        "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
        "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
        "    current_dir = kaggle_work_dir\n",
        "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
        "    os.chdir(current_dir)\n",
        "elif isWsl:\n",
        "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "else:\n",
        "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
        "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "\n",
        "print(f\"Changed directory to: {current_dir}\")\n",
        "\n",
        "# Define absolute paths\n",
        "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
        "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
        "\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Train set directory: {train_set_dir}\")\n",
        "print(f\"Test set directory: {test_set_dir}\")\n",
        "print(f\"Label file: {label_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e0fdc8",
      "metadata": {},
      "source": [
        "## **Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "be49326d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49326d",
        "outputId": "59e423d7-35ea-4fc1-933c-affb303ee9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchview in /home/berto/an2dl-challenge-2/.venv/lib/python3.12/site-packages (0.2.7)\n",
            "Requirement already satisfied: graphviz in /home/berto/an2dl-challenge-2/.venv/lib/python3.12/site-packages (from torchview) (0.21)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "PyTorch version: 2.9.1+cu130\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "%pip install torchview\n",
        "from torchview import draw_graph\n",
        "\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "if isColab or isKaggle:\n",
        "    !pkill -f tensorboard \n",
        "    !mkdir -p models\n",
        "    print(\"Killed existing TensorBoard instances and created models directory.\") \n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)  \n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import cv2\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.gridspec as gridspec\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import ndimage\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcbf293",
      "metadata": {},
      "source": [
        "### **Preparing Dataset for colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "c8f17001",
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    drive_dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "    local_dataset_dir = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(local_dataset_dir):\n",
        "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
        "        try:\n",
        "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
        "            print(\"Copy complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying dataset: {e}\")\n",
        "            print(\"Falling back to Drive dataset (slow).\")\n",
        "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
        "            dataset_dir = drive_dataset_dir\n",
        "    else:\n",
        "        print(\"Dataset already copied to local runtime.\")\n",
        "\n",
        "    # If copy succeeded (or already existed), use local path\n",
        "    if os.path.exists(local_dataset_dir):\n",
        "        dataset_dir = local_dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfe95f2",
      "metadata": {},
      "source": [
        "## â³ **Data Loading**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5fc4301c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loader parameters\n",
        "APPLY_MASK = False\n",
        "BATCH_SIZE = 32\n",
        "LOADER_SHUFFLE = False\n",
        "\n",
        "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZATION_STD = [0.229, 0.224, 0.225]\n",
        "IMG_PADDING = 5\n",
        "\n",
        "IMG_RESIZE = (224, 224)\n",
        "INPUT_SHAPE = (3, *IMG_RESIZE)\n",
        "\n",
        "TEST_SET_SIZE = 0.0\n",
        "VAL_SET_SIZE = 0.2\n",
        "TRAIN_SET_SIZE = 1.0 - TEST_SET_SIZE - VAL_SET_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e112e14",
      "metadata": {},
      "source": [
        "### **Definitions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "7d945105",
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_IGNORE = [\n",
        "    \"img_0001.png\",\n",
        "    \"img_0005.png\",\n",
        "    \"img_0008.png\",\n",
        "    \"img_0012.png\",\n",
        "    \"img_0018.png\",\n",
        "    \"img_0020.png\",\n",
        "    \"img_0022.png\",\n",
        "    \"img_0027.png\",\n",
        "    \"img_0028.png\",\n",
        "    \"img_0036.png\",\n",
        "    \"img_0044.png\",\n",
        "    \"img_0047.png\",\n",
        "    \"img_0048.png\",\n",
        "    \"img_0052.png\",\n",
        "    \"img_0062.png\",\n",
        "    \"img_0078.png\",\n",
        "    \"img_0085.png\",\n",
        "    \"img_0090.png\",\n",
        "    \"img_0094.png\",\n",
        "    \"img_0095.png\",\n",
        "    \"img_0126.png\",\n",
        "    \"img_0129.png\",\n",
        "    \"img_0130.png\",\n",
        "    \"img_0133.png\",\n",
        "    \"img_0136.png\",\n",
        "    \"img_0138.png\",\n",
        "    \"img_0148.png\",\n",
        "    \"img_0150.png\",\n",
        "    \"img_0155.png\",\n",
        "    \"img_0159.png\",\n",
        "    \"img_0161.png\",\n",
        "    \"img_0175.png\",\n",
        "    \"img_0178.png\",\n",
        "    \"img_0179.png\",\n",
        "    \"img_0180.png\",\n",
        "    \"img_0184.png\",\n",
        "    \"img_0187.png\",\n",
        "    \"img_0189.png\",\n",
        "    \"img_0193.png\",\n",
        "    \"img_0196.png\",\n",
        "    \"img_0222.png\",\n",
        "    \"img_0251.png\",\n",
        "    \"img_0254.png\",\n",
        "    \"img_0263.png\",\n",
        "    \"img_0268.png\",\n",
        "    \"img_0286.png\",\n",
        "    \"img_0293.png\",\n",
        "    \"img_0313.png\",\n",
        "    \"img_0319.png\",\n",
        "    \"img_0333.png\",\n",
        "    \"img_0342.png\",\n",
        "    \"img_0344.png\",\n",
        "    \"img_0346.png\",\n",
        "    \"img_0355.png\",\n",
        "    \"img_0368.png\",\n",
        "    \"img_0371.png\",\n",
        "    \"img_0376.png\",\n",
        "    \"img_0380.png\",\n",
        "    \"img_0390.png\",\n",
        "    \"img_0393.png\",\n",
        "    \"img_0407.png\",\n",
        "    \"img_0410.png\",\n",
        "    \"img_0415.png\",\n",
        "    \"img_0424.png\",\n",
        "    \"img_0443.png\",\n",
        "    \"img_0453.png\",\n",
        "    \"img_0459.png\",\n",
        "    \"img_0463.png\",\n",
        "    \"img_0486.png\",\n",
        "    \"img_0497.png\",\n",
        "    \"img_0498.png\",\n",
        "    \"img_0499.png\",\n",
        "    \"img_0509.png\",\n",
        "    \"img_0521.png\",\n",
        "    \"img_0530.png\",\n",
        "    \"img_0531.png\",\n",
        "    \"img_0533.png\",\n",
        "    \"img_0537.png\",\n",
        "    \"img_0540.png\",\n",
        "    \"img_0544.png\",\n",
        "    \"img_0547.png\",\n",
        "    \"img_0557.png\",\n",
        "    \"img_0558.png\",\n",
        "    \"img_0560.png\",\n",
        "    \"img_0565.png\",\n",
        "    \"img_0567.png\",\n",
        "    \"img_0572.png\",\n",
        "    \"img_0578.png\",\n",
        "    \"img_0580.png\",\n",
        "    \"img_0586.png\",\n",
        "    \"img_0602.png\",\n",
        "    \"img_0603.png\",\n",
        "    \"img_0607.png\",\n",
        "    \"img_0609.png\",\n",
        "    \"img_0614.png\",\n",
        "    \"img_0620.png\",\n",
        "    \"img_0623.png\",\n",
        "    \"img_0629.png\",\n",
        "    \"img_0635.png\",\n",
        "    \"img_0639.png\",\n",
        "    \"img_0643.png\",\n",
        "    \"img_0644.png\",\n",
        "    \"img_0645.png\",\n",
        "    \"img_0646.png\",\n",
        "    \"img_0656.png\",\n",
        "    \"img_0657.png\",\n",
        "    \"img_0658.png\",\n",
        "    \"img_0670.png\",\n",
        "    \"img_0673.png\",\n",
        "    \"img_0675.png\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "7afdb353",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 4\n"
          ]
        }
      ],
      "source": [
        "# Load the full dataframe\n",
        "full_df = pd.read_csv(label_file)\n",
        "\n",
        "# Remove cursed images\n",
        "full_df = full_df[~full_df[\"sample_index\"].isin(SAMPLES_TO_IGNORE)].reset_index(\n",
        "    drop=True\n",
        ")\n",
        "\n",
        "# Label mapping\n",
        "class_names = sorted(full_df[\"label\"].unique())\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "full_df[\"label_index\"] = full_df[\"label\"].map(label_to_index)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "9e13a32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last=False):\n",
        "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(6, cpu_cores))\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,\n",
        "        persistent_workers=isWsl,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c536e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "\n",
        "class MaskedFixedTileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset class that extracts fixed-size patches from the center of tissue masks\n",
        "    to preserve biological scale (magnification), rather than resizing variable crops.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataframe, img_dir, transform=None, target_size=(224, 224), debug_max=None\n",
        "    ):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.img_dir = img_dir\n",
        "        self.target_size = target_size\n",
        "\n",
        "        # Handling inference mode (no labels) vs training mode\n",
        "        self.is_inference_mode = False\n",
        "        if dataframe is None or \"label_index\" not in dataframe.columns:\n",
        "            self.is_inference_mode = True\n",
        "            if dataframe is None:\n",
        "                # If just a directory, list images\n",
        "                img_names = sorted(\n",
        "                    [f for f in os.listdir(img_dir) if f.startswith(\"img_\")]\n",
        "                )\n",
        "            else:\n",
        "                img_names = dataframe[\"sample_index\"].tolist()\n",
        "            iterator = zip(img_names, [-1] * len(img_names))\n",
        "            total_items = len(img_names)\n",
        "        else:\n",
        "            iterator = zip(dataframe[\"sample_index\"], dataframe[\"label_index\"])\n",
        "            total_items = len(dataframe)\n",
        "\n",
        "        print(\n",
        "            f\"Processing {total_items} images to extract fixed-size {target_size} tiles...\"\n",
        "        )\n",
        "\n",
        "        count = 0\n",
        "        for img_name, label in tqdm(iterator, total=total_items):\n",
        "            if debug_max and count >= debug_max:\n",
        "                break\n",
        "            self._process_and_extract(img_name, label)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Extraction complete. Total patches: {len(self.samples)}\")\n",
        "\n",
        "    def _process_and_extract(self, img_name, label):\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask = Image.open(mask_path).convert(\"L\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load {img_name}: {e}\")\n",
        "            return\n",
        "\n",
        "        img_w, img_h = image.size\n",
        "        # Create binary mask for component labeling\n",
        "        mask_arr = np.array(mask) > 0\n",
        "\n",
        "        # Label connected components (blobs) in the mask\n",
        "        labeled_mask, n_components = ndimage.label(mask_arr) # type: ignore\n",
        "\n",
        "        for cid in range(1, n_components + 1):\n",
        "            # Extract coordinates of the current blob\n",
        "            ys, xs = np.where(labeled_mask == cid)\n",
        "\n",
        "            # Filter out very small noise artifacts (< 50 pixels)\n",
        "            if len(xs) < 50:\n",
        "                continue\n",
        "\n",
        "            # Calculate the centroid (center of mass) of the blob\n",
        "            cy, cx = int(np.mean(ys)), int(np.mean(xs))\n",
        "\n",
        "            # Define the fixed-size crop window around the centroid\n",
        "            th, tw = self.target_size\n",
        "            half_h, half_w = th // 2, tw // 2\n",
        "\n",
        "            y1 = cy - half_h\n",
        "            y2 = cy + half_h\n",
        "            x1 = cx - half_w\n",
        "            x2 = cx + half_w\n",
        "\n",
        "            # Handle Edge Cases: Calculate intersection with the actual image\n",
        "            img_y1, img_y2 = max(0, y1), min(img_h, y2)\n",
        "            img_x1, img_x2 = max(0, x1), min(img_w, x2)\n",
        "\n",
        "            # Extract the valid region from the image\n",
        "            patch_crop = image.crop((img_x1, img_y1, img_x2, img_y2))\n",
        "\n",
        "            # Calculate required padding if the crop extended beyond image bounds\n",
        "            pad_left = max(0, -x1)\n",
        "            pad_top = max(0, -y1)\n",
        "            pad_right = max(0, x2 - img_w)\n",
        "            pad_bottom = max(0, y2 - img_h)\n",
        "\n",
        "            # If padding is needed, pad with white (255) which is standard background in histology\n",
        "            if pad_left > 0 or pad_top > 0 or pad_right > 0 or pad_bottom > 0:\n",
        "                patch = ImageOps.expand(\n",
        "                    patch_crop,\n",
        "                    border=(pad_left, pad_top, pad_right, pad_bottom),\n",
        "                    fill=255,\n",
        "                )\n",
        "            else:\n",
        "                patch = patch_crop\n",
        "\n",
        "            # Ensure precise size match (e.g., if rounding errors occurred)\n",
        "            if patch.size != self.target_size:\n",
        "                patch = patch.resize(self.target_size, Image.BICUBIC) # type: ignore\n",
        "\n",
        "            # Store in RAM (Efficient for ~2k images yielding ~10k-20k patches)\n",
        "            self.samples.append(\n",
        "                {\"patch\": np.array(patch), \"label\": label, \"parent\": img_name}\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        img = Image.fromarray(item[\"patch\"])\n",
        "        label = item[\"label\"]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label, item[\"parent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147c9151",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_dataset_stats(dataset_class, dataframe, img_dir):\n",
        "    \"\"\"\n",
        "    Computes channel-wise Mean and Std on the dataset without any normalization applied.\n",
        "    \"\"\"\n",
        "    print(\"Computing dataset Mean and Std (this may take a moment)...\")\n",
        "\n",
        "    # define a simple transform that only converts to tensor\n",
        "    basic_transform = transforms.Compose(\n",
        "        [transforms.Resize(IMG_RESIZE), transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    # Instantiate dataset temporarily\n",
        "    temp_ds = dataset_class(dataframe, img_dir, transform=basic_transform)\n",
        "    loader = make_loader(temp_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    nb_samples = 0.0\n",
        "\n",
        "    for data, _, _ in tqdm(loader):\n",
        "        batch_samples = data.size(0)\n",
        "        # Flatten H and W to calculate stats per channel\n",
        "        data = data.view(batch_samples, data.size(1), -1)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        nb_samples += batch_samples\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    print(f\"\\nDONE. Copy these values into your config:\")\n",
        "    print(f\"NEW_MEAN = {mean.tolist()}\") # type: ignore\n",
        "    print(f\"NEW_STD = {std.tolist()}\") # type: ignore\n",
        "    return mean.tolist(), std.tolist()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "f6cadb5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating stats on Training Data...\n",
            "Computing dataset Mean and Std (this may take a moment)...\n",
            "Processing 581 images to extract fixed-size (224, 224) tiles...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18f73375ef0948f09e475ccc5be5e03f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/581 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 4955\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67ec254ad7f4403683678297d17dcd2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/155 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DONE. Copy these values into your config:\n",
            "NEW_MEAN = [0.6673933863639832, 0.6174471974372864, 0.6541767120361328]\n",
            "NEW_STD = [0.08422686159610748, 0.11849816143512726, 0.0835428312420845]\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating stats on Training Data...\")\n",
        "    \n",
        "# We use the class we just defined\n",
        "custom_mean, custom_std = compute_dataset_stats(\n",
        "        dataset_class=MaskedFixedTileDataset, \n",
        "        dataframe=full_df, \n",
        "        img_dir=train_set_dir\n",
        "    )\n",
        "\n",
        "NORMALIZATION_MEAN = custom_mean\n",
        "NORMALIZATION_STD = custom_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "1a69ba8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "\n",
        "# ADVICE 3\n",
        "train_transform_tl = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transform_ft = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(\n",
        "            IMG_RESIZE, scale=(0.7, 1.0), ratio=(0.75, 1.33), antialias=True\n",
        "        ),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=7),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(15),  # type: ignore\n",
        "        \n",
        "        transforms.RandomApply([\n",
        "            transforms.ElasticTransform(alpha=50.0, sigma=5.0)\n",
        "        ], p=0.25),\n",
        "        \n",
        "        \n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "a2ad77bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 464, Val samples: 117\n"
          ]
        }
      ],
      "source": [
        "# Train/validation split (stratified)\n",
        "train_df, val_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=(TEST_SET_SIZE + VAL_SET_SIZE),\n",
        "    stratify=full_df[\"label\"],\n",
        "    random_state=SEED,\n",
        ")\n",
        "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b14fe3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46b14fe3",
        "outputId": "7b0afa5a-011d-4ff0-d681-6d810b6b87ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 464 images to extract fixed-size (224, 224) tiles...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d268116585594073bee14b217670fe73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/464 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_dataset = MaskedFixedTileDataset(\n",
        "    train_df,\n",
        "    train_set_dir,\n",
        "    transform=train_transform_tl,\n",
        "    target_size=IMG_RESIZE,\n",
        "    debug_max=None,\n",
        ")\n",
        "val_dataset = MaskedFixedTileDataset(\n",
        "    val_df, train_set_dir, transform=data_transforms, target_size=IMG_RESIZE, debug_max=None\n",
        ")\n",
        "\n",
        "train_loader = make_loader(train_dataset, BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader = make_loader(val_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "# Alias per le fasi di training/fine-tuning\n",
        "train_tl_loader, val_tl_loader = train_loader, val_loader\n",
        "train_ft_loader, val_ft_loader = train_loader, val_loader\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "print(f\"Input tensor shape: {INPUT_SHAPE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aKwmsAocGJ1y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aKwmsAocGJ1y",
        "outputId": "521ec826-8a67-4ec0-cd4b-f68a91da7c3b"
      },
      "outputs": [],
      "source": [
        "def unnormalize(img, mean, std):\n",
        "    img = np.array(img, copy=True)\n",
        "    for c in range(3):\n",
        "        img[c] = img[c] * std[c] + mean[c]\n",
        "    return np.clip(img, 0, 1)\n",
        "\n",
        "\n",
        "def show_mask_patch_effect(dataset, train_transform, data_transforms, n=3):\n",
        "    parents = list({s[\"parent\"] for s in dataset.samples})\n",
        "    parents = random.sample(parents, min(n, len(parents)))\n",
        "\n",
        "    for img_name in parents:\n",
        "        img_path = os.path.join(dataset.img_dir, img_name)\n",
        "        mask_path = os.path.join(dataset.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\")) > 0\n",
        "\n",
        "        labeled_mask, n_components = ndimage.label(mask)  # type: ignore\n",
        "\n",
        "        fig = plt.figure(figsize=(4 * (n_components + 1), 5))\n",
        "        gs = fig.add_gridspec(1, n_components + 1)\n",
        "\n",
        "        # Originale + maschera\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(image)\n",
        "        ax0.imshow(mask, alpha=0.4, cmap=\"Reds\")\n",
        "        ax0.set_title(\"Originale + Maschera\")\n",
        "        ax0.axis(\"off\")\n",
        "\n",
        "        col = 1\n",
        "        for cid in range(1, n_components + 1):\n",
        "            ys, xs = np.where(labeled_mask == cid)\n",
        "            if len(xs) < 15:\n",
        "                continue\n",
        "\n",
        "            x1, x2 = xs.min(), xs.max()\n",
        "            y1, y2 = ys.min(), ys.max()\n",
        "\n",
        "            patch = image[y1:y2, x1:x2]\n",
        "            patch_pil = Image.fromarray(patch)\n",
        "\n",
        "            train_img = train_transform(patch_pil)\n",
        "            train_img = unnormalize(train_img.numpy(), NORMALIZATION_MEAN, NORMALIZATION_STD)\n",
        "\n",
        "            ax = fig.add_subplot(gs[col])\n",
        "            ax.imshow(np.transpose(train_img, (1, 2, 0)))\n",
        "            ax.set_title(f\"Patch {cid}\")\n",
        "            ax.axis(\"off\")\n",
        "            col += 1\n",
        "\n",
        "        plt.suptitle(f\"{img_name} â€“ Patch estratte dai blob\", fontsize=14)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def5ab26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: fix the show mask function, beacuse data_transform does nothing\n",
        "\n",
        "show_mask_patch_effect(\n",
        "    dataset=train_dataset,\n",
        "    train_transform=train_transform_tl,\n",
        "    data_transforms=data_transforms,\n",
        "    n=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f6b750",
      "metadata": {},
      "source": [
        "## ðŸ§® **Network Parameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c901504",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "DROPOUT_RATE = 0.25\n",
        "WEIGHT_DECAY = 1e-2\n",
        "\n",
        "# Regularization\n",
        "DROPOUT_RATE = 0.3\n",
        "LABEL_SMOOTHING = 0.05\n",
        "\n",
        "# Fine tuning parameters\n",
        "FT_LEARNING_RATE = 5e-6\n",
        "FT_WEIGHT_DECAY = 1e-3\n",
        "FT_DROPOUT_RATE = 0.2\n",
        "N_LAYERS_TO_UNFREEZE = 20\n",
        "\n",
        "VERBOSE = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd872b8a",
      "metadata": {
        "id": "fd872b8a"
      },
      "source": [
        "## ðŸ§  **Training Functions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea708144",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2, device=\"cuda\"):\n",
        "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Calculates weighted loss for mixed targets\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5956abc5",
      "metadata": {
        "id": "5956abc5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, mixup_alpha=0.0):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for inputs, targets, _ in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if mixup_alpha > 0:\n",
        "            # Generate mixed inputs\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(\n",
        "                inputs, targets, mixup_alpha, device\n",
        "            )\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")): # type: ignore\n",
        "                logits = model(inputs)\n",
        "                # Calculate loss mixing both targets\n",
        "                loss = mixup_criterion(criterion, logits, targets_a, targets_b, lam)\n",
        "        else:\n",
        "            # Standard training\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")): # type: ignore\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        # If mixed, use targets_a for rough accuracy estimation\n",
        "        current_targets = targets_a if mixup_alpha > 0 else targets # type: ignore\n",
        "\n",
        "        all_predictions.append(preds.cpu().numpy())\n",
        "        all_targets.append(current_targets.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(all_targets)\n",
        "    y_pred = np.concatenate(all_predictions)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b05dac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate one epoch with test time augmentation\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): model to validate\n",
        "        val_loader (torch.utils.data.DataLoader): dataset loader\n",
        "        criterion (torch.nn.modules.loss._Loss): criterion\n",
        "        device (torch.device): device\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, _ in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")): # type: ignore\n",
        "                # TTA Strategy:\n",
        "                # 1. Original\n",
        "                out1 = F.softmax(model(inputs), dim=1)\n",
        "\n",
        "                # 2. Horizontal Flip\n",
        "                inputs_hf = torch.flip(inputs, dims=[3])\n",
        "                out2 = F.softmax(model(inputs_hf), dim=1)\n",
        "\n",
        "                # 3. Vertical Flip\n",
        "                inputs_vf = torch.flip(inputs, dims=[2])\n",
        "                out3 = F.softmax(model(inputs_vf), dim=1)\n",
        "\n",
        "                # Average predictions\n",
        "                avg_probs = (out1 + out2 + out3) / 3.0\n",
        "\n",
        "                # Calculate Loss (using original inputs for approximation)\n",
        "                # Note: Loss with TTA is tricky, usually we just track accuracy/F1\n",
        "                # We use out1 for loss logging to be consistent\n",
        "                loss = criterion(model(inputs), targets)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                preds = avg_probs.argmax(dim=1)\n",
        "\n",
        "                all_predictions.append(preds.cpu().numpy())\n",
        "                all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(all_targets)\n",
        "    y_pred = np.concatenate(all_predictions)\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae97722",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scaler,\n",
        "    device,\n",
        "    scheduler=None,\n",
        "    patience=0,\n",
        "    evaluation_metric=\"val_f1\",\n",
        "    mode=\"max\",\n",
        "    restore_best_weights=True,\n",
        "    writer=None,\n",
        "    verbose=1,\n",
        "    experiment_name=\"\",\n",
        "    mixup_alpha=0.0,\n",
        "):\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_f1\": [], \"val_loss\": [], \"val_f1\": []}\n",
        "\n",
        "    best_metric = float(\"-inf\") if mode == \"max\" else float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, _, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, mixup_alpha)\n",
        "        val_loss, _, val_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            # If using ReduceLROnPlateau, step based on validation metric\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(val_f1)\n",
        "            else:\n",
        "                # If using CosineAnnealing, step based on epoch\n",
        "                scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_f1\"].append(train_f1)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"Loss/Training\", train_loss, epoch)\n",
        "            writer.add_scalar(\"F1/Training\", train_f1, epoch)\n",
        "            writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
        "            writer.add_scalar(\"F1/Validation\", val_f1, epoch)\n",
        "\n",
        "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
        "            print(\n",
        "                f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
        "                f\"Val: Loss={val_loss:.4f}, F1={val_f1:.4f} | \"\n",
        "                f\"LR: {scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            )\n",
        "\n",
        "        current_metric = history[evaluation_metric][-1]\n",
        "        is_improvement = (\n",
        "            (current_metric > best_metric)\n",
        "            if mode == \"max\"\n",
        "            else (current_metric < best_metric)\n",
        "        )\n",
        "\n",
        "        if is_improvement:\n",
        "            best_metric = current_metric\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience and patience > 0:\n",
        "                print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                break\n",
        "\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\" + experiment_name + \"_model.pt\"))\n",
        "        print(\n",
        "            f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\"\n",
        "        )\n",
        "    elif patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8643d67",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def analyze_performance(model, loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Generating predictions for Confusion Matrix...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, _ in tqdm(loader, desc=\"Validating\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # --- 1. Confusion Matrix ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Normalize by row (True Label) to see recall percentages\n",
        "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm_normalized,\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "    )\n",
        "    plt.ylabel(\"True Label (Ground Truth)\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.title(\"Normalized Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- 2. Classification Report ---\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\n",
        "        classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
        "    )\n",
        "\n",
        "    return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a5ecf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        # Alpha should be the class weights you already calculated\n",
        "        # If alpha is None, no class weighting is applied\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction=\"none\")\n",
        "        pt = torch.exp(-ce_loss)  # prevents nans when probability is 0\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d2ad0c",
      "metadata": {
        "id": "29d2ad0c"
      },
      "source": [
        "## ðŸ› ï¸ **Transfer Learning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f17a8ac",
      "metadata": {
        "id": "3f17a8ac"
      },
      "outputs": [],
      "source": [
        "class EfficientNetCustom(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiates EfficientNet-B0 with ImageNet weights.\n",
        "    Replaces the classifier head with a high-dropout dense layer to prevent overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.efficientnet_b0(weights=self.weights)\n",
        "        \n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, self.num_classes),  # type: ignore\n",
        "        )\n",
        "        self.freeze_backbone()\n",
        "    \n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all layers except the classifier head\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not name.startswith(\"classifier\"):\n",
        "                param.requires_grad = False\n",
        "        # Ensure classifier params are trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "        # Unfreeze the last n_layers of the backbone (excluding classifier which is already trainable)\n",
        "        child_counter = 0\n",
        "        for child in reversed(list(self.backbone.children())):\n",
        "            child_counter += 1\n",
        "            if child_counter <= n_layers:\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0869540",
      "metadata": {},
      "outputs": [],
      "source": [
        "tl_model = EfficientNetCustom(\n",
        "    num_classes, DROPOUT_RATE\n",
        ").to(device)\n",
        "\n",
        "tl_model.freeze_backbone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb30fce2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# calcolo class weights dal TRAIN\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.arange(num_classes),\n",
        "    y=train_df[\"label_index\"].values,\n",
        ")\n",
        "\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0, reduction=\"mean\")\n",
        "\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ulntce16E9NJ",
      "metadata": {
        "id": "ulntce16E9NJ"
      },
      "outputs": [],
      "source": [
        "# Setup training\n",
        "experiment_name = \"transfer_learning\"\n",
        "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + experiment_name)\n",
        "\n",
        "optimizer = AdamW(tl_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=EPOCHS, eta_min=1e-6\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "si9DNlteE_UK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si9DNlteE_UK",
        "outputId": "46c0d782-bcb0-400a-ceef-fb71296c6481"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Train with transfer learning\n",
        "tl_model, tl_history = fit(\n",
        "    model=tl_model,\n",
        "    train_loader=train_tl_loader,\n",
        "    val_loader=val_tl_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    scheduler=scheduler,\n",
        "    writer=writer,\n",
        "    verbose=VERBOSE,\n",
        "    experiment_name=experiment_name,\n",
        "    patience=PATIENCE,\n",
        "    mixup_alpha=0.0,\n",
        ")\n",
        "\n",
        "final_f1_score = max(tl_history['val_f1'])\n",
        "print(f'Maximum f1 score: {final_f1_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bed00f",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_performance(tl_model, val_loader, device, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f3f7ec",
      "metadata": {
        "id": "66f3f7ec"
      },
      "source": [
        "## **Fine-Tuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25029e59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25029e59",
        "outputId": "0bcaed2e-e0b6-46ea-931a-a9753b363d87"
      },
      "outputs": [],
      "source": [
        "train_dataset.transform = train_transform_ft\n",
        "\n",
        "train_ft_loader = make_loader(train_dataset, BATCH_SIZE, shuffle=True)\n",
        "val_ft_loader = make_loader(val_dataset, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "ft_model = EfficientNetCustom(\n",
        "    num_classes, FT_DROPOUT_RATE,\n",
        ").to(device)\n",
        "ft_model.load_state_dict(torch.load(f\"models/{experiment_name}_model.pt\"))\n",
        "\n",
        "# ft_model.unfreeze_backbone(N_LAYERS_TO_UNFREEZE, all=False)\n",
        "ft_model.unfreeze_backbone(N_LAYERS_TO_UNFREEZE, all=True)\n",
        "\n",
        "total_params = sum(p.numel() for p in ft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KKbWzNMBN1_n",
      "metadata": {
        "id": "KKbWzNMBN1_n"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"fine_tuning\"\n",
        "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + experiment_name)\n",
        "\n",
        "optimizer = AdamW(\n",
        "    ft_model.parameters(), lr=FT_LEARNING_RATE, weight_decay=FT_WEIGHT_DECAY\n",
        ")\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))  # type: ignore\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=EPOCHS, eta_min=1e-6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22dec830",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dec830",
        "outputId": "62f4e1ce-3ade-4f83-ef5a-754c35b0ab04"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "ft_model, ft_history = fit(\n",
        "    model=ft_model,\n",
        "    train_loader=train_ft_loader,\n",
        "    val_loader=val_ft_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    scheduler=scheduler,\n",
        "    writer=writer,\n",
        "    verbose=VERBOSE,\n",
        "    experiment_name=experiment_name,\n",
        "    patience=PATIENCE,\n",
        "    mixup_alpha=0.2,\n",
        ")\n",
        "\n",
        "final_f1_score = max(ft_history['val_f1'])\n",
        "print(f'Maximum f1 score: {final_f1_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f50176",
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_performance(ft_model, val_ft_loader, device, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365f9814",
      "metadata": {
        "id": "365f9814"
      },
      "source": [
        "## **Evaluation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e4ea049",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot TL vs FT\n",
        "def plot_history(tl_history, ft_history):\n",
        "    epochs_tl = len(tl_history[\"val_f1\"])\n",
        "    epochs_ft = len(ft_history[\"val_f1\"])\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # F1\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(tl_history[\"train_f1\"], label=\"Train F1 TL\")\n",
        "    plt.plot(tl_history[\"val_f1\"], label=\"Val F1 TL\")\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft),\n",
        "        ft_history[\"train_f1\"],\n",
        "        label=\"Train F1 FT\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft), ft_history[\"val_f1\"], label=\"Val F1 FT\"\n",
        "    )\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1 macro\")\n",
        "    plt.title(\"Andamento F1 TL+FT\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(tl_history[\"train_loss\"], label=\"Train Loss TL\")\n",
        "    plt.plot(tl_history[\"val_loss\"], label=\"Val Loss TL\")\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft),\n",
        "        ft_history[\"train_loss\"],\n",
        "        label=\"Train Loss FT\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft),\n",
        "        ft_history[\"val_loss\"],\n",
        "        label=\"Val Loss FT\",\n",
        "    )\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Andamento Loss TL+FT\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b0ee58",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_history(tl_history, ft_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dae20b4",
      "metadata": {
        "id": "6dae20b4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def evaluate_macro_f1(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _ in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "            y_pred.extend(preds.cpu().tolist())\n",
        "\n",
        "    return f1_score(y_true, y_pred, average=\"macro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4fc853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4fc853",
        "outputId": "c4e27bbc-14b2-4c71-b1d0-0fb68a24a285"
      },
      "outputs": [],
      "source": [
        "best_model = EfficientNetCustom(\n",
        "    num_classes, DROPOUT_RATE,\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load(f\"models/{experiment_name}_model.pt\"))\n",
        "best_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9964ad6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9964ad6",
        "outputId": "577dadf8-ba13-4832-a3c0-dde85f0cc914"
      },
      "outputs": [],
      "source": [
        "train_f1 = evaluate_macro_f1(best_model, train_ft_loader, device)\n",
        "val_f1 = evaluate_macro_f1(best_model, val_ft_loader, device)\n",
        "\n",
        "print(f\"F1 TRAIN (macro): {train_f1:.4f}\")\n",
        "print(f\"F1 VAL   (macro): {val_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f7d2d7",
      "metadata": {
        "id": "a3f7d2d7"
      },
      "source": [
        "## **Inference on test_data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5780d454",
      "metadata": {
        "id": "5780d454"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "test_dataset = MaskedFixedTileDataset(\n",
        "    dataframe=None, img_dir=test_set_dir, transform=data_transforms, target_size=IMG_RESIZE, debug_max=None\n",
        ")\n",
        "\n",
        "test_loader = make_loader(test_dataset, BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "print(f\"Starting Inference on {len(test_dataset)} patches...\")\n",
        "\n",
        "# Dictionary to store probabilities: { 'img_123.png': [ [p0, p1, p2, p3], ... ] }\n",
        "patch_probs = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _, parent_ids in tqdm(test_loader, desc=\"Inference\"):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # TTA Strategy:\n",
        "        # 1. Original\n",
        "        out1 = F.softmax(best_model(inputs), dim=1)\n",
        "\n",
        "        # 2. Horizontal Flip\n",
        "        inputs_hf = torch.flip(inputs, dims=[3])\n",
        "        out2 = F.softmax(best_model(inputs_hf), dim=1)\n",
        "\n",
        "        # 3. Vertical Flip\n",
        "        inputs_vf = torch.flip(inputs, dims=[2])\n",
        "        out3 = F.softmax(best_model(inputs_vf), dim=1)\n",
        "\n",
        "        # Average predictions\n",
        "        avg_probs = (out1 + out2 + out3) / 3.0\n",
        "        avg_probs = avg_probs.cpu().numpy()\n",
        "\n",
        "        # Group by Parent Image\n",
        "        for i, pid in enumerate(parent_ids):\n",
        "            if pid not in patch_probs:\n",
        "                patch_probs[pid] = []\n",
        "            patch_probs[pid].append(probs[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbafce6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Aggregating predictions (Soft Voting)...\")\n",
        "final_rows = []\n",
        "\n",
        "# 2. Aggregation Loop\n",
        "for img_name, prob_list in patch_probs.items():\n",
        "    # Stack into shape (N_patches, 4)\n",
        "    prob_matrix = np.array(prob_list)\n",
        "    \n",
        "    # SOFT VOTING: Average the probabilities across all patches for this slide\n",
        "    avg_probs = np.mean(prob_matrix, axis=0)\n",
        "    \n",
        "    # The class with the highest average probability is our prediction\n",
        "    pred_class = np.argmax(avg_probs)\n",
        "    \n",
        "    # Format the filename to match sample_index constraints if needed\n",
        "    # Assuming sample_index is the full filename like \"img_0.png\" or just \"0\"\n",
        "    # Adjusting to match your snippet's likely expectation:\n",
        "    sample_index = img_name \n",
        "    \n",
        "    final_rows.append({\n",
        "        \"sample_index\": sample_index,\n",
        "        \"label\": pred_class\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652168da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "652168da",
        "outputId": "a9816364-39b0-42be-8843-1309ba6d477a"
      },
      "outputs": [],
      "source": [
        "# map label index to class name name\n",
        "for row in final_rows:\n",
        "    row[\"label\"] = class_names[row[\"label\"]]\n",
        " \n",
        "submission_df = pd.DataFrame(final_rows)\n",
        "submission_df = submission_df.sort_values(by=\"sample_index\")\n",
        "\n",
        "os.makedirs(os.path.join(current_dir, \"submission\"), exist_ok=True)\n",
        "submission_file_pos = os.path.join(current_dir, \"submission\", \"efficient_net_b0.csv\")\n",
        "submission_df.to_csv(\n",
        "    submission_file_pos, index=False\n",
        ")\n",
        "\n",
        "print(\"Submission file created: \", submission_file_pos)\n",
        "submission_df.head(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d5198a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
