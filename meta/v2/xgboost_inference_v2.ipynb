{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a75bb6ad",
      "metadata": {},
      "source": [
        "# **XGBoost**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "900b0225",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enviroment\n",
        "isColab = False\n",
        "colab_dir = \"/gdrive/My Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "\n",
        "isKaggle = False\n",
        "isWsl = True\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "591ec1c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loader parameters\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZATION_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "IMG_RESIZE = (300, 300)\n",
        "INPUT_SHAPE = (3, *IMG_RESIZE)\n",
        "\n",
        "MASK_THRESHOLD = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "e38d561c",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"densenet121_300p_v2\"\n",
        "NET_NAME = \"densenet121\"\n",
        "\n",
        "# Fine tuning parameters\n",
        "FT_DROPOUT_RATE = 0.45\n",
        "\n",
        "K_FOLD_LIMIT = 3\n",
        "K_FOLD_MAX_VALUE = 5\n",
        "\n",
        "submission_path = f\"submission/{EXPERIMENT_NAME}_submission_xgb.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4cb00e",
      "metadata": {},
      "source": [
        "## **Loading Enviroment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "e09b8c7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esecuzione su WSL. Directory corrente impostata a: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
            "Changed directory to: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
            "Dataset directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset\n",
            "Train set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_data\n",
            "Test set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/test_data\n",
            "Label file: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_labels.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Directory di default\n",
        "current_dir = os.getcwd()   \n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive # type: ignore\n",
        "    drive.mount(\"/gdrive\")\n",
        "    current_dir = colab_dir\n",
        "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
        "    %cd $current_dir\n",
        "elif isKaggle:\n",
        "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
        "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
        "    current_dir = kaggle_work_dir\n",
        "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
        "    os.chdir(current_dir)\n",
        "elif isWsl:\n",
        "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "else:\n",
        "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
        "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "\n",
        "print(f\"Changed directory to: {current_dir}\")\n",
        "\n",
        "# Define absolute paths\n",
        "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
        "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
        "\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Train set directory: {train_set_dir}\")\n",
        "print(f\"Test set directory: {test_set_dir}\")\n",
        "print(f\"Label file: {label_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e0fdc8",
      "metadata": {},
      "source": [
        "## **Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be49326d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49326d",
        "outputId": "59e423d7-35ea-4fc1-933c-affb303ee9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1+cu130\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "if isColab or isKaggle:\n",
        "    !pkill -f tensorboard \n",
        "    !mkdir -p models\n",
        "    print(\"Killed existing TensorBoard instances and created models directory.\") \n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)  \n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import shutil\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcbf293",
      "metadata": {},
      "source": [
        "### **Preparing Dataset for colab**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "c8f17001",
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    drive_dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "    local_dataset_dir = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(local_dataset_dir):\n",
        "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
        "        try:\n",
        "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
        "            print(\"Copy complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying dataset: {e}\")\n",
        "            print(\"Falling back to Drive dataset (slow).\")\n",
        "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
        "            dataset_dir = drive_dataset_dir\n",
        "    else:\n",
        "        print(\"Dataset already copied to local runtime.\")\n",
        "\n",
        "    # If copy succeeded (or already existed), use local path\n",
        "    if os.path.exists(local_dataset_dir):\n",
        "        dataset_dir = local_dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfe95f2",
      "metadata": {},
      "source": [
        "## ‚è≥ **Data Loading**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e112e14",
      "metadata": {},
      "source": [
        "### **Definitions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "7d945105",
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_IGNORE = [\n",
        "    \"img_0001.png\",\n",
        "    \"img_0005.png\",\n",
        "    \"img_0008.png\",\n",
        "    \"img_0012.png\",\n",
        "    \"img_0018.png\",\n",
        "    \"img_0020.png\",\n",
        "    \"img_0022.png\",\n",
        "    \"img_0027.png\",\n",
        "    \"img_0028.png\",\n",
        "    \"img_0036.png\",\n",
        "    \"img_0044.png\",\n",
        "    \"img_0047.png\",\n",
        "    \"img_0048.png\",\n",
        "    \"img_0052.png\",\n",
        "    \"img_0062.png\",\n",
        "    \"img_0078.png\",\n",
        "    \"img_0085.png\",\n",
        "    \"img_0090.png\",\n",
        "    \"img_0094.png\",\n",
        "    \"img_0095.png\",\n",
        "    \"img_0126.png\",\n",
        "    \"img_0129.png\",\n",
        "    \"img_0130.png\",\n",
        "    \"img_0133.png\",\n",
        "    \"img_0136.png\",\n",
        "    \"img_0138.png\",\n",
        "    \"img_0148.png\",\n",
        "    \"img_0150.png\",\n",
        "    \"img_0155.png\",\n",
        "    \"img_0159.png\",\n",
        "    \"img_0161.png\",\n",
        "    \"img_0175.png\",\n",
        "    \"img_0178.png\",\n",
        "    \"img_0179.png\",\n",
        "    \"img_0180.png\",\n",
        "    \"img_0184.png\",\n",
        "    \"img_0187.png\",\n",
        "    \"img_0189.png\",\n",
        "    \"img_0193.png\",\n",
        "    \"img_0196.png\",\n",
        "    \"img_0222.png\",\n",
        "    \"img_0251.png\",\n",
        "    \"img_0254.png\",\n",
        "    \"img_0263.png\",\n",
        "    \"img_0268.png\",\n",
        "    \"img_0286.png\",\n",
        "    \"img_0293.png\",\n",
        "    \"img_0313.png\",\n",
        "    \"img_0319.png\",\n",
        "    \"img_0333.png\",\n",
        "    \"img_0342.png\",\n",
        "    \"img_0344.png\",\n",
        "    \"img_0346.png\",\n",
        "    \"img_0355.png\",\n",
        "    \"img_0368.png\",\n",
        "    \"img_0371.png\",\n",
        "    \"img_0376.png\",\n",
        "    \"img_0380.png\",\n",
        "    \"img_0390.png\",\n",
        "    \"img_0393.png\",\n",
        "    \"img_0407.png\",\n",
        "    \"img_0410.png\",\n",
        "    \"img_0415.png\",\n",
        "    \"img_0424.png\",\n",
        "    \"img_0443.png\",\n",
        "    \"img_0453.png\",\n",
        "    \"img_0459.png\",\n",
        "    \"img_0463.png\",\n",
        "    \"img_0486.png\",\n",
        "    \"img_0497.png\",\n",
        "    \"img_0498.png\",\n",
        "    \"img_0499.png\",\n",
        "    \"img_0509.png\",\n",
        "    \"img_0521.png\",\n",
        "    \"img_0530.png\",\n",
        "    \"img_0531.png\",\n",
        "    \"img_0533.png\",\n",
        "    \"img_0537.png\",\n",
        "    \"img_0540.png\",\n",
        "    \"img_0544.png\",\n",
        "    \"img_0547.png\",\n",
        "    \"img_0557.png\",\n",
        "    \"img_0558.png\",\n",
        "    \"img_0560.png\",\n",
        "    \"img_0565.png\",\n",
        "    \"img_0567.png\",\n",
        "    \"img_0572.png\",\n",
        "    \"img_0578.png\",\n",
        "    \"img_0580.png\",\n",
        "    \"img_0586.png\",\n",
        "    \"img_0602.png\",\n",
        "    \"img_0603.png\",\n",
        "    \"img_0607.png\",\n",
        "    \"img_0609.png\",\n",
        "    \"img_0614.png\",\n",
        "    \"img_0620.png\",\n",
        "    \"img_0623.png\",\n",
        "    \"img_0629.png\",\n",
        "    \"img_0635.png\",\n",
        "    \"img_0639.png\",\n",
        "    \"img_0643.png\",\n",
        "    \"img_0644.png\",\n",
        "    \"img_0645.png\",\n",
        "    \"img_0646.png\",\n",
        "    \"img_0656.png\",\n",
        "    \"img_0657.png\",\n",
        "    \"img_0658.png\",\n",
        "    \"img_0670.png\",\n",
        "    \"img_0673.png\",\n",
        "    \"img_0675.png\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "7afdb353",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 4\n"
          ]
        }
      ],
      "source": [
        "# Load the full dataframe\n",
        "full_df = pd.read_csv(label_file)\n",
        "\n",
        "# Remove cursed images\n",
        "full_df = full_df[~full_df[\"sample_index\"].isin(SAMPLES_TO_IGNORE)].reset_index(\n",
        "    drop=True\n",
        ")\n",
        "\n",
        "# Label mapping\n",
        "class_names = sorted(full_df[\"label\"].unique())\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "full_df[\"label_index\"] = full_df[\"label\"].map(label_to_index)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "9e13a32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last=False):\n",
        "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(6, cpu_cores))\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,\n",
        "        persistent_workers=isWsl,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "8b0c274f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MacenkoNormalizer:\n",
        "    \"\"\"\n",
        "    Normalizes H&E stained images to a reference appearance using the Macenko method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Target reference values (standard for CPath)\n",
        "        # These are pre-computed means/stds from a \"perfect\" slide\n",
        "        self.HERef = np.array([[0.5626, 0.2159], [0.7201, 0.8012], [0.4062, 0.5581]])\n",
        "        self.maxCRef = np.array([1.9705, 1.0308])\n",
        "\n",
        "    def __call__(self, img_arr, Io=240, alpha=1, beta=0.15):\n",
        "        \"\"\"\n",
        "        img_arr: RGB numpy array (H, W, 3)\n",
        "        Returns: Normalized numpy array\n",
        "        \"\"\"\n",
        "        # 1. Convert to Optical Density (OD)\n",
        "        h, w, c = img_arr.shape\n",
        "        img_arr = img_arr.reshape((-1, 3))\n",
        "\n",
        "        # Avoid division by zero\n",
        "        OD = -np.log((img_arr.astype(np.float64) + 1) / Io)\n",
        "\n",
        "        # 2. Remove transparent pixels\n",
        "        ODhat = OD[~np.any(OD < beta, axis=1)]\n",
        "        if ODhat.shape[0] < 10:  # Safety check for empty patches\n",
        "            return img_arr.reshape(h, w, c).copy()\n",
        "\n",
        "        # 3. Compute SVD\n",
        "        _, eigvecs = np.linalg.eigh(np.cov(ODhat.T))\n",
        "\n",
        "        # 4. Project on the plane spanned by the eigenvectors corresponding to the two largest eigenvalues\n",
        "        That = ODhat.dot(eigvecs[:, -2:])\n",
        "\n",
        "        # 5. Find robust extremes (1st and 99th percentiles)\n",
        "        phi = np.arctan2(That[:, 1], That[:, 0])\n",
        "        minPhi = np.percentile(phi, alpha)\n",
        "        maxPhi = np.percentile(phi, 100 - alpha)\n",
        "\n",
        "        vMin = eigvecs[:, -2:].dot(np.array([(np.cos(minPhi), np.sin(minPhi))]).T)\n",
        "        vMax = eigvecs[:, -2:].dot(np.array([(np.cos(maxPhi), np.sin(maxPhi))]).T)\n",
        "\n",
        "        # 6. Heuristic to ensure H is first vector, E is second\n",
        "        if vMin[0] > vMax[0]:\n",
        "            HE = np.array((vMin[:, 0], vMax[:, 0])).T\n",
        "        else:\n",
        "            HE = np.array((vMax[:, 0], vMin[:, 0])).T\n",
        "\n",
        "        # 7. Rows correspond to channels (RGB), columns to H&E stains\n",
        "        Y = np.reshape(OD, (-1, 3)).T\n",
        "\n",
        "        # Determine concentrations of the individual stains\n",
        "        C = np.linalg.lstsq(HE, Y, rcond=None)[0]\n",
        "\n",
        "        # 8. Normalize concentrations\n",
        "        maxC = np.percentile(C, 99, axis=1)\n",
        "        tmp = np.divide(maxC, self.maxCRef)\n",
        "        C2 = np.divide(C, tmp[:, np.newaxis])\n",
        "\n",
        "        # 9. Reconstruct the image\n",
        "        Inorm = np.multiply(Io, np.exp(-self.HERef.dot(C2)))\n",
        "        Inorm[Inorm > 255] = 254\n",
        "        Inorm = np.reshape(Inorm.T, (h, w, 3)).astype(np.uint8)\n",
        "\n",
        "        return Inorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c536e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageOps\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "class MaskedGridTileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset class that performs Grid Tiling over the tissue masks.\n",
        "    It extracts patches based on a sliding window, keeping only those\n",
        "    that contain sufficient biological tissue.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataframe,\n",
        "        img_dir,\n",
        "        transforms=None,\n",
        "        target_size=(300, 300),\n",
        "        mask_threshold=0.05,  # Keep patch if at least 5% is tissue\n",
        "        overlap_ratio=0.0,  # 0.0 = distinct tiles; 0.5 = 50% overlap\n",
        "        normalize=True,\n",
        "        debug_max=None,\n",
        "    ):\n",
        "        self.samples = []\n",
        "        self.transforms = transforms\n",
        "        self.img_dir = img_dir\n",
        "        self.target_size = target_size\n",
        "        self.mask_threshold = mask_threshold\n",
        "        self.overlap_ratio = overlap_ratio\n",
        "        self.normalizer = MacenkoNormalizer() if normalize else None\n",
        "        self.dropped_slides = 0\n",
        "\n",
        "        # Determine if we are in inference mode (no labels)\n",
        "        if dataframe is None or \"label_index\" not in dataframe.columns:\n",
        "            # We are in inference mode\n",
        "            if dataframe is None:\n",
        "                img_names = sorted(\n",
        "                    [f for f in os.listdir(img_dir) if f.startswith(\"img_\")]\n",
        "                )\n",
        "            else:\n",
        "                img_names = dataframe[\"sample_index\"].tolist()\n",
        "            iterator = zip(img_names, [-1] * len(img_names))\n",
        "            total_items = len(img_names)\n",
        "        else:\n",
        "            iterator = zip(dataframe[\"sample_index\"], dataframe[\"label_index\"])\n",
        "            total_items = len(dataframe)\n",
        "\n",
        "        print(\n",
        "            f\"Processing {total_items} slides with Grid Tiling (Thr={mask_threshold}, Overlap={overlap_ratio})...\"\n",
        "        )\n",
        "\n",
        "        count = 0\n",
        "        for img_name, label in tqdm(iterator, total=total_items):\n",
        "            if debug_max and count >= debug_max:\n",
        "                break\n",
        "            self._process_and_extract(img_name, label)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Extraction complete. Total patches: {len(self.samples)}\")\n",
        "        if self.dropped_slides > 0:\n",
        "            print(\n",
        "                f\"‚ö†Ô∏è Warning: {self.dropped_slides} slides were completely empty/corrupt.\"\n",
        "            )\n",
        "\n",
        "    def _process_and_extract(self, img_name, label):\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask = Image.open(mask_path).convert(\"L\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load {img_name}: {e}\")\n",
        "            self.dropped_slides += 1\n",
        "            return\n",
        "\n",
        "        img_w, img_h = image.size\n",
        "        tile_h, tile_w = self.target_size\n",
        "\n",
        "        # Calculate stride based on overlap\n",
        "        stride_h = int(tile_h * (1 - self.overlap_ratio))\n",
        "        stride_w = int(tile_w * (1 - self.overlap_ratio))\n",
        "\n",
        "        # Ensure stride is at least 1 pixel to prevent infinite loops\n",
        "        stride_h = max(1, stride_h)\n",
        "        stride_w = max(1, stride_w)\n",
        "\n",
        "        mask_arr = np.array(mask)\n",
        "        # Binarize mask (assuming any non-zero value is tissue)\n",
        "        binary_mask = (mask_arr > 0).astype(np.uint8)\n",
        "\n",
        "        # Iterate via sliding window\n",
        "        # We allow y and x to go slightly out of bounds to catch edges, handled by padding later\n",
        "        for y in range(0, img_h, stride_h):\n",
        "            for x in range(0, img_w, stride_w):\n",
        "\n",
        "                # Define current window coordinates\n",
        "                x2 = x + tile_w\n",
        "                y2 = y + tile_h\n",
        "\n",
        "                # --- 1. MASK CHECK (Fast filtering) ---\n",
        "                # Clip coordinates to image bounds for the mask check\n",
        "                mx1, my1 = x, y\n",
        "                mx2, my2 = min(x2, img_w), min(y2, img_h)\n",
        "\n",
        "                # Extract the mask patch corresponding to this tile\n",
        "                mask_patch = binary_mask[my1:my2, mx1:mx2]\n",
        "\n",
        "                # If the patch is empty or purely padding, skip\n",
        "                if mask_patch.size == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate tissue percentage\n",
        "                tissue_pixels = np.count_nonzero(mask_patch)\n",
        "                total_pixels = tile_w * tile_h  # Use theoretical size\n",
        "\n",
        "                # If we are on the edge, the actual mask_patch might be smaller,\n",
        "                # but we normalize by the Target Tile Size to penalize mostly-empty edge crops.\n",
        "                tissue_coverage = tissue_pixels / total_pixels\n",
        "\n",
        "                if tissue_coverage < self.mask_threshold:\n",
        "                    continue\n",
        "\n",
        "                # --- 2. IMAGE EXTRACTION (Only if mask check passed) ---\n",
        "                # Handle Edge Padding\n",
        "                # If x2 > img_w or y2 > img_h, we need to crop what we can and pad the rest\n",
        "\n",
        "                # Actual crop coordinates within image\n",
        "                crop_x1, crop_y1 = x, y\n",
        "                crop_x2, crop_y2 = min(x2, img_w), min(y2, img_h)\n",
        "\n",
        "                patch_crop = image.crop((crop_x1, crop_y1, crop_x2, crop_y2))\n",
        "\n",
        "                # Calculate padding needed\n",
        "                pad_right = max(0, x2 - img_w)\n",
        "                pad_bottom = max(0, y2 - img_h)\n",
        "\n",
        "                if pad_right > 0 or pad_bottom > 0:\n",
        "                    # Pad with white (255) for histology background\n",
        "                    patch = ImageOps.expand(\n",
        "                        patch_crop, border=(0, 0, pad_right, pad_bottom), fill=255\n",
        "                    )\n",
        "                else:\n",
        "                    patch = patch_crop\n",
        "\n",
        "                # Final safeguard on size\n",
        "                if patch.size != self.target_size:\n",
        "                    patch = patch.resize(self.target_size, Image.BICUBIC)  # type: ignore\n",
        "\n",
        "                patch_arr = np.array(patch)\n",
        "\n",
        "                if self.normalizer:\n",
        "                    try:\n",
        "                        # Macenko needs robust pixel data. If patch is mostly white/padding,\n",
        "                        # it might fail or look weird. We only run it if we have tissue.\n",
        "                        patch_arr = self.normalizer(patch_arr)\n",
        "                    except Exception as e:\n",
        "                        # Fallback if SVD fails on weird patches\n",
        "                        print(\n",
        "                            f\"Normalization failed on patch from {img_name} at ({x},{y}): {e}\"\n",
        "                        )\n",
        "                        pass\n",
        "\n",
        "                # Store in RAM\n",
        "                self.samples.append(\n",
        "                    {\"patch\": patch_arr, \"label\": label, \"parent\": img_name}\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        img = Image.fromarray(item[\"patch\"])\n",
        "        label = item[\"label\"]\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, label, item[\"parent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "147c9151",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "\n",
        "def compute_dataset_stats(\n",
        "    dataset_class,\n",
        "    dataframe,\n",
        "    img_dir,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=0.05,\n",
        "    overlap_ratio=0.0,\n",
        "    normalize=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes channel-wise Mean and Std on the dataset without any normalization applied.\n",
        "    \"\"\"\n",
        "    print(\"Computing dataset Mean and Std (this may take a moment)...\")\n",
        "\n",
        "    # define a simple transform that only converts to tensor\n",
        "    basic_transforms = transforms.Compose(\n",
        "        [transforms.Resize(IMG_RESIZE), transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    # Instantiate dataset temporarily\n",
        "    temp_ds = dataset_class(\n",
        "        dataframe,\n",
        "        img_dir,\n",
        "        transforms=basic_transforms,\n",
        "        target_size=target_size,\n",
        "        mask_threshold=mask_threshold,\n",
        "        overlap_ratio=overlap_ratio,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    loader = make_loader(temp_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    nb_samples = 0.0\n",
        "\n",
        "    for data, _, _ in tqdm(loader):\n",
        "        batch_samples = data.size(0)\n",
        "        # Flatten H and W to calculate stats per channel\n",
        "        data = data.view(batch_samples, data.size(1), -1)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        nb_samples += batch_samples\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    print(f\"\\nDONE. Copy these values into your config:\")\n",
        "    print(f\"NEW_MEAN = {mean.tolist()}\")  # type: ignore\n",
        "    print(f\"NEW_STD = {std.tolist()}\")  # type: ignore\n",
        "    return mean.tolist(), std.tolist()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cadb5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating stats on Training Data...\n",
            "Computing dataset Mean and Std (this may take a moment)...\n",
            "Processing 581 slides with Grid Tiling (Thr=0.05, Overlap=0.0)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "610961018a4b45c5b5a694e873d40040",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/581 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 919\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab6a78f3703c48baa7b15054fb2635ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/29 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DONE. Copy these values into your config:\n",
            "NEW_MEAN = [0.5219733119010925, 0.39480817317962646, 0.5568122267723083]\n",
            "NEW_STD = [0.10120205581188202, 0.08823584765195847, 0.07768534123897552]\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating stats on Training Data...\")\n",
        "\n",
        "# We use the class we just defined\n",
        "custom_mean, custom_std = compute_dataset_stats(\n",
        "    dataset_class=MaskedGridTileDataset,\n",
        "    dataframe=full_df,\n",
        "    img_dir=train_set_dir,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=MASK_THRESHOLD,\n",
        "    overlap_ratio=0.0,\n",
        "    normalize=True,\n",
        ")\n",
        "\n",
        "NORMALIZATION_MEAN = custom_mean\n",
        "NORMALIZATION_STD = custom_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c1b624",
      "metadata": {},
      "source": [
        "### **Transforms**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "1a69ba8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "\n",
        "data_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f6b750",
      "metadata": {},
      "source": [
        "## üßÆ **Network Parameters**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d25c92c",
      "metadata": {},
      "source": [
        "### **Custom Nets**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "3f17a8ac",
      "metadata": {
        "id": "3f17a8ac"
      },
      "outputs": [],
      "source": [
        "class EfficientNetCustom(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiates EfficientNet-B0 with ImageNet weights.\n",
        "    Replaces the classifier head with a high-dropout dense layer to prevent overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.EfficientNet_V2_S_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.efficientnet_v2_s(weights=self.weights)\n",
        "\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, self.num_classes),  # type: ignore\n",
        "        )\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all layers except the classifier head\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not name.startswith(\"classifier\"):\n",
        "                param.requires_grad = False\n",
        "        # Ensure classifier params are trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "        # Unfreeze the last n_layers of the backbone (excluding classifier which is already trainable)\n",
        "        child_counter = 0\n",
        "        for child in reversed(list(self.backbone.children())):\n",
        "            child_counter += 1\n",
        "            if child_counter <= n_layers:\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        \"\"\"Returns the flattened feature vector before the classifier.\"\"\"\n",
        "        # 1. Run features\n",
        "        x = self.backbone.features(x)\n",
        "        # 2. Global Average Pooling (same as original forward)\n",
        "        x = self.backbone.avgpool(x)\n",
        "        # 3. Flatten\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "701e496f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DenseNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.DenseNet121_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.densenet121(weights=self.weights)\n",
        "\n",
        "        # DenseNet classifier is stored in .classifier\n",
        "        in_features = self.backbone.classifier.in_features\n",
        "\n",
        "        # Replace Classifier\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all feature layers\n",
        "        for param in self.backbone.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Unfreeze classifier\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_blocks, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "\n",
        "        # Keep classifier trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Unfreeze the last n_blocks within features\n",
        "        children = list(self.backbone.features.children())\n",
        "        total_children = len(children)\n",
        "        if n_blocks <= 0:\n",
        "            return\n",
        "\n",
        "        start = max(0, total_children - n_blocks)\n",
        "        for i in range(start, total_children):\n",
        "            for param in children[i].parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        \"\"\"Returns the flattened feature vector before the classifier.\"\"\"\n",
        "        # DenseNet features\n",
        "        features = self.backbone.features(x)\n",
        "        # ReLU + Pooling (Standard DenseNet finish)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "41f44462",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper that can interchange between DenseNetCustom and CustomNet.\n",
        "    Keeps the same constructor signature used in the notebook:\n",
        "    CustomNet(num_classes, dropout_rate, backbone=...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4, backbone=\"densenet121\"):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.backbone_name = backbone.lower()\n",
        "\n",
        "        if self.backbone_name in (\"densenet\", \"densenet121\"):\n",
        "            self.backbone = DenseNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        elif self.backbone_name in (\n",
        "            \"efficientnet\",\n",
        "            \"efficientnet_v2s\",\n",
        "            \"efficientnetv2s\",\n",
        "        ):\n",
        "            self.backbone = EfficientNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported backbone '{backbone}'. Use 'densenet' or 'efficientnet'.\"\n",
        "            )\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"freeze_backbone\"):\n",
        "            self.backbone.freeze_backbone()\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"unfreeze_backbone\"):\n",
        "            self.backbone.unfreeze_backbone(n_layers, all=all)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        return self.backbone.extract_embeddings(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365f9814",
      "metadata": {
        "id": "365f9814"
      },
      "source": [
        "## **XGboost**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "b2e44904",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_mean_aggregation(prob_matrix, k_percent=0.3):\n",
        "    \"\"\"\n",
        "    Aggregates patch probabilities into a slide prediction by averaging\n",
        "    only the most confident patches (Top-K%).\n",
        "\n",
        "    Args:\n",
        "        prob_matrix: Numpy array of shape [N_patches, N_classes]\n",
        "        k_percent: Float (0.0 to 1.0). Percentage of patches to keep.\n",
        "                   0.3 means we only average the top 30% scores.\n",
        "    \"\"\"\n",
        "    n_patches = prob_matrix.shape[0]\n",
        "\n",
        "    # Safety check: if slide has very few patches, keep at least 1\n",
        "    k = max(1, int(n_patches * k_percent))\n",
        "\n",
        "    # Sort probabilities for each class INDEPENDENTLY (Axis 0 = patches)\n",
        "    # We want the highest probabilities for Class 0, Class 1, etc.\n",
        "    sorted_probs = np.sort(prob_matrix, axis=0)\n",
        "\n",
        "    # Take the top K (the last K elements in the sorted array)\n",
        "    top_k_probs = sorted_probs[-k:, :]\n",
        "\n",
        "    # Average them\n",
        "    slide_score = np.mean(top_k_probs, axis=0)\n",
        "\n",
        "    return slide_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "e7a663f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_from_probs(prob_matrix):\n",
        "    \"\"\"\n",
        "    Turns a (N_patches, 4) probability matrix into a single feature vector (1, N_features).\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # 1. Mean Probabilities (Standard Soft Voting) - 4 features\n",
        "    mean_probs = np.mean(prob_matrix, axis=0)\n",
        "    features.extend(mean_probs)\n",
        "\n",
        "    # 2. Max Probabilities (Detect strong tumor signal) - 4 features\n",
        "    max_probs = np.max(prob_matrix, axis=0)\n",
        "    features.extend(max_probs)\n",
        "\n",
        "    # --- NEW INSERTION HERE ---\n",
        "    # 3. Top-K Means (The \"Clean\" Signal)\n",
        "    # Top 30%: Averages the \"surest\" 1/3 of the slide.\n",
        "    # Filters out background but keeps the tumor chunks.\n",
        "    features.extend(top_k_mean_aggregation(prob_matrix, k_percent=0.3))\n",
        "\n",
        "    # Top 10%: Very aggressive. Focuses only on the absolute peak regions.\n",
        "    features.extend(top_k_mean_aggregation(prob_matrix, k_percent=0.1))\n",
        "\n",
        "    # 3. Standard Deviation (Detect tissue heterogeneity) - 4 features\n",
        "    std_probs = np.std(prob_matrix, axis=0)\n",
        "    features.extend(std_probs)\n",
        "\n",
        "    # 4. Percentiles (Robust Max) - 4 features\n",
        "    p90_probs = np.percentile(prob_matrix, 90, axis=0)\n",
        "    features.extend(p90_probs)\n",
        "\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "6bb1386a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def generate_hybrid_dataset(full_df):\n",
        "    print(\">>> Generating Hybrid (Embeddings + Prob Stats) Dataset...\")\n",
        "\n",
        "    # We must recreate the exact same split\n",
        "    skf = StratifiedKFold(n_splits=K_FOLD_MAX_VALUE, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # Storage for the Stacking Dataset\n",
        "    X_list = []  # Features\n",
        "    y_list = []  # True Labels\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(\n",
        "        skf.split(full_df, full_df[\"label_index\"])\n",
        "    ):\n",
        "        if fold_idx >= K_FOLD_LIMIT:\n",
        "            print(f\"Reached K_FOLD_LIMIT of {K_FOLD_LIMIT}. Stopping further folds.\")\n",
        "            break\n",
        "\n",
        "        print(f\"Processing OOF for Fold {fold_idx+1}/{min(K_FOLD_MAX_VALUE, K_FOLD_LIMIT)}...\")\n",
        "\n",
        "        fold_val_df = full_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        model_path = f\"models/{EXPERIMENT_NAME}_fold{fold_idx+1}_ft_model.pt\"\n",
        "        try:\n",
        "            # Initialize appropriate model\n",
        "            model = CustomNet(num_classes, FT_DROPOUT_RATE, backbone=NET_NAME).to(\n",
        "                device\n",
        "            )\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            classifier_head = model.backbone.backbone.classifier\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Skipping Fold {fold_idx+1} (Model not found)\")\n",
        "            continue\n",
        "\n",
        "        # Create Dataset/Loader\n",
        "        val_ds = MaskedGridTileDataset(\n",
        "            fold_val_df,\n",
        "            train_set_dir,\n",
        "            transforms=data_transforms,\n",
        "            target_size=IMG_RESIZE,\n",
        "            mask_threshold=MASK_THRESHOLD,\n",
        "            overlap_ratio=0.0,\n",
        "            normalize=True,\n",
        "        )\n",
        "        val_loader = make_loader(val_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Run Inference\n",
        "        slide_feats = {}  # pid -> list of embeddings\n",
        "        slide_probs = {}  # pid -> list of probabilities\n",
        "        slide_labels = {}  # { 'img_name': label_idx }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, parent_ids in tqdm(val_loader, desc=\"Predicting\", leave=False):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # --- TTA: Prepare Views ---\n",
        "                inputs_orig = inputs\n",
        "                inputs_h = torch.flip(inputs, [3])\n",
        "                inputs_v = torch.flip(inputs, [2])\n",
        "\n",
        "                # --- 1. Extract Embeddings for all views ---\n",
        "                # Shape: [Batch, D]\n",
        "                e1 = model.extract_embeddings(inputs_orig)\n",
        "                e2 = model.extract_embeddings(inputs_h)\n",
        "                e3 = model.extract_embeddings(inputs_v)\n",
        "\n",
        "                # Average the embeddings (Feature Stability)\n",
        "                avg_embeds = (e1 + e2 + e3) / 3.0\n",
        "\n",
        "                # --- 2. Compute Probabilities for all views ---\n",
        "                # We pass the specific embeddings to the classifier, then average the result\n",
        "                # Note: We use model.backbone.classifier because our custom classes wrap it there\n",
        "                p1 = F.softmax(classifier_head(e1), dim=1)\n",
        "                p2 = F.softmax(classifier_head(e2), dim=1)\n",
        "                p3 = F.softmax(classifier_head(e3), dim=1)\n",
        "\n",
        "                # Average the probabilities (Prediction Stability)\n",
        "                avg_probs = (p1 + p2 + p3) / 3.0\n",
        "\n",
        "                # Move to CPU\n",
        "                avg_embeds = avg_embeds.cpu().numpy()\n",
        "                avg_probs = avg_probs.cpu().numpy()\n",
        "                labels = labels.cpu().numpy()\n",
        "\n",
        "                for i, pid in enumerate(parent_ids):\n",
        "                    if pid not in slide_feats:\n",
        "                        slide_feats[pid] = []\n",
        "                        slide_probs[pid] = []\n",
        "                        slide_labels[pid] = labels[i]\n",
        "\n",
        "                    # Accumulate the Averaged TTA results\n",
        "                    slide_feats[pid].append(avg_embeds[i])\n",
        "                    slide_probs[pid].append(avg_probs[i])\n",
        "\n",
        "        # Feature Engineering per Slide\n",
        "        for pid in slide_feats.keys():\n",
        "            # A. Embedding Features (Matrix: N_patches x 1024)\n",
        "            E_mat = np.array(slide_feats[pid])\n",
        "            feat_emb_mean = np.mean(E_mat, axis=0)\n",
        "            feat_emb_max = np.max(E_mat, axis=0)\n",
        "\n",
        "            # B. Probability Features (Matrix: N_patches x 4)\n",
        "            P_mat = np.array(slide_probs[pid])\n",
        "\n",
        "            # Re-use your existing feature extraction logic for probs\n",
        "            # (Mean, Max, Std, Top-K, etc.)\n",
        "            feat_prob_stats = extract_features_from_probs(P_mat)\n",
        "\n",
        "            # C. Concatenate: [Embed_Mean, Embed_Max, Prob_Stats]\n",
        "            final_vec = np.concatenate([feat_emb_mean, feat_emb_max, feat_prob_stats])\n",
        "\n",
        "            X_list.append(final_vec)\n",
        "            y_list.append(slide_labels[pid])\n",
        "\n",
        "    # Convert to Arrays\n",
        "    X_stack = np.array(X_list)\n",
        "    y_stack = np.array(y_list)\n",
        "    return X_stack, y_stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "c8ed9dfa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Generating Hybrid (Embeddings + Prob Stats) Dataset...\n",
            "Processing OOF for Fold 1/3...\n",
            "Processing 117 slides with Grid Tiling (Thr=0.01, Overlap=0.0)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4478b62388944719332529369c4210d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/117 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 461\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db05946d1c294ef58ee8c05105d74fde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Predicting:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing OOF for Fold 2/3...\n",
            "Processing 116 slides with Grid Tiling (Thr=0.01, Overlap=0.0)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9f912feffc147d7bd1bf4a7c0396a6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/116 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 411\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d043d2a018584f1cafbb286effeb38f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Predicting:   0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing OOF for Fold 3/3...\n",
            "Processing 116 slides with Grid Tiling (Thr=0.01, Overlap=0.0)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc262c09869d46c6976d7b53cc4fa139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/116 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 447\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bd51c510138477d8d6ae60dff2db742",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Predicting:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reached K_FOLD_LIMIT of 3. Stopping further folds.\n",
            "OOF Dataset Created. Shape: (349, 2072)\n"
          ]
        }
      ],
      "source": [
        "X_stack, y_stack = generate_hybrid_dataset(full_df)\n",
        "print(f\"OOF Dataset Created. Shape: {X_stack.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "e292bc55",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def apply_pca(pca_n_components=0.95, X_stack=X_stack, n_embeddings=2048):\n",
        "    \"\"\"\n",
        "    Applies PCA to reduce dimensionality of the hybrid dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the data\n",
        "    X_embeds = X_stack[:, :n_embeddings] \n",
        "    X_stats = X_stack[:, n_embeddings:]\n",
        "\n",
        "    # Standardize Embeddings (Required for PCA)\n",
        "    scaler = StandardScaler()\n",
        "    X_embeds_scaled = scaler.fit_transform(X_embeds)\n",
        "\n",
        "    pca = PCA(n_components=pca_n_components, random_state=SEED)\n",
        "    X_embeds_pca = pca.fit_transform(X_embeds_scaled)\n",
        "\n",
        "    print(f\"PCA reduced embeddings from {X_embeds.shape[1]} to {X_embeds_pca.shape[1]}\")\n",
        "\n",
        "    # Re-concatenate: [Compact Embeddings] + [Original Stats]\n",
        "    X_stack_optimized = np.concatenate([X_embeds_pca, X_stats], axis=1)\n",
        "\n",
        "    print(f\"New Feature Vector Shape: {X_stack_optimized.shape}\")\n",
        "\n",
        "    return X_stack_optimized, pca, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "de28b414",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Embedding Features: 2048\n",
            "PCA reduced embeddings from 2048 to 125\n",
            "New Feature Vector Shape: (349, 149)\n"
          ]
        }
      ],
      "source": [
        "dummy_probs = np.zeros((10, num_classes))  # 10 dummy patches, 4 classes\n",
        "dummy_stats_vec = extract_features_from_probs(dummy_probs)\n",
        "n_stats_features = len(dummy_stats_vec)\n",
        "\n",
        "# The rest of the columns MUST be the embeddings\n",
        "n_embed_features = X_stack.shape[1] - n_stats_features\n",
        "print(f\"Number of Embedding Features: {n_embed_features}\")\n",
        "\n",
        "X_stack, pca_model, scaler_model = apply_pca(\n",
        "    pca_n_components=0.9, X_stack=X_stack, n_embeddings=n_embed_features\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e64fd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "\n",
        "def grid_search_xgboost(X_stack, y_stack):\n",
        "    print(\">>> Starting XGBoost Hyperparameter Grid Search...\")\n",
        "    max_folds = 3\n",
        "\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [400, 800],\n",
        "        \"max_depth\": [5, 7],\n",
        "        \"learning_rate\": [0.1, 0.01],\n",
        "        \"colsample_bytree\": [0.9],\n",
        "        \"subsample\": [0.5],\n",
        "    }\n",
        "\n",
        "    best_params = None\n",
        "    best_score = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "    for params in product(*param_grid.values()):\n",
        "        param_dict = dict(zip(param_grid.keys(), params))\n",
        "\n",
        "        fold_scores = []\n",
        "        fold_models = []\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_stack, y_stack)):\n",
        "            if fold_idx >= max_folds:\n",
        "                break\n",
        "            X_train, X_val = X_stack[train_idx], X_stack[val_idx]\n",
        "            y_train, y_val = y_stack[train_idx], y_stack[val_idx]\n",
        "\n",
        "            sample_weights_train = compute_sample_weight(\n",
        "                class_weight=\"balanced\", y=y_train\n",
        "            )\n",
        "\n",
        "            model = xgb.XGBClassifier(\n",
        "                objective=\"multi:softmax\",\n",
        "                num_class=num_classes,\n",
        "                random_state=SEED,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=1.3,\n",
        "                early_stopping_rounds=100,\n",
        "                **param_dict,\n",
        "            )\n",
        "            model.fit(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                sample_weight=sample_weights_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False,\n",
        "            )\n",
        "            preds = model.predict(X_val)\n",
        "            fold_scores.append(f1_score(y_val, preds, average=\"macro\"))\n",
        "            fold_models.append(model)\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        print(f\"Params: {param_dict} => Mean F1 Score: {mean_score:.4f}\")\n",
        "        if mean_score > best_score:\n",
        "            best_score = mean_score\n",
        "            best_params = param_dict\n",
        "            best_model = fold_models[int(np.argmax(fold_scores))]\n",
        "\n",
        "    print(f\"Best F1 Score: {best_score:.4f} with params: {best_params}\")\n",
        "    return best_model, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "6011a548",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Starting XGBoost Hyperparameter Grid Search...\n",
            "Params: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 1.0, 'subsample': 0.5} => Mean F1 Score: 0.3179\n",
            "Params: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0, 'subsample': 0.5} => Mean F1 Score: 0.3530\n",
            "Best F1 Score: 0.3530 with params: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0, 'subsample': 0.5}\n",
            "XGBoost Grid Search Complete. Best Model Obtained.\n",
            "Best Hyperparameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 1.0, 'subsample': 0.5}\n"
          ]
        }
      ],
      "source": [
        "best_model, best_params = grid_search_xgboost(X_stack, y_stack)\n",
        "print(\"XGBoost Grid Search Complete. Best Model Obtained.\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4578c00c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_xgboost_submission(test_loader, cnn_exp_name, xgb_model, scaler, pca, n_embed_features):\n",
        "    print(\"--- Running Inference with XGBoost Stacking (Vote of Experts)...\")\n",
        "\n",
        "    # Storage for FINAL XGBoost predictions per slide\n",
        "    # { 'slide_id': [xgb_prob_fold1, xgb_prob_fold2, ...] }\n",
        "    slide_final_preds = {}\n",
        "\n",
        "    # 2. Iterate Folds: Run CNN -> Extract Features -> Predict XGB -> Store\n",
        "    max_fold = min(K_FOLD_MAX_VALUE, K_FOLD_LIMIT)\n",
        "    for fold in range(1, max_fold + 1):\n",
        "        model_path = f\"models/{cnn_exp_name}_fold{fold}_ft_model.pt\"\n",
        "        print(f\"\\n--- Processing Fold {fold}/{max_fold} ---\")\n",
        "\n",
        "        # Load CNN\n",
        "        try:\n",
        "            model = CustomNet(num_classes, FT_DROPOUT_RATE, backbone=NET_NAME).to(\n",
        "                device\n",
        "            )\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            classifier_head = model.backbone.backbone.classifier\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"WARNING: Skipping Fold {fold} (Model not found)\")\n",
        "            continue\n",
        "\n",
        "        # Dictionary to collect patches for this SPECIFIC fold\n",
        "        slide_feats = {}  # { 'pid': [ [p0..p3], [p0..p3] ] }\n",
        "        slide_probs = {}  # pid -> list of probabilities\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, _, parent_ids in tqdm(\n",
        "                test_loader, desc=f\"Inference Fold {fold}\", leave=False\n",
        "            ):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # --- TTA: Prepare Views ---\n",
        "                inputs_orig = inputs\n",
        "                inputs_h = torch.flip(inputs, [3])\n",
        "                inputs_v = torch.flip(inputs, [2])\n",
        "\n",
        "                # --- 1. Extract Embeddings for all views ---\n",
        "                # Shape: [Batch, D]\n",
        "                e1 = model.extract_embeddings(inputs_orig)\n",
        "                e2 = model.extract_embeddings(inputs_h)\n",
        "                e3 = model.extract_embeddings(inputs_v)\n",
        "\n",
        "                # Average the embeddings (Feature Stability)\n",
        "                avg_embeds = (e1 + e2 + e3) / 3.0\n",
        "\n",
        "                # --- 2. Compute Probabilities for all views ---\n",
        "                # We pass the specific embeddings to the classifier, then average the result\n",
        "                # Note: We use model.backbone.classifier because our custom classes wrap it there\n",
        "                p1 = F.softmax(classifier_head(e1), dim=1)\n",
        "                p2 = F.softmax(classifier_head(e2), dim=1)\n",
        "                p3 = F.softmax(classifier_head(e3), dim=1)\n",
        "\n",
        "                # Average the probabilities (Prediction Stability)\n",
        "                avg_probs = (p1 + p2 + p3) / 3.0\n",
        "\n",
        "                # Move to CPU\n",
        "                avg_embeds = avg_embeds.cpu().numpy()\n",
        "                avg_probs = avg_probs.cpu().numpy()\n",
        "\n",
        "                # Collect patches for this fold\n",
        "                for i, pid in enumerate(parent_ids):\n",
        "                    if pid not in slide_feats:\n",
        "                        slide_feats[pid] = []\n",
        "                        slide_probs[pid] = []\n",
        "\n",
        "                    # Accumulate the Averaged TTA results\n",
        "                    slide_feats[pid].append(avg_embeds[i])\n",
        "                    slide_probs[pid].append(avg_probs[i])\n",
        "\n",
        "        # 3. Feature Engineer & Predict with XGBoost for THIS fold\n",
        "        print(f\"XGBoost Prediction for Fold {fold}...\")\n",
        "        for pid in slide_feats.keys():\n",
        "            # A. Embedding Features (Matrix: N_patches x 1024)\n",
        "            E_mat = np.array(slide_feats[pid])\n",
        "            feat_emb_mean = np.mean(E_mat, axis=0)\n",
        "            feat_emb_max = np.max(E_mat, axis=0)\n",
        "\n",
        "            # B. Probability Features (Matrix: N_patches x 4)\n",
        "            P_mat = np.array(slide_probs[pid])\n",
        "\n",
        "            # Re-use your existing feature extraction logic for probs\n",
        "            # (Mean, Max, Std, Top-K, etc.)\n",
        "            feat_prob_stats = extract_features_from_probs(P_mat)\n",
        "\n",
        "            # C. Concatenate: [Embed_Mean, Embed_Max, Prob_Stats]\n",
        "            final_vec = np.concatenate([feat_emb_mean, feat_emb_max, feat_prob_stats])\n",
        "\n",
        "            vec_embed = final_vec[:n_embed_features].reshape(1, -1)\n",
        "            vec_stats = final_vec[n_embed_features:].reshape(1, -1)\n",
        "\n",
        "            vec_embed_scaled = scaler.transform(vec_embed)\n",
        "            vec_embed_pca = pca.transform(vec_embed_scaled)\n",
        "            \n",
        "            final_vec_reduced = np.concatenate([vec_embed_pca, vec_stats], axis=1)\n",
        "\n",
        "            # Predict using XGBoost\n",
        "            # We use predict_proba to get soft voting capability for the stacker\n",
        "            # Shape: (1, n_classes)\n",
        "            xgb_prob = xgb_model.predict_proba(final_vec_reduced)[0]\n",
        "\n",
        "            if pid not in slide_final_preds:\n",
        "                slide_final_preds[pid] = []\n",
        "            slide_final_preds[pid].append(xgb_prob)\n",
        "\n",
        "    # 4. Average XGBoost Predictions (Soft Voting of Stackers)\n",
        "    final_rows = []\n",
        "    print(\"\\nAggregating Ensemble Predictions...\")\n",
        "\n",
        "    for pid, preds_list in slide_final_preds.items():\n",
        "        # preds_list is a list of arrays (one per fold)\n",
        "        # Average them\n",
        "        avg_xgb_probs = np.mean(preds_list, axis=0)\n",
        "\n",
        "        # Final Argmax\n",
        "        pred_idx = np.argmax(avg_xgb_probs)\n",
        "        pred_label = class_names[pred_idx]\n",
        "\n",
        "        final_rows.append({\"sample_index\": pid, \"label\": pred_label})\n",
        "\n",
        "    # Save\n",
        "    sub = pd.DataFrame(final_rows).sort_values(\"sample_index\")\n",
        "    print(\"Submission DataFrame created\")\n",
        "    return sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10df074",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 477 slides with Grid Tiling (Thr=0.1, Overlap=0.5)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "074a4688d0e247c9aae3ad0b795dcf69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/477 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_ds = \u001b[43mMaskedGridTileDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_set_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_RESIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMASK_TRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverlap_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m test_loader = make_loader(test_ds, BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mMaskedGridTileDataset.__init__\u001b[39m\u001b[34m(self, dataframe, img_dir, transforms, target_size, mask_threshold, overlap_ratio, normalize, debug_max)\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m debug_max \u001b[38;5;129;01mand\u001b[39;00m count >= debug_max:\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     count += \u001b[32m1\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtraction complete. Total patches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mMaskedGridTileDataset._process_and_extract\u001b[39m\u001b[34m(self, img_name, label)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.normalizer:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    148\u001b[39m         \u001b[38;5;66;03m# Macenko needs robust pixel data. If patch is mostly white/padding,\u001b[39;00m\n\u001b[32m    149\u001b[39m         \u001b[38;5;66;03m# it might fail or look weird. We only run it if we have tissue.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         patch_arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m         \u001b[38;5;66;03m# Fallback if SVD fails on weird patches\u001b[39;00m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    154\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNormalization failed on patch from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    155\u001b[39m         )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mMacenkoNormalizer.__call__\u001b[39m\u001b[34m(self, img_arr, Io, alpha, beta)\u001b[39m\n\u001b[32m     33\u001b[39m _, eigvecs = np.linalg.eigh(np.cov(ODhat.T))\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 4. Project on the plane spanned by the eigenvectors corresponding to the two largest eigenvalues\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m That = \u001b[43mODhat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigvecs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 5. Find robust extremes (1st and 99th percentiles)\u001b[39;00m\n\u001b[32m     39\u001b[39m phi = np.arctan2(That[:, \u001b[32m1\u001b[39m], That[:, \u001b[32m0\u001b[39m])\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "test_ds = MaskedGridTileDataset(\n",
        "    dataframe=None,\n",
        "    img_dir=test_set_dir,\n",
        "    transforms=data_transforms,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=MASK_THRESHOLD,\n",
        "    overlap_ratio=0.5,\n",
        "    normalize=True,\n",
        ")\n",
        "\n",
        "test_loader = make_loader(test_ds, BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6163117",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXECUTE\n",
        "submission_df = generate_xgboost_submission(\n",
        "    test_loader=test_loader,\n",
        "    cnn_exp_name=EXPERIMENT_NAME,\n",
        "    xgb_model=best_model,\n",
        "    pca=pca_model,\n",
        "    scaler=scaler_model,\n",
        "    n_embed_features=n_embed_features,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c35b187",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"submission\", exist_ok=True)\n",
        "\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"‚úÖ Saved Robust Stacking Submission: {submission_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdca857",
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a14688d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
