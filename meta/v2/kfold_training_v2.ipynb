{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a75bb6ad",
      "metadata": {},
      "source": [
        "# **K-Fold training**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900b0225",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enviroment\n",
        "isColab = False\n",
        "colab_dir = \"/gdrive/My Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "\n",
        "isKaggle = False\n",
        "isWsl = True\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 46"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4cb00e",
      "metadata": {},
      "source": [
        "## **Loading Enviroment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09b8c7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory di default\n",
        "current_dir = os.getcwd()   \n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive # type: ignore\n",
        "    drive.mount(\"/gdrive\")\n",
        "    current_dir = colab_dir\n",
        "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
        "    %cd $current_dir\n",
        "elif isKaggle:\n",
        "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
        "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
        "    current_dir = kaggle_work_dir\n",
        "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
        "    os.chdir(current_dir)\n",
        "elif isWsl:\n",
        "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "else:\n",
        "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
        "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "\n",
        "print(f\"Changed directory to: {current_dir}\")\n",
        "\n",
        "# Define absolute paths\n",
        "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
        "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
        "\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Train set directory: {train_set_dir}\")\n",
        "print(f\"Test set directory: {test_set_dir}\")\n",
        "print(f\"Label file: {label_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e0fdc8",
      "metadata": {},
      "source": [
        "## **Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be49326d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49326d",
        "outputId": "59e423d7-35ea-4fc1-933c-affb303ee9f5"
      },
      "outputs": [],
      "source": [
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "if isColab or isKaggle:\n",
        "    !pkill -f tensorboard \n",
        "    !mkdir -p models\n",
        "    print(\"Killed existing TensorBoard instances and created models directory.\") \n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)  \n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import  DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcbf293",
      "metadata": {},
      "source": [
        "### **Preparing Dataset for colab**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f17001",
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    drive_dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "    local_dataset_dir = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(local_dataset_dir):\n",
        "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
        "        try:\n",
        "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
        "            print(\"Copy complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying dataset: {e}\")\n",
        "            print(\"Falling back to Drive dataset (slow).\")\n",
        "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
        "            dataset_dir = drive_dataset_dir\n",
        "    else:\n",
        "        print(\"Dataset already copied to local runtime.\")\n",
        "\n",
        "    # If copy succeeded (or already existed), use local path\n",
        "    if os.path.exists(local_dataset_dir):\n",
        "        dataset_dir = local_dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfe95f2",
      "metadata": {},
      "source": [
        "## ‚è≥ **Data Loading**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc4301c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loader parameters\n",
        "BATCH_SIZE = 128\n",
        "GRAD_ACCUMULATION_STEPS = 1\n",
        "\n",
        "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZATION_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "MASK_THRESHOLD = 0.01\n",
        "OVERLAP_RATIO = 0.3\n",
        "\n",
        "IMG_RESIZE = (224, 224)\n",
        "INPUT_SHAPE = (3, *IMG_RESIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e112e14",
      "metadata": {},
      "source": [
        "### **Definitions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d945105",
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_IGNORE = [\n",
        "    \"img_0001.png\",\n",
        "    \"img_0005.png\",\n",
        "    \"img_0008.png\",\n",
        "    \"img_0012.png\",\n",
        "    \"img_0018.png\",\n",
        "    \"img_0020.png\",\n",
        "    \"img_0022.png\",\n",
        "    \"img_0027.png\",\n",
        "    \"img_0028.png\",\n",
        "    \"img_0036.png\",\n",
        "    \"img_0044.png\",\n",
        "    \"img_0047.png\",\n",
        "    \"img_0048.png\",\n",
        "    \"img_0052.png\",\n",
        "    \"img_0062.png\",\n",
        "    \"img_0078.png\",\n",
        "    \"img_0085.png\",\n",
        "    \"img_0090.png\",\n",
        "    \"img_0094.png\",\n",
        "    \"img_0095.png\",\n",
        "    \"img_0126.png\",\n",
        "    \"img_0129.png\",\n",
        "    \"img_0130.png\",\n",
        "    \"img_0133.png\",\n",
        "    \"img_0136.png\",\n",
        "    \"img_0138.png\",\n",
        "    \"img_0148.png\",\n",
        "    \"img_0150.png\",\n",
        "    \"img_0155.png\",\n",
        "    \"img_0159.png\",\n",
        "    \"img_0161.png\",\n",
        "    \"img_0175.png\",\n",
        "    \"img_0178.png\",\n",
        "    \"img_0179.png\",\n",
        "    \"img_0180.png\",\n",
        "    \"img_0184.png\",\n",
        "    \"img_0187.png\",\n",
        "    \"img_0189.png\",\n",
        "    \"img_0193.png\",\n",
        "    \"img_0196.png\",\n",
        "    \"img_0222.png\",\n",
        "    \"img_0251.png\",\n",
        "    \"img_0254.png\",\n",
        "    \"img_0263.png\",\n",
        "    \"img_0268.png\",\n",
        "    \"img_0286.png\",\n",
        "    \"img_0293.png\",\n",
        "    \"img_0313.png\",\n",
        "    \"img_0319.png\",\n",
        "    \"img_0333.png\",\n",
        "    \"img_0342.png\",\n",
        "    \"img_0344.png\",\n",
        "    \"img_0346.png\",\n",
        "    \"img_0355.png\",\n",
        "    \"img_0368.png\",\n",
        "    \"img_0371.png\",\n",
        "    \"img_0376.png\",\n",
        "    \"img_0380.png\",\n",
        "    \"img_0390.png\",\n",
        "    \"img_0393.png\",\n",
        "    \"img_0407.png\",\n",
        "    \"img_0410.png\",\n",
        "    \"img_0415.png\",\n",
        "    \"img_0424.png\",\n",
        "    \"img_0443.png\",\n",
        "    \"img_0453.png\",\n",
        "    \"img_0459.png\",\n",
        "    \"img_0463.png\",\n",
        "    \"img_0486.png\",\n",
        "    \"img_0497.png\",\n",
        "    \"img_0498.png\",\n",
        "    \"img_0499.png\",\n",
        "    \"img_0509.png\",\n",
        "    \"img_0521.png\",\n",
        "    \"img_0530.png\",\n",
        "    \"img_0531.png\",\n",
        "    \"img_0533.png\",\n",
        "    \"img_0537.png\",\n",
        "    \"img_0540.png\",\n",
        "    \"img_0544.png\",\n",
        "    \"img_0547.png\",\n",
        "    \"img_0557.png\",\n",
        "    \"img_0558.png\",\n",
        "    \"img_0560.png\",\n",
        "    \"img_0565.png\",\n",
        "    \"img_0567.png\",\n",
        "    \"img_0572.png\",\n",
        "    \"img_0578.png\",\n",
        "    \"img_0580.png\",\n",
        "    \"img_0586.png\",\n",
        "    \"img_0602.png\",\n",
        "    \"img_0603.png\",\n",
        "    \"img_0607.png\",\n",
        "    \"img_0609.png\",\n",
        "    \"img_0614.png\",\n",
        "    \"img_0620.png\",\n",
        "    \"img_0623.png\",\n",
        "    \"img_0629.png\",\n",
        "    \"img_0635.png\",\n",
        "    \"img_0639.png\",\n",
        "    \"img_0643.png\",\n",
        "    \"img_0644.png\",\n",
        "    \"img_0645.png\",\n",
        "    \"img_0646.png\",\n",
        "    \"img_0656.png\",\n",
        "    \"img_0657.png\",\n",
        "    \"img_0658.png\",\n",
        "    \"img_0670.png\",\n",
        "    \"img_0673.png\",\n",
        "    \"img_0675.png\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7afdb353",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the full dataframe\n",
        "full_df = pd.read_csv(label_file)\n",
        "\n",
        "# Remove cursed images\n",
        "full_df = full_df[~full_df[\"sample_index\"].isin(SAMPLES_TO_IGNORE)].reset_index(\n",
        "    drop=True\n",
        ")\n",
        "\n",
        "# Label mapping\n",
        "class_names = sorted(full_df[\"label\"].unique())\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "full_df[\"label_index\"] = full_df[\"label\"].map(label_to_index)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e13a32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last=False):\n",
        "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(6, cpu_cores))\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,\n",
        "        persistent_workers=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8155cb2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MacenkoNormalizer:\n",
        "    \"\"\"\n",
        "    Normalizes H&E stained images to a reference appearance using the Macenko method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Target reference values (standard for CPath)\n",
        "        # These are pre-computed means/stds from a \"perfect\" slide\n",
        "        self.HERef = np.array([[0.5626, 0.2159], [0.7201, 0.8012], [0.4062, 0.5581]])\n",
        "        self.maxCRef = np.array([1.9705, 1.0308])\n",
        "\n",
        "    def __call__(self, img_arr, Io=240, alpha=1, beta=0.15):\n",
        "        \"\"\"\n",
        "        img_arr: RGB numpy array (H, W, 3)\n",
        "        Returns: Normalized numpy array\n",
        "        \"\"\"\n",
        "        # 1. Convert to Optical Density (OD)\n",
        "        h, w, c = img_arr.shape\n",
        "        img_arr = img_arr.reshape((-1, 3))\n",
        "\n",
        "        # Avoid division by zero\n",
        "        OD = -np.log((img_arr.astype(np.float64) + 1) / Io)\n",
        "\n",
        "        # 2. Remove transparent pixels\n",
        "        ODhat = OD[~np.any(OD < beta, axis=1)]\n",
        "        if ODhat.shape[0] < 10:  # Safety check for empty patches\n",
        "            return img_arr.reshape(h, w, c).copy()\n",
        "\n",
        "        # 3. Compute SVD\n",
        "        _, eigvecs = np.linalg.eigh(np.cov(ODhat.T))\n",
        "\n",
        "        # 4. Project on the plane spanned by the eigenvectors corresponding to the two largest eigenvalues\n",
        "        That = ODhat.dot(eigvecs[:, -2:])\n",
        "\n",
        "        # 5. Find robust extremes (1st and 99th percentiles)\n",
        "        phi = np.arctan2(That[:, 1], That[:, 0])\n",
        "        minPhi = np.percentile(phi, alpha)\n",
        "        maxPhi = np.percentile(phi, 100 - alpha)\n",
        "\n",
        "        vMin = eigvecs[:, -2:].dot(np.array([(np.cos(minPhi), np.sin(minPhi))]).T)\n",
        "        vMax = eigvecs[:, -2:].dot(np.array([(np.cos(maxPhi), np.sin(maxPhi))]).T)\n",
        "\n",
        "        # 6. Heuristic to ensure H is first vector, E is second\n",
        "        if vMin[0] > vMax[0]:\n",
        "            HE = np.array((vMin[:, 0], vMax[:, 0])).T\n",
        "        else:\n",
        "            HE = np.array((vMax[:, 0], vMin[:, 0])).T\n",
        "\n",
        "        # 7. Rows correspond to channels (RGB), columns to H&E stains\n",
        "        Y = np.reshape(OD, (-1, 3)).T\n",
        "\n",
        "        # Determine concentrations of the individual stains\n",
        "        C = np.linalg.lstsq(HE, Y, rcond=None)[0]\n",
        "\n",
        "        # 8. Normalize concentrations\n",
        "        maxC = np.percentile(C, 99, axis=1)\n",
        "        tmp = np.divide(maxC, self.maxCRef)\n",
        "        C2 = np.divide(C, tmp[:, np.newaxis])\n",
        "\n",
        "        # 9. Reconstruct the image\n",
        "        Inorm = np.multiply(Io, np.exp(-self.HERef.dot(C2)))\n",
        "        Inorm[Inorm > 255] = 254\n",
        "        Inorm = np.reshape(Inorm.T, (h, w, 3)).astype(np.uint8)\n",
        "\n",
        "        return Inorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec50e55",
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageOps\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "class MaskedGridTileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset class that performs Grid Tiling over the tissue masks.\n",
        "    It extracts patches based on a sliding window, keeping only those\n",
        "    that contain sufficient biological tissue.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataframe,\n",
        "        img_dir,\n",
        "        transforms=None,\n",
        "        target_size=(300, 300),\n",
        "        mask_threshold=0.05,  # Keep patch if at least 5% is tissue\n",
        "        overlap_ratio=0.0,  # 0.0 = distinct tiles; 0.5 = 50% overlap\n",
        "        normalize=True,\n",
        "        debug_max=None,\n",
        "    ):\n",
        "        self.samples = []\n",
        "        self.transforms = transforms\n",
        "        self.img_dir = img_dir\n",
        "        self.target_size = target_size\n",
        "        self.mask_threshold = mask_threshold\n",
        "        self.overlap_ratio = overlap_ratio\n",
        "        self.normalizer = MacenkoNormalizer() if normalize else None\n",
        "        self.dropped_slides = 0\n",
        "\n",
        "        # Determine if we are in inference mode (no labels)\n",
        "        if dataframe is None or \"label_index\" not in dataframe.columns:\n",
        "            # We are in inference mode\n",
        "            if dataframe is None:\n",
        "                img_names = sorted(\n",
        "                    [f for f in os.listdir(img_dir) if f.startswith(\"img_\")]\n",
        "                )\n",
        "            else:\n",
        "                img_names = dataframe[\"sample_index\"].tolist()\n",
        "            iterator = zip(img_names, [-1] * len(img_names))\n",
        "            total_items = len(img_names)\n",
        "        else:\n",
        "            iterator = zip(dataframe[\"sample_index\"], dataframe[\"label_index\"])\n",
        "            total_items = len(dataframe)\n",
        "\n",
        "        print(\n",
        "            f\"Processing {total_items} slides with Grid Tiling (Thr={mask_threshold}, Overlap={overlap_ratio})...\"\n",
        "        )\n",
        "\n",
        "        count = 0\n",
        "        for img_name, label in tqdm(iterator, total=total_items, leave=False):\n",
        "            if debug_max and count >= debug_max:\n",
        "                break\n",
        "            self._process_and_extract(img_name, label)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Extraction complete. Total patches: {len(self.samples)}\")\n",
        "        if self.dropped_slides > 0:\n",
        "            print(\n",
        "                f\"‚ö†Ô∏è Warning: {self.dropped_slides} slides were completely empty/corrupt.\"\n",
        "            )\n",
        "\n",
        "    def _process_and_extract(self, img_name, label):\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask = Image.open(mask_path).convert(\"L\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load {img_name}: {e}\")\n",
        "            self.dropped_slides += 1\n",
        "            return\n",
        "\n",
        "        img_w, img_h = image.size\n",
        "        tile_h, tile_w = self.target_size\n",
        "\n",
        "        # Calculate stride based on overlap\n",
        "        stride_h = int(tile_h * (1 - self.overlap_ratio))\n",
        "        stride_w = int(tile_w * (1 - self.overlap_ratio))\n",
        "\n",
        "        # Ensure stride is at least 1 pixel to prevent infinite loops\n",
        "        stride_h = max(1, stride_h)\n",
        "        stride_w = max(1, stride_w)\n",
        "\n",
        "        mask_arr = np.array(mask)\n",
        "        # Binarize mask (assuming any non-zero value is tissue)\n",
        "        binary_mask = (mask_arr > 0).astype(np.uint8)\n",
        "\n",
        "        # Iterate via sliding window\n",
        "        # We allow y and x to go slightly out of bounds to catch edges, handled by padding later\n",
        "        for y in range(0, img_h, stride_h):\n",
        "            for x in range(0, img_w, stride_w):\n",
        "\n",
        "                # Define current window coordinates\n",
        "                x2 = x + tile_w\n",
        "                y2 = y + tile_h\n",
        "\n",
        "                # --- 1. MASK CHECK (Fast filtering) ---\n",
        "                # Clip coordinates to image bounds for the mask check\n",
        "                mx1, my1 = x, y\n",
        "                mx2, my2 = min(x2, img_w), min(y2, img_h)\n",
        "\n",
        "                # Extract the mask patch corresponding to this tile\n",
        "                mask_patch = binary_mask[my1:my2, mx1:mx2]\n",
        "\n",
        "                # If the patch is empty or purely padding, skip\n",
        "                if mask_patch.size == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate tissue percentage\n",
        "                tissue_pixels = np.count_nonzero(mask_patch)\n",
        "                total_pixels = tile_w * tile_h  # Use theoretical size\n",
        "\n",
        "                # If we are on the edge, the actual mask_patch might be smaller,\n",
        "                # but we normalize by the Target Tile Size to penalize mostly-empty edge crops.\n",
        "                tissue_coverage = tissue_pixels / total_pixels\n",
        "\n",
        "                if tissue_coverage < self.mask_threshold:\n",
        "                    continue\n",
        "\n",
        "                # --- 2. IMAGE EXTRACTION (Only if mask check passed) ---\n",
        "                # Handle Edge Padding\n",
        "                # If x2 > img_w or y2 > img_h, we need to crop what we can and pad the rest\n",
        "\n",
        "                # Actual crop coordinates within image\n",
        "                crop_x1, crop_y1 = x, y\n",
        "                crop_x2, crop_y2 = min(x2, img_w), min(y2, img_h)\n",
        "\n",
        "                patch_crop = image.crop((crop_x1, crop_y1, crop_x2, crop_y2))\n",
        "\n",
        "                # Calculate padding needed\n",
        "                pad_right = max(0, x2 - img_w)\n",
        "                pad_bottom = max(0, y2 - img_h)\n",
        "\n",
        "                if pad_right > 0 or pad_bottom > 0:\n",
        "                    # Pad with white (255) for histology background\n",
        "                    patch = ImageOps.expand(\n",
        "                        patch_crop, border=(0, 0, pad_right, pad_bottom), fill=255\n",
        "                    )\n",
        "                else:\n",
        "                    patch = patch_crop\n",
        "\n",
        "                # Final safeguard on size\n",
        "                if patch.size != self.target_size:\n",
        "                    patch = patch.resize(self.target_size, Image.BICUBIC) # type: ignore\n",
        "\n",
        "                patch_arr = np.array(patch)\n",
        "\n",
        "                if self.normalizer:\n",
        "                    try:\n",
        "                        # Macenko needs robust pixel data. If patch is mostly white/padding,\n",
        "                        # it might fail or look weird. We only run it if we have tissue.\n",
        "                        patch_arr = self.normalizer(patch_arr)\n",
        "                    except Exception as e:\n",
        "                        # Fallback if SVD fails on weird patches\n",
        "                        print(f\"Normalization failed on patch from {img_name} at ({x},{y}): {e}\")\n",
        "                        pass\n",
        "\n",
        "                # Store in RAM\n",
        "                self.samples.append(\n",
        "                    {\"patch\": patch_arr, \"label\": label, \"parent\": img_name}\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        img = Image.fromarray(item[\"patch\"])\n",
        "        label = item[\"label\"]\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, label, item[\"parent\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8606806",
      "metadata": {},
      "source": [
        "### **Recomputing mean and std for normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147c9151",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "\n",
        "def compute_dataset_stats(\n",
        "    dataset_class,\n",
        "    dataframe,\n",
        "    img_dir,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=0.05,\n",
        "    overlap_ratio=0.0,\n",
        "    normalize=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes channel-wise Mean and Std on the dataset without any normalization applied.\n",
        "    \"\"\"\n",
        "    print(\"Computing dataset Mean and Std (this may take a moment)...\")\n",
        "\n",
        "    # define a simple transform that only converts to tensor\n",
        "    basic_transforms = transforms.Compose(\n",
        "        [transforms.Resize(target_size), transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    # Instantiate dataset temporarily\n",
        "    temp_ds = dataset_class(\n",
        "        dataframe,\n",
        "        img_dir,\n",
        "        transforms=basic_transforms,\n",
        "        target_size=target_size,\n",
        "        mask_threshold=mask_threshold,\n",
        "        overlap_ratio=overlap_ratio,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    loader = make_loader(temp_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    nb_samples = 0.0\n",
        "\n",
        "    for data, _, _ in tqdm(loader):\n",
        "        batch_samples = data.size(0)\n",
        "        # Flatten H and W to calculate stats per channel\n",
        "        data = data.view(batch_samples, data.size(1), -1)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        nb_samples += batch_samples\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    print(f\"\\nDONE. Copy these values into your config:\")\n",
        "    print(f\"NEW_MEAN = {mean.tolist()}\")  # type: ignore\n",
        "    print(f\"NEW_STD = {std.tolist()}\")  # type: ignore\n",
        "    return mean.tolist(), std.tolist()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cadb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Calculating stats on Training Data...\")\n",
        "\n",
        "# We use the class we just defined\n",
        "custom_mean, custom_std = compute_dataset_stats(\n",
        "    dataset_class=MaskedGridTileDataset,\n",
        "    dataframe=full_df,\n",
        "    img_dir=train_set_dir,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=MASK_THRESHOLD,\n",
        "    overlap_ratio=0.0,\n",
        "    normalize=True,\n",
        ")\n",
        "\n",
        "NORMALIZATION_MEAN = custom_mean\n",
        "NORMALIZATION_STD = custom_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378879f6",
      "metadata": {},
      "source": [
        "### **Transforms**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a69ba8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "\n",
        "train_transform_tl = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transform_ft = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(\n",
        "            IMG_RESIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2), antialias=True\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=180),  # type: ignore\n",
        "        transforms.RandomApply(\n",
        "            [transforms.ElasticTransform(alpha=50.0, sigma=5.0)], p=0.25\n",
        "        ),\n",
        "        transforms.RandomApply(\n",
        "            [transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2.0))], p=0.2\n",
        "        ),\n",
        "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
        "        #transforms.RandAugment(num_ops=2, magnitude=5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "        #transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=\"random\"  # type: ignore),\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd852fa",
      "metadata": {},
      "source": [
        "### **A couple of visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e9e5a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visualize_dataset_samples(dataset, num_samples=5, class_names=None):\n",
        "    \"\"\"\n",
        "    Visualizes random samples from the dataset to verify:\n",
        "    1. Tissue masking (patches should contain tissue, not just white background)\n",
        "    2. Normalization/Augmentation (images should look valid)\n",
        "\n",
        "    Args:\n",
        "        dataset: The PyTorch Dataset object\n",
        "        num_samples: Number of images to display\n",
        "        class_names: List of class names strings (optional) to map label indices\n",
        "    \"\"\"\n",
        "    # Create plot\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 4))\n",
        "\n",
        "    # Pick random indices\n",
        "    if len(dataset) < num_samples:\n",
        "        indices = range(len(dataset))\n",
        "    else:\n",
        "        indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        # 1. Get Data\n",
        "        # __getitem__ returns: img_tensor, label, parent_id\n",
        "        img_tensor, label, parent_id = dataset[idx]\n",
        "\n",
        "        # 2. Un-Normalize\n",
        "        # The tensor is (C, H, W) in range like [-2, 2] due to normalization.\n",
        "        # We need (H, W, C) in range [0, 1] for matplotlib.\n",
        "        img = img_tensor.permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Reverse the normalization formula: pixel = (pixel * std) + mean\n",
        "        mean = np.array(NORMALIZATION_MEAN)\n",
        "        std = np.array(NORMALIZATION_STD)\n",
        "        img = (img * std) + mean\n",
        "\n",
        "        # Clip to [0,1] range to avoid matplotlib warnings for slightly out-of-bound values\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        # 3. Plot\n",
        "        ax = axes[i] if num_samples > 1 else axes\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Title with metadata\n",
        "        label_name = class_names[label] if class_names else f\"Class {label}\"\n",
        "        ax.set_title(f\"{label_name}\\n{parent_id}\", fontsize=9)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Displaying {num_samples} patches from {len(dataset)} total samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1f3c17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Sample 5 random slides from the full dataframe\n",
        "# We do this to avoid waiting for the entire dataset to process\n",
        "small_sample_df = full_df.sample(n=5, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"Sub-sampling {len(small_sample_df)} real slides for visualization...\")\n",
        "\n",
        "# 2. Instantiate the Dataset with ONLY these 5 slides\n",
        "# We use validation settings (no overlap) to see the 'pure' tiles\n",
        "sample_ds = MaskedGridTileDataset(\n",
        "    dataframe=small_sample_df,\n",
        "    img_dir=train_set_dir,\n",
        "    transforms=train_transform_tl,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=0.1,\n",
        "    overlap_ratio=0.2,\n",
        "    normalize=True,  # Apply Macenko to see the normalized color\n",
        ")\n",
        "\n",
        "# 3. Visualize using the helper function\n",
        "print(f\"\\nVisualizing {len(sample_ds)} patches extracted from these slides...\")\n",
        "# We use the function I provided in the previous step\n",
        "visualize_dataset_samples(sample_ds, num_samples=5, class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737af292",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sample_ds = MaskedGridTileDataset(\n",
        "    dataframe=small_sample_df,\n",
        "    img_dir=train_set_dir,\n",
        "    transforms=train_transform_ft,\n",
        "    target_size=IMG_RESIZE,\n",
        "    mask_threshold=0.1,\n",
        "    overlap_ratio=0.2,\n",
        "    normalize=True,  # Apply Macenko to see the normalized color\n",
        ")\n",
        "\n",
        "# 3. Visualize using the helper function\n",
        "print(f\"\\nVisualizing {len(sample_ds)} patches extracted from these slides...\")\n",
        "# We use the function I provided in the previous step\n",
        "visualize_dataset_samples(sample_ds, num_samples=5, class_names=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f6b750",
      "metadata": {},
      "source": [
        "## üßÆ **Network Parameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c901504",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"efficientNetV2_S_tiling_macenko\"\n",
        "NET_NAME = \"efficientNetV2_S\"\n",
        "\n",
        "SHOW_PLOTS = True\n",
        "VERBOSE = 5\n",
        "\n",
        "# Transfer learning parameters\n",
        "LEARNING_RATE = 5e-4\n",
        "TL_EPOCHS = 1#70\n",
        "TL_PATIENCE = 8\n",
        "TL_DROPOUT_RATE = 0.5\n",
        "TL_WEIGHT_DECAY = 1e-2\n",
        "\n",
        "# Fine tuning parameters\n",
        "FT_EPOCHS = 1#100\n",
        "FT_PATIENCE = 15\n",
        "FT_LEARNING_RATE = 4e-5\n",
        "FT_WEIGHT_DECAY = 1e-3\n",
        "FT_DROPOUT_RATE = 0.4\n",
        "N_LAYERS_TO_UNFREEZE = 17\n",
        "MIXUP_ALPHA = 0.2\n",
        "\n",
        "FOCAL_LOSS_GAMMA = 1.5\n",
        "\n",
        "# K fold cross validation parameters\n",
        "K_FOLD_LIMIT = 5  # Number of folds to run (for quick testing)\n",
        "K_FOLD_MAX_VALUE = (\n",
        "    5  # SETTING K_FOLD_MAX_VALUE TO 5 means you get 1/5=20% of data as validation set\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd872b8a",
      "metadata": {
        "id": "fd872b8a"
      },
      "source": [
        "## üß† **Training Functions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea708144",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2, device=\"cuda\"):\n",
        "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Calculates weighted loss for mixed targets\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5956abc5",
      "metadata": {
        "id": "5956abc5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model,\n",
        "    train_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scaler,\n",
        "    device,\n",
        "    mixup_alpha=0.0,\n",
        "    grad_accumulation_steps=1,\n",
        "):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for idx, (inputs, targets, _) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if mixup_alpha > 0:\n",
        "            # Generate mixed inputs\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(\n",
        "                inputs, targets, mixup_alpha, device\n",
        "            )\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):  # type: ignore\n",
        "                logits = model(inputs)\n",
        "                # Calculate loss mixing both targets\n",
        "                loss = mixup_criterion(criterion, logits, targets_a, targets_b, lam)\n",
        "        else:\n",
        "            # Standard training\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):  # type: ignore\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss = loss / grad_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # We only update weights every 'accumulation_steps' OR at the very last batch\n",
        "        if (idx + 1) % grad_accumulation_steps == 0 or (idx + 1) == len(train_loader):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Reset gradients for the next accumulation cycle\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        running_loss += (loss.item() * grad_accumulation_steps) * inputs.size(0)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        # If mixed, use targets_a for rough accuracy estimation\n",
        "        current_targets = targets_a if mixup_alpha > 0 else targets  # type: ignore\n",
        "        all_predictions.append(preds.cpu().numpy())\n",
        "        all_targets.append(current_targets.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(all_targets)\n",
        "    y_pred = np.concatenate(all_predictions)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b05dac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate one epoch with test time augmentation\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): model to validate\n",
        "        val_loader (torch.utils.data.DataLoader): dataset loader\n",
        "        criterion (torch.nn.modules.loss._Loss): criterion\n",
        "        device (torch.device): device\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for inputs, targets, _ in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):  # type: ignore\n",
        "                logits_orig = model(inputs)\n",
        "                avg_probs = F.softmax(logits_orig, dim=1)\n",
        "\n",
        "            # Calculate Loss OUTSIDE autocast for numerical stability\n",
        "            # Use float32 for loss computation\n",
        "            loss = criterion(logits_orig.float(), targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            preds = avg_probs.argmax(dim=1)\n",
        "\n",
        "            all_predictions.append(preds.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(all_targets)\n",
        "    y_pred = np.concatenate(all_predictions)\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_acc = accuracy_score(y_true, y_pred)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad2fde2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def validate_slide_level(model, loader, device, top_k_percent=0.2):\n",
        "    \"\"\"\n",
        "    Evaluates the model at the SLIDE level using Top-K Pooling.\n",
        "\n",
        "    Args:\n",
        "        top_k_percent (float): Percentage of patches to consider for the slide-level prediction.\n",
        "                               e.g., 0.2 means we average the top 20% confident patches.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    slide_probs, slide_labels = {}, {}\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels, slide_ids in loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)  # Get probabilities\n",
        "\n",
        "            # Move to CPU\n",
        "            probs = probs.cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            # Group by Slide ID\n",
        "            for i, slide_id in enumerate(slide_ids):\n",
        "                slide_probs.setdefault(slide_id, []).append(probs[i])\n",
        "                slide_labels[slide_id] = labels[i].item()\n",
        "\n",
        "    # --- Aggregation (Top-K Pooling) ---\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for sid, patches in slide_probs.items():\n",
        "        patches = np.array(patches)\n",
        "        k = max(1, int(len(patches) * top_k_percent))\n",
        "\n",
        "        # Sort descending per class -> Take Top-K -> Average -> Argmax\n",
        "        slide_score = np.mean(np.sort(patches, axis=0)[::-1][:k], axis=0)\n",
        "        y_pred.append(np.argmax(slide_score))\n",
        "        y_true.append(slide_labels[sid])\n",
        "\n",
        "    return f1_score(y_true, y_pred, average=\"macro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae97722",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "\n",
        "\n",
        "def fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scaler,\n",
        "    device,\n",
        "    scheduler=None,\n",
        "    patience=0,\n",
        "    evaluation_metric=\"val_slide_f1\",\n",
        "    mode=\"max\",\n",
        "    restore_best_weights=True,\n",
        "    writer=None,\n",
        "    verbose=1,\n",
        "    experiment_name=\"\",\n",
        "    mixup_alpha=0.0,\n",
        "    use_swa=False,\n",
        "    swa_start_epoch=10,\n",
        "    grad_accumulation_steps=1,\n",
        ") -> tuple[torch.nn.Module, dict]:\n",
        "    \"\"\"\n",
        "    Fit the provided model\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
        "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
        "        epochs (int): Number of training epochs.\n",
        "        criterion (torch.nn.Module): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
        "        scaler (torch.cuda.amp.GradScaler): Gradient scaler for mixed precision training.\n",
        "        device (torch.device): Device to run the training on.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.ReduceLROnPlateau, optional): Learning rate scheduler. Defaults to None.\n",
        "        patience (int, optional): Number of epochs with no improvement after which training will be stopped. Defaults to 0.\n",
        "        evaluation_metric (str, optional): Metric to evaluate for early stopping and best model selection. Defaults to \"val_f1\".\n",
        "        mode (str, optional): One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing. Defaults to \"max\".\n",
        "        restore_best_weights (bool, optional): Whether to restore model weights from the epoch with the best value of the monitored quantity. Defaults to True.\n",
        "        writer (torch.utils.tensorboard.SummaryWriter, optional): TensorBoard writer for logging. Defaults to None.\n",
        "        verbose (int, optional): Verbosity level. Defaults to 1.\n",
        "        experiment_name (str, optional): Name of the experiment for saving models. Defaults to \"\".\n",
        "        mixup_alpha (float, optional): Alpha value for MixUp data augmentation. Defaults to 0.0.\n",
        "        use_swa (bool, optional): Whether to use Stochastic Weight Averaging (SWA). Defaults to False.\n",
        "        swa_start_epoch (int, optional): Epoch to start averaging for SWA. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        tuple[torch.nn.Module, dict]: The trained model and training history.\n",
        "    \"\"\"\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_f1\": [], \"val_loss\": [], \"val_patch_f1\": [], \"val_slide_f1\": []}\n",
        "\n",
        "    best_metric = float(\"-inf\") if mode == \"max\" else float(\"inf\")\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Initialize SWA Model if enabled\n",
        "    if use_swa:\n",
        "        print(f\"SWA Enabled. Averaging will start after epoch {swa_start_epoch}.\")\n",
        "        swa_model = AveragedModel(model)\n",
        "    else:\n",
        "        swa_model = None\n",
        "\n",
        "    print(f\"Training {epochs} epochs (MixUp: {mixup_alpha}, SWA: {use_swa})...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, _, train_f1 = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scaler,\n",
        "            device,\n",
        "            mixup_alpha,\n",
        "            grad_accumulation_steps,\n",
        "        )\n",
        "        val_loss, _, val_patch_f1 = validate_one_epoch(model, val_loader, criterion, device)\n",
        "        val_slide_f1 = validate_slide_level(model,val_loader, device, top_k_percent=0.15)\n",
        "\n",
        "        # 2. Update SWA (if active and past start epoch)\n",
        "        if use_swa and epoch >= swa_start_epoch:\n",
        "            swa_model.update_parameters(model)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            # If using ReduceLROnPlateau, step based on validation metric\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(val_patch_f1)\n",
        "            else:\n",
        "                # If using CosineAnnealing, step based on epoch\n",
        "                scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_f1\"].append(train_f1)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_patch_f1\"].append(val_patch_f1)\n",
        "        history[\"val_slide_f1\"].append(val_slide_f1)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"Loss/Training\", train_loss, epoch)\n",
        "            writer.add_scalar(\"F1/Training\", train_f1, epoch)\n",
        "            writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
        "            writer.add_scalar(\"F1/Validation\", val_patch_f1, epoch)\n",
        "\n",
        "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
        "            print(\n",
        "                f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                f\"Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | \"\n",
        "                f\"Patch Val: Loss={val_loss:.4f}, F1={val_patch_f1:.4f} | \"\n",
        "                f\"Slide Val: F1={val_slide_f1:.4f} | \"\n",
        "                f\"LR: {scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            )\n",
        "\n",
        "        current_metric = history[evaluation_metric][-1]\n",
        "        is_improvement = (\n",
        "            (current_metric > best_metric)\n",
        "            if mode == \"max\"\n",
        "            else (current_metric < best_metric)\n",
        "        )\n",
        "\n",
        "        if is_improvement:\n",
        "            best_metric = current_metric\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience and patience > 0:\n",
        "                print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                break\n",
        "\n",
        "    # Handle SWA Finalization\n",
        "    if use_swa:\n",
        "        print(\"Finalizing SWA Batch Normalization statistics...\")\n",
        "        # We need to compute BN stats for the averaged weights using the train loader\n",
        "        update_bn(train_loader, swa_model, device=device)\n",
        "\n",
        "        # Save SWA model\n",
        "        torch.save(swa_model.state_dict(), f\"models/{experiment_name}_model.pt\")  # type: ignore\n",
        "        print(f\"SWA model saved to models/{experiment_name}_model.pt\")\n",
        "\n",
        "        # Return SWA model as the primary model\n",
        "        return swa_model, history\n",
        "\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\" + experiment_name + \"_model.pt\"))\n",
        "        print(\n",
        "            f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\"\n",
        "        )\n",
        "    elif patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8643d67",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def analyze_performance(model, loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Generating predictions for Confusion Matrix...\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for inputs, labels, _ in tqdm(loader, desc=\"Validating\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # --- 1. Confusion Matrix ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Normalize by row (True Label) to see recall percentages\n",
        "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm_normalized,\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "    )\n",
        "    plt.ylabel(\"True Label (Ground Truth)\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.title(\"Normalized Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- 2. Classification Report ---\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\n",
        "        classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
        "    )\n",
        "\n",
        "    return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a5ecf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        # Alpha should be the class weights you already calculated\n",
        "        # If alpha is None, no class weighting is applied\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Clamp inputs to prevent overflow in exp()\n",
        "        inputs = torch.clamp(inputs, min=-100, max=100)\n",
        "\n",
        "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction=\"none\")\n",
        "        pt = torch.exp(-ce_loss)  # prevents nans when probability is 0\n",
        "\n",
        "        # Add small epsilon to prevent numerical issues\n",
        "        pt = torch.clamp(pt, min=1e-7, max=1.0)\n",
        "\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa79a1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot TL vs FT\n",
        "def plot_history(tl_history, ft_history):\n",
        "    epochs_tl = len(tl_history[\"val_slide_f1\"])\n",
        "    epochs_ft = len(ft_history[\"val_slide_f1\"])\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # F1\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(tl_history[\"train_f1\"], label=\"Train F1 TL\")\n",
        "    plt.plot(tl_history[\"val_slide_f1\"], label=\"Slide Val F1 TL\")\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft),\n",
        "        ft_history[\"train_f1\"],\n",
        "        label=\"Train F1 FT\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft), ft_history[\"val_slide_f1\"], label=\"Slide Val F1 FT\"\n",
        "    )\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1 macro\")\n",
        "    plt.title(\"Andamento F1 TL+FT\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(tl_history[\"train_loss\"], label=\"Train Loss TL\")\n",
        "    plt.plot(tl_history[\"val_loss\"], label=\"Val Loss TL\")\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft),\n",
        "        ft_history[\"train_loss\"],\n",
        "        label=\"Train Loss FT\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        range(epochs_tl, epochs_tl + epochs_ft),\n",
        "        ft_history[\"val_loss\"],\n",
        "        label=\"Val Loss FT\",\n",
        "    )\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Andamento Loss TL+FT\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d2ad0c",
      "metadata": {
        "id": "29d2ad0c"
      },
      "source": [
        "## üõ†Ô∏è **Transfer Learning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f17a8ac",
      "metadata": {
        "id": "3f17a8ac"
      },
      "outputs": [],
      "source": [
        "class EfficientNetCustom(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiates EfficientNet-B0 with ImageNet weights.\n",
        "    Replaces the classifier head with a high-dropout dense layer to prevent overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.EfficientNet_V2_S_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.efficientnet_v2_s(weights=self.weights)\n",
        "\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, self.num_classes),  # type: ignore\n",
        "        )\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all layers except the classifier head\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not name.startswith(\"classifier\"):\n",
        "                param.requires_grad = False\n",
        "        # Ensure classifier params are trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "        if n_layers <= 0:\n",
        "            return\n",
        "\n",
        "        # Keep classifier trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Get all named parameters in features (in order)\n",
        "        feature_params = list(self.backbone.features.named_parameters())\n",
        "\n",
        "        # Unfreeze the last n_layers parameters\n",
        "        params_to_unfreeze = feature_params[-n_layers:]\n",
        "\n",
        "        for name, param in params_to_unfreeze:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        \"\"\"Returns the flattened feature vector before the classifier.\"\"\"\n",
        "        # 1. Run features\n",
        "        x = self.backbone.features(x)\n",
        "        # 2. Global Average Pooling (same as original forward)\n",
        "        x = self.backbone.avgpool(x)\n",
        "        # 3. Flatten\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c08f15",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DenseNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.DenseNet121_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.densenet121(weights=self.weights)\n",
        "\n",
        "        # DenseNet classifier is stored in .classifier\n",
        "        in_features = self.backbone.classifier.in_features\n",
        "\n",
        "        # Replace Classifier\n",
        "        self.backbone.classifier = nn.Sequential(  # type: ignore\n",
        "            nn.Dropout(dropout_rate), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all feature layers\n",
        "        for param in self.backbone.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Unfreeze classifier\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "        if n_layers <= 0:\n",
        "            return\n",
        "\n",
        "        # Keep classifier trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Get all named parameters in features (in order)\n",
        "        feature_params = list(self.backbone.features.named_parameters())\n",
        "\n",
        "        # Unfreeze the last n_layers parameters\n",
        "        params_to_unfreeze = feature_params[-n_layers:]\n",
        "\n",
        "        for name, param in params_to_unfreeze:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        \"\"\"Returns the flattened feature vector before the classifier.\"\"\"\n",
        "        # DenseNet features\n",
        "        features = self.backbone.features(x)\n",
        "        # ReLU + Pooling (Standard DenseNet finish)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd6cb3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper that can interchange between DenseNetCustom and EfficientNetCustom.\n",
        "    Keeps the same constructor signature used in the notebook:\n",
        "    CustomNet(num_classes, dropout_rate, backbone=...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4, backbone=\"densenet121\"):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.backbone_name = backbone.lower()\n",
        "\n",
        "        if self.backbone_name in (\"densenet\", \"densenet121\"):\n",
        "            self.backbone = DenseNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        elif self.backbone_name in (\n",
        "            \"efficientnet\",\n",
        "            \"efficientnet_v2s\",\n",
        "            \"efficientnetv2s\",\n",
        "            \"efficientnetv2_s\",\n",
        "        ):\n",
        "            self.backbone = EfficientNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported backbone '{backbone}'. Use 'densenet' or 'efficientnet'.\"\n",
        "            )\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"freeze_backbone\"):\n",
        "            self.backbone.freeze_backbone()\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"unfreeze_backbone\"):\n",
        "            self.backbone.unfreeze_backbone(n_layers, all=all)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def extract_embeddings(self, x):\n",
        "        return self.backbone.extract_embeddings(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53e8f77",
      "metadata": {},
      "source": [
        "## **K Fold Routine**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f93d7e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import gc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "def run_k_fold_training(\n",
        "    full_df,\n",
        "    max_splits=5,\n",
        "    split_limit=1,\n",
        "    base_experiment_name=\"efficientnet_v2s\",\n",
        "    writer=None,\n",
        "    show_plots=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Orchestrates the complete K-Fold Cross Validation pipeline.\n",
        "    For each fold:\n",
        "      1. Splits data\n",
        "      2. Runs Transfer Learning (TL) -> Warmup\n",
        "      3. Runs Fine Tuning (FT) -> Optimization with MixUp & SWA\n",
        "    \"\"\"\n",
        "    if split_limit > max_splits:\n",
        "        print(\n",
        "            f\"Warning: split_limit ({split_limit}) > max_splits ({max_splits}). Using max_splits.\"\n",
        "        )\n",
        "        split_limit = max_splits\n",
        "\n",
        "    # Initialize K-Fold\n",
        "    skf = StratifiedKFold(n_splits=max_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # Store results\n",
        "    fold_results = []\n",
        "\n",
        "    print(f\"Starting {split_limit}-Fold Cross Validation...\")\n",
        "    print(f\"Total Epochs per Fold: {TL_EPOCHS} (TL) + {FT_EPOCHS} (FT)\")\n",
        "\n",
        "    # Iterate through folds\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(\n",
        "        skf.split(full_df, full_df[\"label_index\"])\n",
        "    ):\n",
        "        if fold_idx >= split_limit:\n",
        "            break\n",
        "\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"FOLD {fold_idx+1}/{split_limit}\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # --- 1. DATA PREPARATION ---\n",
        "        fold_train_df = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "        fold_val_df = full_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        # Calculate Class Weights for this specific fold\n",
        "        # (Crucial: Weights change slightly between folds)\n",
        "        fold_class_weights = compute_class_weight(\n",
        "            class_weight=\"balanced\",\n",
        "            classes=np.arange(num_classes),\n",
        "            y=fold_train_df[\"label_index\"].values,\n",
        "        )\n",
        "        fold_weights_tensor = torch.tensor(fold_class_weights, dtype=torch.float).to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "        # Re-initialize Loss with specific fold weights\n",
        "        fold_criterion = FocalLoss(alpha=fold_weights_tensor, gamma=FOCAL_LOSS_GAMMA)\n",
        "\n",
        "        # Create Datasets\n",
        "        # Note: We re-instantiate datasets to ensure clean separation\n",
        "        train_ds = MaskedGridTileDataset(\n",
        "            fold_train_df,\n",
        "            train_set_dir,\n",
        "            transforms=train_transform_tl,\n",
        "            target_size=IMG_RESIZE,\n",
        "            mask_threshold=MASK_THRESHOLD,\n",
        "            overlap_ratio=OVERLAP_RATIO,\n",
        "            normalize=True,\n",
        "            debug_max=None,\n",
        "        )\n",
        "        val_ds = MaskedGridTileDataset(\n",
        "            fold_val_df,\n",
        "            train_set_dir,\n",
        "            transforms=data_transforms,\n",
        "            target_size=IMG_RESIZE,\n",
        "            mask_threshold=MASK_THRESHOLD,\n",
        "            overlap_ratio=0.0,\n",
        "            normalize=True,\n",
        "            debug_max=None,\n",
        "        )\n",
        "\n",
        "        train_loader = make_loader(train_ds, BATCH_SIZE, shuffle=True)\n",
        "        val_loader = make_loader(val_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # --- 2. PHASE 1: TRANSFER LEARNING (Warm-Up) ---\n",
        "        print(f\"Fold {fold_idx+1} >> Phase 1: Transfer Learning (Frozen Backbone)\")\n",
        "\n",
        "        # Initialize Fresh Model\n",
        "        model = CustomNet(num_classes, TL_DROPOUT_RATE, backbone=NET_NAME).to(device)\n",
        "        model.freeze_backbone()\n",
        "\n",
        "        optimizer_tl = AdamW(\n",
        "            model.parameters(), lr=LEARNING_RATE, weight_decay=TL_WEIGHT_DECAY\n",
        "        )\n",
        "        scaler_tl = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))  # type: ignore\n",
        "        scheduler_tl = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer_tl, T_max=TL_EPOCHS, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # Run Fit (TL)\n",
        "        tl_model, tl_hystory = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=TL_EPOCHS,\n",
        "            criterion=fold_criterion,\n",
        "            optimizer=optimizer_tl,\n",
        "            scaler=scaler_tl,\n",
        "            device=device,\n",
        "            scheduler=scheduler_tl,\n",
        "            experiment_name=f\"{base_experiment_name}_fold{fold_idx+1}_tl\",\n",
        "            patience=TL_PATIENCE,\n",
        "            mixup_alpha=0.0,\n",
        "            use_swa=False,\n",
        "            verbose=VERBOSE,\n",
        "            writer=writer,\n",
        "            grad_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
        "        )\n",
        "\n",
        "        # --- 3. PHASE 2: FINE TUNING (Full Training) ---\n",
        "        print(f\"Fold {fold_idx+1} >> Phase 2: Fine Tuning (Unfrozen + MixUp + SWA)\")\n",
        "\n",
        "        # Unfreeze\n",
        "        tl_model.unfreeze_backbone(N_LAYERS_TO_UNFREEZE, all=False)  # type: ignore\n",
        "\n",
        "        if fold_idx == 0:\n",
        "            total_params = sum(p.numel() for p in tl_model.parameters())\n",
        "            trainable_params = sum(\n",
        "                p.numel() for p in tl_model.parameters() if p.requires_grad\n",
        "            )\n",
        "\n",
        "            print(f\"Total parameters: {total_params:,}\")\n",
        "            print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "            print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "        # Switch Transforms to Heavy Augmentation\n",
        "        train_ds.transforms = train_transform_ft  # Update dataset transform in-place\n",
        "\n",
        "        # New Optimizer (Lower LR)\n",
        "        optimizer_ft = AdamW(\n",
        "            model.parameters(), lr=FT_LEARNING_RATE, weight_decay=FT_WEIGHT_DECAY\n",
        "        )\n",
        "        scaler_ft = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))  # type: ignore\n",
        "        scheduler_ft = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer_ft, T_max=FT_EPOCHS, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # Run Fit (FT)\n",
        "        # Note: We use the SWA Start logic here.\n",
        "        # If Epochs=200, start averaging around 150.\n",
        "        swa_start = int(FT_EPOCHS * 0.75)\n",
        "\n",
        "        ft_model, ft_history = fit(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=FT_EPOCHS,\n",
        "            criterion=fold_criterion,\n",
        "            optimizer=optimizer_ft,\n",
        "            scaler=scaler_ft,\n",
        "            device=device,\n",
        "            scheduler=scheduler_ft,\n",
        "            experiment_name=f\"{base_experiment_name}_fold{fold_idx+1}_ft\",\n",
        "            patience=FT_PATIENCE,\n",
        "            mixup_alpha=MIXUP_ALPHA,\n",
        "            use_swa=False,\n",
        "            swa_start_epoch=swa_start,\n",
        "            verbose=VERBOSE,\n",
        "            writer=writer,\n",
        "            grad_accumulation_steps=GRAD_ACCUMULATION_STEPS,\n",
        "        )\n",
        "\n",
        "        best_f1 = max(ft_history[\"val_slide_f1\"])\n",
        "        print(f\"Fold {fold_idx+1} Completed. Best Slide Val F1: {best_f1:.4f}\")\n",
        "        fold_results.append(best_f1)\n",
        "\n",
        "        if show_plots:\n",
        "            analyze_performance(ft_model, val_loader, device, class_names)\n",
        "            plot_history(tl_hystory, ft_history)\n",
        "\n",
        "        # Cleanup to free GPU memory for next fold\n",
        "        del (\n",
        "            tl_model,\n",
        "            ft_model,\n",
        "            optimizer_tl,\n",
        "            optimizer_ft,\n",
        "            scaler_tl,\n",
        "            scaler_ft,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            train_ds,\n",
        "            val_ds,\n",
        "        )\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # --- SUMMARY ---\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(\"CROSS VALIDATION COMPLETE\")\n",
        "    print(f\"Folds: {fold_results}\")\n",
        "    print(f\"Average Slide Val F1: {np.mean(fold_results):.4f} (+/- {np.std(fold_results):.4f})\")\n",
        "    print(f\"{'='*40}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f64a21ea",
      "metadata": {},
      "source": [
        "## **Training**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ulntce16E9NJ",
      "metadata": {
        "id": "ulntce16E9NJ"
      },
      "outputs": [],
      "source": [
        "# Setup training\n",
        "\n",
        "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + EXPERIMENT_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967d7d8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "run_k_fold_training(\n",
        "    full_df,\n",
        "    max_splits=K_FOLD_MAX_VALUE, \n",
        "    split_limit=K_FOLD_LIMIT, \n",
        "    base_experiment_name=EXPERIMENT_NAME, \n",
        "    writer=writer, \n",
        "    show_plots=SHOW_PLOTS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365f9814",
      "metadata": {
        "id": "365f9814"
      },
      "source": [
        "## **Inference on test data for kaggle**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb67d492",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_mean_aggregation(prob_matrix, k_percent=0.5):\n",
        "    \"\"\"\n",
        "    Aggregates patch probabilities into a slide prediction by averaging\n",
        "    only the most confident patches (Top-K%).\n",
        "\n",
        "    Args:\n",
        "        prob_matrix: Numpy array of shape [N_patches, N_classes]\n",
        "        k_percent: Float (0.0 to 1.0). Percentage of patches to keep.\n",
        "                   0.3 means we only average the top 30% scores.\n",
        "    \"\"\"\n",
        "    n_patches = prob_matrix.shape[0]\n",
        "\n",
        "    # Safety check: if slide has very few patches, keep at least 1\n",
        "    k = max(1, int(n_patches * k_percent))\n",
        "\n",
        "    # Sort probabilities for each class INDEPENDENTLY (Axis 0 = patches)\n",
        "    # We want the highest probabilities for Class 0, Class 1, etc.\n",
        "    sorted_probs = np.sort(prob_matrix, axis=0)\n",
        "\n",
        "    # Take the top K (the last K elements in the sorted array)\n",
        "    top_k_probs = sorted_probs[-k:, :]\n",
        "\n",
        "    # Average them\n",
        "    slide_score = np.mean(top_k_probs, axis=0)\n",
        "\n",
        "    return slide_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9964ad6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9964ad6",
        "outputId": "577dadf8-ba13-4832-a3c0-dde85f0cc914"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "\n",
        "def generate_ensemble_submission(\n",
        "    test_dir,\n",
        "    base_experiment_name=\"efficientnet_v2s\",\n",
        "    num_max_folds=5,\n",
        "    num_fold_limit=1,\n",
        "    output_file=\"submission_ensemble.csv\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads K models, sums their probabilities, and performs Soft Voting.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Setup Test Dataset & Loader\n",
        "    # We use the dataset once to establish the order of patches\n",
        "    print(f\"Preparing Test Dataset from {test_dir}...\")\n",
        "    test_ds = MaskedGridTileDataset(\n",
        "        dataframe=None,\n",
        "        img_dir=test_dir,\n",
        "        transforms=data_transforms,\n",
        "        target_size=IMG_RESIZE,\n",
        "        mask_threshold=MASK_THRESHOLD,\n",
        "        overlap_ratio=0.5,\n",
        "        debug_max=None,\n",
        "    )\n",
        "\n",
        "    test_loader = make_loader(test_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    num_patches = len(test_ds)\n",
        "    print(f\"Total Patches to Process: {num_patches}\")\n",
        "\n",
        "    # matrix to store accumulated probabilities [N_patches, N_classes]\n",
        "    # We initialize with zeros\n",
        "    accumulated_probs = np.zeros((num_patches, num_classes), dtype=np.float32)\n",
        "\n",
        "    # We need to store parent_ids to aggregate later\n",
        "    # The loader is deterministic (shuffle=False), so we can extract them once\n",
        "    all_parent_ids = []\n",
        "\n",
        "    # 2. Iterate through each Fold Model\n",
        "    num_folds_to_use = min(num_fold_limit, num_max_folds)\n",
        "    for fold in range(1, num_folds_to_use + 1):\n",
        "        model_path = f\"models/{base_experiment_name}_fold{fold}_ft_model.pt\"\n",
        "        print(f\"\\n--- Loading Model {fold}/{num_folds_to_use}: {model_path} ---\")\n",
        "\n",
        "        # Load Model\n",
        "        try:\n",
        "            model = CustomNet(num_classes, FT_DROPOUT_RATE).to(device)\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ö†Ô∏è Warning: Model file {model_path} not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Inference Loop for this model\n",
        "        fold_patch_probs = []\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for inputs, _, parent_ids in tqdm(\n",
        "                test_loader, desc=f\"Inference Fold {fold}\"\n",
        "            ):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # --- TTA Strategy (Same as Validation) ---\n",
        "                # 1. Original\n",
        "                out1 = F.softmax(model(inputs), dim=1)\n",
        "\n",
        "                # 2. Horizontal Flip\n",
        "                inputs_hf = torch.flip(inputs, dims=[3])\n",
        "                out2 = F.softmax(model(inputs_hf), dim=1)\n",
        "\n",
        "                # 3. Vertical Flip\n",
        "                inputs_vf = torch.flip(inputs, dims=[2])\n",
        "                out3 = F.softmax(model(inputs_vf), dim=1)\n",
        "\n",
        "                # Average predictions for this model\n",
        "                avg_probs = (out1 + out2 + out3) / 3.0\n",
        "\n",
        "                # Store\n",
        "                fold_patch_probs.extend(avg_probs.cpu().numpy())\n",
        "\n",
        "                # Only collect parent_ids on the first fold (optimization)\n",
        "                if fold == 1:\n",
        "                    all_parent_ids.extend(parent_ids)\n",
        "\n",
        "        # Add to global accumulator\n",
        "        accumulated_probs += np.array(fold_patch_probs)\n",
        "\n",
        "        # Cleanup\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # 3. Average & Aggregate\n",
        "    print(\"\\nAveraging Ensembled Probabilities...\")\n",
        "    final_avg_probs = accumulated_probs / num_folds_to_use\n",
        "\n",
        "    # Now we have [Patch, Probs]. We need [Slide, Probs]\n",
        "    slide_probs = {}\n",
        "\n",
        "    print(\"Aggregating Patches to Slides (Soft Voting)...\")\n",
        "    for i, pid in enumerate(all_parent_ids):\n",
        "        if pid not in slide_probs:\n",
        "            slide_probs[pid] = []\n",
        "        slide_probs[pid].append(final_avg_probs[i])\n",
        "\n",
        "    final_rows = []\n",
        "\n",
        "    for img_name, prob_list in slide_probs.items():\n",
        "        # Stack patches for this slide\n",
        "        prob_matrix = np.array(prob_list)\n",
        "\n",
        "        slide_avg = top_k_mean_aggregation(prob_matrix=prob_matrix, k_percent=0.3)\n",
        "\n",
        "        # Argmax for final label\n",
        "        pred_idx = np.argmax(slide_avg)\n",
        "        pred_label = class_names[pred_idx]\n",
        "\n",
        "        # Filename formatting\n",
        "        sample_index = img_name  # Assuming format matches submission reqs\n",
        "\n",
        "        final_rows.append({\"sample_index\": sample_index, \"label\": pred_label})\n",
        "\n",
        "    # 4. Save Submission\n",
        "    submission_df = pd.DataFrame(final_rows)\n",
        "    submission_df = submission_df.sort_values(by=\"sample_index\")\n",
        "\n",
        "    os.makedirs(\"submission\", exist_ok=True)\n",
        "    submission_path = os.path.join(\"submission\", output_file)\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Ensemble Submission Saved: {submission_path}\")\n",
        "    return submission_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356d24aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_df = generate_ensemble_submission(\n",
        "    test_dir=test_set_dir,\n",
        "    base_experiment_name=EXPERIMENT_NAME,\n",
        "    num_max_folds=K_FOLD_MAX_VALUE,\n",
        "    num_fold_limit=K_FOLD_LIMIT,\n",
        "    output_file=f\"{EXPERIMENT_NAME}_ensemble_{K_FOLD_LIMIT}fold.csv\",\n",
        ")\n",
        "\n",
        "ensemble_df.head(20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
