{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a75bb6ad",
      "metadata": {},
      "source": [
        "# **XGBoost**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900b0225",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enviroment\n",
        "isColab = False\n",
        "colab_dir = \"/gdrive/My Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "\n",
        "isKaggle = False\n",
        "isWsl = True\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 46"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "591ec1c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loader parameters\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZATION_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "IMG_RESIZE = (224, 224)\n",
        "INPUT_SHAPE = (3, *IMG_RESIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e38d561c",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"efficientNetV2_S_fra\"\n",
        "NET_NAME = \"efficientnet\"\n",
        "\n",
        "# Fine tuning parameters\n",
        "FT_DROPOUT_RATE = 0.3\n",
        "K_FOLD = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4cb00e",
      "metadata": {},
      "source": [
        "## **Loading Enviroment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09b8c7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory di default\n",
        "current_dir = os.getcwd()   \n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive # type: ignore\n",
        "    drive.mount(\"/gdrive\")\n",
        "    current_dir = colab_dir\n",
        "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
        "    %cd $current_dir\n",
        "elif isKaggle:\n",
        "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
        "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
        "    current_dir = kaggle_work_dir\n",
        "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
        "    os.chdir(current_dir)\n",
        "elif isWsl:\n",
        "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "else:\n",
        "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
        "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "\n",
        "print(f\"Changed directory to: {current_dir}\")\n",
        "\n",
        "# Define absolute paths\n",
        "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
        "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
        "\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Train set directory: {train_set_dir}\")\n",
        "print(f\"Test set directory: {test_set_dir}\")\n",
        "print(f\"Label file: {label_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e0fdc8",
      "metadata": {},
      "source": [
        "## **Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be49326d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49326d",
        "outputId": "59e423d7-35ea-4fc1-933c-affb303ee9f5"
      },
      "outputs": [],
      "source": [
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "%pip install torchview\n",
        "from torchview import draw_graph\n",
        "\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "if isColab or isKaggle:\n",
        "    !pkill -f tensorboard \n",
        "    !mkdir -p models\n",
        "    print(\"Killed existing TensorBoard instances and created models directory.\") \n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)  \n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import cv2\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.gridspec as gridspec\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import ndimage\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcbf293",
      "metadata": {},
      "source": [
        "### **Preparing Dataset for colab**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f17001",
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    drive_dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "    local_dataset_dir = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(local_dataset_dir):\n",
        "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
        "        try:\n",
        "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
        "            print(\"Copy complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying dataset: {e}\")\n",
        "            print(\"Falling back to Drive dataset (slow).\")\n",
        "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
        "            dataset_dir = drive_dataset_dir\n",
        "    else:\n",
        "        print(\"Dataset already copied to local runtime.\")\n",
        "\n",
        "    # If copy succeeded (or already existed), use local path\n",
        "    if os.path.exists(local_dataset_dir):\n",
        "        dataset_dir = local_dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfe95f2",
      "metadata": {},
      "source": [
        "## â³ **Data Loading**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e112e14",
      "metadata": {},
      "source": [
        "### **Definitions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d945105",
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_IGNORE = [\n",
        "    \"img_0001.png\",\n",
        "    \"img_0005.png\",\n",
        "    \"img_0008.png\",\n",
        "    \"img_0012.png\",\n",
        "    \"img_0018.png\",\n",
        "    \"img_0020.png\",\n",
        "    \"img_0022.png\",\n",
        "    \"img_0027.png\",\n",
        "    \"img_0028.png\",\n",
        "    \"img_0036.png\",\n",
        "    \"img_0044.png\",\n",
        "    \"img_0047.png\",\n",
        "    \"img_0048.png\",\n",
        "    \"img_0052.png\",\n",
        "    \"img_0062.png\",\n",
        "    \"img_0078.png\",\n",
        "    \"img_0085.png\",\n",
        "    \"img_0090.png\",\n",
        "    \"img_0094.png\",\n",
        "    \"img_0095.png\",\n",
        "    \"img_0126.png\",\n",
        "    \"img_0129.png\",\n",
        "    \"img_0130.png\",\n",
        "    \"img_0133.png\",\n",
        "    \"img_0136.png\",\n",
        "    \"img_0138.png\",\n",
        "    \"img_0148.png\",\n",
        "    \"img_0150.png\",\n",
        "    \"img_0155.png\",\n",
        "    \"img_0159.png\",\n",
        "    \"img_0161.png\",\n",
        "    \"img_0175.png\",\n",
        "    \"img_0178.png\",\n",
        "    \"img_0179.png\",\n",
        "    \"img_0180.png\",\n",
        "    \"img_0184.png\",\n",
        "    \"img_0187.png\",\n",
        "    \"img_0189.png\",\n",
        "    \"img_0193.png\",\n",
        "    \"img_0196.png\",\n",
        "    \"img_0222.png\",\n",
        "    \"img_0251.png\",\n",
        "    \"img_0254.png\",\n",
        "    \"img_0263.png\",\n",
        "    \"img_0268.png\",\n",
        "    \"img_0286.png\",\n",
        "    \"img_0293.png\",\n",
        "    \"img_0313.png\",\n",
        "    \"img_0319.png\",\n",
        "    \"img_0333.png\",\n",
        "    \"img_0342.png\",\n",
        "    \"img_0344.png\",\n",
        "    \"img_0346.png\",\n",
        "    \"img_0355.png\",\n",
        "    \"img_0368.png\",\n",
        "    \"img_0371.png\",\n",
        "    \"img_0376.png\",\n",
        "    \"img_0380.png\",\n",
        "    \"img_0390.png\",\n",
        "    \"img_0393.png\",\n",
        "    \"img_0407.png\",\n",
        "    \"img_0410.png\",\n",
        "    \"img_0415.png\",\n",
        "    \"img_0424.png\",\n",
        "    \"img_0443.png\",\n",
        "    \"img_0453.png\",\n",
        "    \"img_0459.png\",\n",
        "    \"img_0463.png\",\n",
        "    \"img_0486.png\",\n",
        "    \"img_0497.png\",\n",
        "    \"img_0498.png\",\n",
        "    \"img_0499.png\",\n",
        "    \"img_0509.png\",\n",
        "    \"img_0521.png\",\n",
        "    \"img_0530.png\",\n",
        "    \"img_0531.png\",\n",
        "    \"img_0533.png\",\n",
        "    \"img_0537.png\",\n",
        "    \"img_0540.png\",\n",
        "    \"img_0544.png\",\n",
        "    \"img_0547.png\",\n",
        "    \"img_0557.png\",\n",
        "    \"img_0558.png\",\n",
        "    \"img_0560.png\",\n",
        "    \"img_0565.png\",\n",
        "    \"img_0567.png\",\n",
        "    \"img_0572.png\",\n",
        "    \"img_0578.png\",\n",
        "    \"img_0580.png\",\n",
        "    \"img_0586.png\",\n",
        "    \"img_0602.png\",\n",
        "    \"img_0603.png\",\n",
        "    \"img_0607.png\",\n",
        "    \"img_0609.png\",\n",
        "    \"img_0614.png\",\n",
        "    \"img_0620.png\",\n",
        "    \"img_0623.png\",\n",
        "    \"img_0629.png\",\n",
        "    \"img_0635.png\",\n",
        "    \"img_0639.png\",\n",
        "    \"img_0643.png\",\n",
        "    \"img_0644.png\",\n",
        "    \"img_0645.png\",\n",
        "    \"img_0646.png\",\n",
        "    \"img_0656.png\",\n",
        "    \"img_0657.png\",\n",
        "    \"img_0658.png\",\n",
        "    \"img_0670.png\",\n",
        "    \"img_0673.png\",\n",
        "    \"img_0675.png\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7afdb353",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the full dataframe\n",
        "full_df = pd.read_csv(label_file)\n",
        "\n",
        "# Remove cursed images\n",
        "full_df = full_df[~full_df[\"sample_index\"].isin(SAMPLES_TO_IGNORE)].reset_index(\n",
        "    drop=True\n",
        ")\n",
        "\n",
        "# Label mapping\n",
        "class_names = sorted(full_df[\"label\"].unique())\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "full_df[\"label_index\"] = full_df[\"label\"].map(label_to_index)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e13a32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last=False):\n",
        "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(6, cpu_cores))\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,\n",
        "        persistent_workers=isWsl,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c536e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "\n",
        "class MaskedFixedTileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset class that extracts fixed-size patches from the center of tissue masks\n",
        "    to preserve biological scale (magnification), rather than resizing variable crops.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataframe,\n",
        "        img_dir,\n",
        "        transforms=None,\n",
        "        target_size=(224, 224),\n",
        "        debug_max=None,\n",
        "    ):\n",
        "        self.samples = []\n",
        "        self.transforms = transforms\n",
        "        self.img_dir = img_dir\n",
        "        self.target_size = target_size\n",
        "\n",
        "        # Handling inference mode (no labels) vs training mode\n",
        "        self.is_inference_mode = False\n",
        "        if dataframe is None or \"label_index\" not in dataframe.columns:\n",
        "            self.is_inference_mode = True\n",
        "            if dataframe is None:\n",
        "                # If just a directory, list images\n",
        "                img_names = sorted(\n",
        "                    [f for f in os.listdir(img_dir) if f.startswith(\"img_\")]\n",
        "                )\n",
        "            else:\n",
        "                img_names = dataframe[\"sample_index\"].tolist()\n",
        "            iterator = zip(img_names, [-1] * len(img_names))\n",
        "            total_items = len(img_names)\n",
        "        else:\n",
        "            iterator = zip(dataframe[\"sample_index\"], dataframe[\"label_index\"])\n",
        "            total_items = len(dataframe)\n",
        "\n",
        "        print(\n",
        "            f\"Processing {total_items} images to extract fixed-size {target_size} tiles...\"\n",
        "        )\n",
        "\n",
        "        count = 0\n",
        "        for img_name, label in tqdm(iterator, total=total_items):\n",
        "            if debug_max and count >= debug_max:\n",
        "                break\n",
        "            self._process_and_extract(img_name, label)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Extraction complete. Total patches: {len(self.samples)}\")\n",
        "\n",
        "    def _process_and_extract(self, img_name, label):\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask = Image.open(mask_path).convert(\"L\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load {img_name}: {e}\")\n",
        "            return\n",
        "\n",
        "        img_w, img_h = image.size\n",
        "        # Create binary mask for component labeling\n",
        "        mask_arr = np.array(mask) > 0\n",
        "\n",
        "        # Label connected components (blobs) in the mask\n",
        "        labeled_mask, n_components = ndimage.label(mask_arr)  # type: ignore\n",
        "\n",
        "        for cid in range(1, n_components + 1):\n",
        "            # Extract coordinates of the current blob\n",
        "            ys, xs = np.where(labeled_mask == cid)\n",
        "\n",
        "            # Filter out very small noise artifacts (< 50 pixels)\n",
        "            if len(xs) < 50:\n",
        "                continue\n",
        "\n",
        "            # Calculate the centroid (center of mass) of the blob\n",
        "            cy, cx = int(np.mean(ys)), int(np.mean(xs))\n",
        "\n",
        "            # Define the fixed-size crop window around the centroid\n",
        "            th, tw = self.target_size\n",
        "            half_h, half_w = th // 2, tw // 2\n",
        "\n",
        "            y1 = cy - half_h\n",
        "            y2 = cy + half_h\n",
        "            x1 = cx - half_w\n",
        "            x2 = cx + half_w\n",
        "\n",
        "            # Handle Edge Cases: Calculate intersection with the actual image\n",
        "            img_y1, img_y2 = max(0, y1), min(img_h, y2)\n",
        "            img_x1, img_x2 = max(0, x1), min(img_w, x2)\n",
        "\n",
        "            # Extract the valid region from the image\n",
        "            patch_crop = image.crop((img_x1, img_y1, img_x2, img_y2))\n",
        "\n",
        "            # Calculate required padding if the crop extended beyond image bounds\n",
        "            pad_left = max(0, -x1)\n",
        "            pad_top = max(0, -y1)\n",
        "            pad_right = max(0, x2 - img_w)\n",
        "            pad_bottom = max(0, y2 - img_h)\n",
        "\n",
        "            # If padding is needed, pad with white (255) which is standard background in histology\n",
        "            if pad_left > 0 or pad_top > 0 or pad_right > 0 or pad_bottom > 0:\n",
        "                patch = ImageOps.expand(\n",
        "                    patch_crop,\n",
        "                    border=(pad_left, pad_top, pad_right, pad_bottom),\n",
        "                    fill=255,\n",
        "                )\n",
        "            else:\n",
        "                patch = patch_crop\n",
        "\n",
        "            # Ensure precise size match (e.g., if rounding errors occurred)\n",
        "            if patch.size != self.target_size:\n",
        "                patch = patch.resize(self.target_size, Image.BICUBIC)  # type: ignore\n",
        "\n",
        "            # Store in RAM (Efficient for ~2k images yielding ~10k-20k patches)\n",
        "            self.samples.append(\n",
        "                {\"patch\": np.array(patch), \"label\": label, \"parent\": img_name}\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        img = Image.fromarray(item[\"patch\"])\n",
        "        label = item[\"label\"]\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, label, item[\"parent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147c9151",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_dataset_stats(dataset_class, dataframe, img_dir):\n",
        "    \"\"\"\n",
        "    Computes channel-wise Mean and Std on the dataset without any normalization applied.\n",
        "    \"\"\"\n",
        "    print(\"Computing dataset Mean and Std (this may take a moment)...\")\n",
        "\n",
        "    # define a simple transform that only converts to tensor\n",
        "    basic_transforms = transforms.Compose(\n",
        "        [transforms.Resize(IMG_RESIZE), transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    # Instantiate dataset temporarily\n",
        "    temp_ds = dataset_class(dataframe, img_dir, transforms=basic_transforms)\n",
        "    loader = make_loader(temp_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    nb_samples = 0.0\n",
        "\n",
        "    for data, _, _ in tqdm(loader):\n",
        "        batch_samples = data.size(0)\n",
        "        # Flatten H and W to calculate stats per channel\n",
        "        data = data.view(batch_samples, data.size(1), -1)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        nb_samples += batch_samples\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    print(f\"\\nDONE. Copy these values into your config:\")\n",
        "    print(f\"NEW_MEAN = {mean.tolist()}\")  # type: ignore\n",
        "    print(f\"NEW_STD = {std.tolist()}\")  # type: ignore\n",
        "    return mean.tolist(), std.tolist()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cadb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Calculating stats on Training Data...\")\n",
        "\n",
        "# We use the class we just defined\n",
        "custom_mean, custom_std = compute_dataset_stats(\n",
        "    dataset_class=MaskedFixedTileDataset, dataframe=full_df, img_dir=train_set_dir\n",
        ")\n",
        "\n",
        "NORMALIZATION_MEAN = custom_mean\n",
        "NORMALIZATION_STD = custom_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c1b624",
      "metadata": {},
      "source": [
        "### **Transforms**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a69ba8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "\n",
        "data_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f6b750",
      "metadata": {},
      "source": [
        "## ðŸ§® **Network Parameters**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d25c92c",
      "metadata": {},
      "source": [
        "### **Custom Nets**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f17a8ac",
      "metadata": {
        "id": "3f17a8ac"
      },
      "outputs": [],
      "source": [
        "class EfficientNetCustom(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiates EfficientNet-B0 with ImageNet weights.\n",
        "    Replaces the classifier head with a high-dropout dense layer to prevent overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.EfficientNet_V2_S_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.efficientnet_v2_s(weights=self.weights)\n",
        "\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, self.num_classes),  # type: ignore\n",
        "        )\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all layers except the classifier head\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not name.startswith(\"classifier\"):\n",
        "                param.requires_grad = False\n",
        "        # Ensure classifier params are trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "        # Unfreeze the last n_layers of the backbone (excluding classifier which is already trainable)\n",
        "        child_counter = 0\n",
        "        for child in reversed(list(self.backbone.children())):\n",
        "            child_counter += 1\n",
        "            if child_counter <= n_layers:\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701e496f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DenseNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.DenseNet121_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.densenet121(weights=self.weights)\n",
        "\n",
        "        # DenseNet classifier is stored in .classifier\n",
        "        in_features = self.backbone.classifier.in_features\n",
        "\n",
        "        # Replace Classifier\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all feature layers\n",
        "        for param in self.backbone.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Unfreeze classifier\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_blocks, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "\n",
        "        # Keep classifier trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Unfreeze the last n_blocks within features\n",
        "        children = list(self.backbone.features.children())\n",
        "        total_children = len(children)\n",
        "        if n_blocks <= 0:\n",
        "            return\n",
        "\n",
        "        start = max(0, total_children - n_blocks)\n",
        "        for i in range(start, total_children):\n",
        "            for param in children[i].parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f44462",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper that can interchange between DenseNetCustom and CustomNet.\n",
        "    Keeps the same constructor signature used in the notebook:\n",
        "    CustomNet(num_classes, dropout_rate, backbone=...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4, backbone=\"densenet121\"):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.backbone_name = backbone.lower()\n",
        "\n",
        "        if self.backbone_name in (\"densenet\", \"densenet121\"):\n",
        "            self.backbone = DenseNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        elif self.backbone_name in (\n",
        "            \"efficientnet\",\n",
        "            \"efficientnet_v2s\",\n",
        "            \"efficientnetv2s\",\n",
        "        ):\n",
        "            self.backbone = EfficientNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported backbone '{backbone}'. Use 'densenet' or 'efficientnet'.\"\n",
        "            )\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"freeze_backbone\"):\n",
        "            self.backbone.freeze_backbone()\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"unfreeze_backbone\"):\n",
        "            self.backbone.unfreeze_backbone(n_layers, all=all)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365f9814",
      "metadata": {
        "id": "365f9814"
      },
      "source": [
        "## **XGboost**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e44904",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_mean_aggregation(prob_matrix, k_percent=0.3):\n",
        "    \"\"\"\n",
        "    Aggregates patch probabilities into a slide prediction by averaging\n",
        "    only the most confident patches (Top-K%).\n",
        "\n",
        "    Args:\n",
        "        prob_matrix: Numpy array of shape [N_patches, N_classes]\n",
        "        k_percent: Float (0.0 to 1.0). Percentage of patches to keep.\n",
        "                   0.3 means we only average the top 30% scores.\n",
        "    \"\"\"\n",
        "    n_patches = prob_matrix.shape[0]\n",
        "\n",
        "    # Safety check: if slide has very few patches, keep at least 1\n",
        "    k = max(1, int(n_patches * k_percent))\n",
        "\n",
        "    # Sort probabilities for each class INDEPENDENTLY (Axis 0 = patches)\n",
        "    # We want the highest probabilities for Class 0, Class 1, etc.\n",
        "    sorted_probs = np.sort(prob_matrix, axis=0)\n",
        "\n",
        "    # Take the top K (the last K elements in the sorted array)\n",
        "    top_k_probs = sorted_probs[-k:, :]\n",
        "\n",
        "    # Average them\n",
        "    slide_score = np.mean(top_k_probs, axis=0)\n",
        "\n",
        "    return slide_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a663f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_from_probs(prob_matrix):\n",
        "    \"\"\"\n",
        "    Turns a (N_patches, 4) probability matrix into a single feature vector (1, N_features).\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # 1. Mean Probabilities (Standard Soft Voting) - 4 features\n",
        "    mean_probs = np.mean(prob_matrix, axis=0)\n",
        "    features.extend(mean_probs)\n",
        "\n",
        "    # 2. Max Probabilities (Detect strong tumor signal) - 4 features\n",
        "    max_probs = np.max(prob_matrix, axis=0)\n",
        "    features.extend(max_probs)\n",
        "\n",
        "    # --- NEW INSERTION HERE ---\n",
        "    # 3. Top-K Means (The \"Clean\" Signal)\n",
        "    # Top 30%: Averages the \"surest\" 1/3 of the slide.\n",
        "    # Filters out background but keeps the tumor chunks.\n",
        "    features.extend(top_k_mean_aggregation(prob_matrix, k_percent=0.3))\n",
        "\n",
        "    # Top 10%: Very aggressive. Focuses only on the absolute peak regions.\n",
        "    features.extend(top_k_mean_aggregation(prob_matrix, k_percent=0.1))\n",
        "\n",
        "    # 3. Standard Deviation (Detect tissue heterogeneity) - 4 features\n",
        "    std_probs = np.std(prob_matrix, axis=0)\n",
        "    features.extend(std_probs)\n",
        "\n",
        "    # 4. Percentiles (Robust Max) - 4 features\n",
        "    p90_probs = np.percentile(prob_matrix, 90, axis=0)\n",
        "    features.extend(p90_probs)\n",
        "\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb1386a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def generate_oof_dataset(full_df):\n",
        "    print(\">>> Generating Out-Of-Fold (OOF) Predictions for XGBoost...\")\n",
        "\n",
        "    # We must recreate the exact same split\n",
        "    skf = StratifiedKFold(n_splits=K_FOLD, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # Storage for the Stacking Dataset\n",
        "    X_stack = []  # Features\n",
        "    y_stack = []  # True Labels\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(\n",
        "        skf.split(full_df, full_df[\"label_index\"])\n",
        "    ):\n",
        "        print(f\"Processing OOF for Fold {fold+1}/{K_FOLD}...\")\n",
        "\n",
        "        # Get Validation Data for this fold\n",
        "        fold_val_df = full_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        # Load the Model trained on the OTHER data\n",
        "        model_path = f\"models/{EXPERIMENT_NAME}_fold{fold+1}_ft_model.pt\"\n",
        "        try:\n",
        "            # Initialize appropriate model architecture (EfficientNet or ConvNeXt)\n",
        "            # Ensure this matches what you trained!\n",
        "            model = CustomNet(\n",
        "                num_classes, FT_DROPOUT_RATE, backbone=NET_NAME).to(\n",
        "                device\n",
        "            )\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Skipping Fold {fold+1} (Model not found)\")\n",
        "            continue\n",
        "\n",
        "        # Create Dataset/Loader\n",
        "        val_ds = MaskedFixedTileDataset(\n",
        "            fold_val_df,\n",
        "            train_set_dir,\n",
        "            transforms=data_transforms,\n",
        "            target_size=IMG_RESIZE,\n",
        "        )\n",
        "        val_loader = make_loader(val_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Run Inference\n",
        "        current_probs = {}  # { 'img_name': [ [p0..p3], [p0..p3] ] }\n",
        "        img_labels = {}  # { 'img_name': label_idx }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, parent_ids in tqdm(\n",
        "                val_loader, desc=\"Predicting\", leave=False\n",
        "            ):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # TTA\n",
        "                out1 = F.softmax(model(inputs), dim=1)\n",
        "                out2 = F.softmax(\n",
        "                    model(torch.flip(inputs, [3])), dim=1\n",
        "                )  # Horizontal Flip\n",
        "                out3 = F.softmax(model(torch.flip(inputs, [2])), dim=1)  # Vertical Flip\n",
        "                probs = (out1 + out2 + out3) / 3.0\n",
        "                probs = probs.cpu().numpy()\n",
        "                labels = labels.cpu().numpy()\n",
        "\n",
        "                for i, pid in enumerate(parent_ids):\n",
        "                    if pid not in current_probs:\n",
        "                        current_probs[pid] = []\n",
        "                        img_labels[pid] = labels[i]\n",
        "                    current_probs[pid].append(probs[i])\n",
        "\n",
        "        # Feature Engineering per Slide\n",
        "        for pid, prob_list in current_probs.items():\n",
        "            prob_matrix = np.array(prob_list)\n",
        "\n",
        "            # Extract statistical features\n",
        "            feats = extract_features_from_probs(prob_matrix)\n",
        "\n",
        "            X_stack.append(feats)\n",
        "            y_stack.append(img_labels[pid])\n",
        "\n",
        "    # Convert to Arrays\n",
        "    X_stack = np.array(X_stack)\n",
        "    y_stack = np.array(y_stack)\n",
        "    return X_stack, y_stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ed9dfa",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_stack, y_stack = generate_oof_dataset(full_df)\n",
        "print(f\"OOF Dataset Created. Shape: {X_stack.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e64fd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "\n",
        "def grid_search_xgboost(X_stack, y_stack):\n",
        "    print(\">>> Starting XGBoost Hyperparameter Grid Search...\")\n",
        "    max_folds = 3\n",
        "\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [400, 1000],\n",
        "        \"max_depth\": [3, 5],\n",
        "        \"learning_rate\": [0.0001, 0.00005],\n",
        "        \"colsample_bytree\": [0.5, 1.0],\n",
        "    }\n",
        "\n",
        "    best_params = None\n",
        "    best_score = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "    for params in product(*param_grid.values()):\n",
        "        param_dict = dict(zip(param_grid.keys(), params))\n",
        "\n",
        "        fold_scores = []\n",
        "        fold_models = []\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_stack, y_stack)):\n",
        "            if fold_idx >= max_folds:\n",
        "                break\n",
        "            X_train, X_val = X_stack[train_idx], X_stack[val_idx]\n",
        "            y_train, y_val = y_stack[train_idx], y_stack[val_idx]\n",
        "\n",
        "            sample_weights_train = compute_sample_weight(\n",
        "                class_weight=\"balanced\", y=y_train\n",
        "            )\n",
        "\n",
        "            model = xgb.XGBClassifier(\n",
        "                objective=\"multi:softmax\",\n",
        "                num_class=num_classes,\n",
        "                random_state=SEED,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=1.3,\n",
        "                subsample=0.1,\n",
        "                early_stopping_rounds=70,\n",
        "                **param_dict,\n",
        "            )\n",
        "            model.fit(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                sample_weight=sample_weights_train,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False,\n",
        "            )\n",
        "            preds = model.predict(X_val)\n",
        "            fold_scores.append(f1_score(y_val, preds, average=\"macro\"))\n",
        "            fold_models.append(model)\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        print(f\"Params: {param_dict} => Mean F1 Score: {mean_score:.4f}\")\n",
        "        if mean_score > best_score:\n",
        "            best_score = mean_score\n",
        "            best_params = param_dict\n",
        "            best_model = fold_models[int(np.argmax(fold_scores))]\n",
        "\n",
        "    print(f\"Best F1 Score: {best_score:.4f} with params: {best_params}\")\n",
        "    return best_model, best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6011a548",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model, best_params = grid_search_xgboost(X_stack, y_stack)\n",
        "print(\"XGBoost Grid Search Complete. Best Model Obtained.\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4578c00c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_xgboost_submission(test_loader, cnn_exp_name, xgb_model):\n",
        "    print(\"--- Running Inference with XGBoost Stacking (Vote of Experts)...\")\n",
        "\n",
        "    # Storage for FINAL XGBoost predictions per slide\n",
        "    # { 'slide_id': [xgb_prob_fold1, xgb_prob_fold2, ...] }\n",
        "    slide_final_preds = {}\n",
        "\n",
        "    # 2. Iterate Folds: Run CNN -> Extract Features -> Predict XGB -> Store\n",
        "    for fold in range(1, K_FOLD + 1):\n",
        "        model_path = f\"models/{cnn_exp_name}_fold{fold}_ft_model.pt\"\n",
        "        print(f\"\\n--- Processing Fold {fold}/{K_FOLD} ---\")\n",
        "\n",
        "        # Load CNN\n",
        "        try:\n",
        "            model = CustomNet(num_classes, FT_DROPOUT_RATE, backbone=NET_NAME).to(\n",
        "                device\n",
        "            )\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"WARNING: Skipping Fold {fold} (Model not found)\")\n",
        "            continue\n",
        "\n",
        "        # Dictionary to collect patches for this SPECIFIC fold\n",
        "        current_fold_probs = {}  # { 'pid': [ [p0..p3], [p0..p3] ] }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, _, parent_ids in tqdm(\n",
        "                test_loader, desc=f\"Inference Fold {fold}\", leave=False\n",
        "            ):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # TTA\n",
        "                out1 = F.softmax(model(inputs), dim=1)\n",
        "                out2 = F.softmax(\n",
        "                    model(torch.flip(inputs, [3])), dim=1\n",
        "                )  # Horizontal Flip\n",
        "                out3 = F.softmax(model(torch.flip(inputs, [2])), dim=1)  # Vertical Flip\n",
        "                probs = (out1 + out2 + out3) / 3.0\n",
        "                probs = probs.cpu().numpy()\n",
        "\n",
        "                # Collect patches for this fold\n",
        "                for i, pid in enumerate(parent_ids):\n",
        "                    if pid not in current_fold_probs:\n",
        "                        current_fold_probs[pid] = []\n",
        "                    current_fold_probs[pid].append(probs[i])\n",
        "\n",
        "        # 3. Feature Engineer & Predict with XGBoost for THIS fold\n",
        "        print(f\"XGBoost Prediction for Fold {fold}...\")\n",
        "        for pid, prob_list in current_fold_probs.items():\n",
        "            prob_matrix = np.array(prob_list)\n",
        "\n",
        "            # Extract features (stats) from this fold's CNN predictions\n",
        "            feats = extract_features_from_probs(prob_matrix)\n",
        "\n",
        "            # Predict using XGBoost\n",
        "            # We use predict_proba to get soft voting capability for the stacker\n",
        "            # Shape: (1, n_classes)\n",
        "            xgb_prob = xgb_model.predict_proba(feats.reshape(1, -1))[0]\n",
        "\n",
        "            if pid not in slide_final_preds:\n",
        "                slide_final_preds[pid] = []\n",
        "            slide_final_preds[pid].append(xgb_prob)\n",
        "\n",
        "    # 4. Average XGBoost Predictions (Soft Voting of Stackers)\n",
        "    final_rows = []\n",
        "    print(\"\\nAggregating Ensemble Predictions...\")\n",
        "\n",
        "    for pid, preds_list in slide_final_preds.items():\n",
        "        # preds_list is a list of arrays (one per fold)\n",
        "        # Average them\n",
        "        avg_xgb_probs = np.mean(preds_list, axis=0)\n",
        "\n",
        "        # Final Argmax\n",
        "        pred_idx = np.argmax(avg_xgb_probs)\n",
        "        pred_label = class_names[pred_idx]\n",
        "\n",
        "        final_rows.append({\"sample_index\": pid, \"label\": pred_label})\n",
        "\n",
        "    # Save\n",
        "    sub = pd.DataFrame(final_rows).sort_values(\"sample_index\")\n",
        "    print(\"Submission DataFrame created\")\n",
        "    return sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10df074",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ds = MaskedFixedTileDataset(\n",
        "    None, test_set_dir, transforms=data_transforms, target_size=IMG_RESIZE\n",
        ")\n",
        "\n",
        "test_loader = make_loader(test_ds, BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6163117",
      "metadata": {},
      "outputs": [],
      "source": [
        "# EXECUTE\n",
        "submission_df = generate_xgboost_submission(\n",
        "    test_loader=test_loader,\n",
        "    cnn_exp_name=EXPERIMENT_NAME,\n",
        "    xgb_model=best_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c35b187",
      "metadata": {},
      "outputs": [],
      "source": [
        "sub_path = f\"submission/{EXPERIMENT_NAME}_submission_miglioreSubKaggle_4p052_v2_xgb.csv\"\n",
        "os.makedirs(\"submission\", exist_ok=True)\n",
        "\n",
        "submission_df.to_csv(sub_path, index=False)\n",
        "print(f\"âœ… Saved Robust Stacking Submission: {sub_path}\")\n",
        "\n",
        "submission_df.head(20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
