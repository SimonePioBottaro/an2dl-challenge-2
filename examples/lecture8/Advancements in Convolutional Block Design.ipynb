{"cells":[{"cell_type":"markdown","metadata":{"id":"tRyyx0cfzGF9"},"source":["# **Artificial Neural Networks and Deep Learning**\n","\n","---\n","\n","## **Lecture 7: Advancements in Convolutional Block Design**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1Ruszte0iwJ-i5VgTCApvJXz7yXWgnZzi\" width=\"500\"/>"]},{"cell_type":"markdown","metadata":{"id":"MdD_8Vyswkwf"},"source":["## ‚öôÔ∏è Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_S1JfaW8bIN"},"outputs":[],"source":["# Set seed for reproducibility\n","SEED = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# Import PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","!pip install torchview\n","from torchview import draw_graph\n","\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Device: {device}\")\n","\n","# Import visualization libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"]},{"cell_type":"code","source":["# Define input and output dimensions\n","input_shape = (3, 64, 64)\n","output_shape = 10\n","\n","# Define the batch size\n","BATCH_SIZE = 128\n","\n","# Initialize configuration for convolutional layers\n","stack = 2\n","blocks = 2\n","filters = 32\n","kernel_size = 3\n","\n","\n","print(f\"Input shape: {input_shape}\")\n","print(f\"Output shape: {output_shape}\")\n","print(f\"Batch size: {BATCH_SIZE}\")\n","print(f\"Stack: {stack}\")\n","print(f\"Blocks: {blocks}\")\n","print(f\"Filters: {filters}\")\n","print(f\"Kernel size: {kernel_size}\")"],"metadata":{"id":"nLrhmuHmrxYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **First Convolutional Neural Network Block (AlexNet, 2012)**\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*bD_DMBtKwveuzIkQTwjKQQ.png\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","\n","*   First successful deep CNN for ImageNet\n","*   Introduced ReLU to combat vanishing gradient\n","\n","**Key building block:**\n","\n","*   Conv -> ReLU -> MaxPool sequence\n","*   Multiple layers stacked sequentially\n","\n","**Impact:**\n","\n","*   Started the \"deep learning revolution\"\n","*   Established basic CNN design patterns\n","\n","**üìú Paper:** [\"ImageNet Classification with Deep Convolutional Neural Networks\", Krizhevsky et al.](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n","\n"],"metadata":{"id":"u2arNe9atvaZ"}},{"cell_type":"code","source":["class BasicCNNBlock(nn.Module):\n","    \"\"\"A foundational CNN block: Convolutional layer -> ReLU activation -> Max Pooling.\n","\n","    This block is typical in early CNN architectures like AlexNet, combining feature extraction,\n","    non-linear activation, and dimensionality reduction.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, filters, kernel_size=3, stack=2):\n","        \"\"\"Initialises the BasicCNNBlock.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Number of output filters (channels) for each convolutional layer.\n","            kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n","            stack (int, optional): Number of Conv-ReLU layers before pooling. Defaults to 2.\n","        \"\"\"\n","        super().__init__()\n","\n","        layers_list = []\n","        current_channels = in_channels\n","\n","        # Stack Conv-ReLU layers, preserving spatial dimensions with 'padding=same'.\n","        for i in range(stack):\n","            layers_list.append(\n","                nn.Conv2d(current_channels, filters, kernel_size, padding='same', bias=False) # Bias is often omitted before BatchNorm.\n","            )\n","            layers_list.append(nn.ReLU()) # Apply non-linearity.\n","            current_channels = filters # Update channels for the next layer.\n","\n","        # MaxPool to halve spatial dimensions and provide translation robustness.\n","        layers_list.append(nn.MaxPool2d(2))\n","\n","        self.block = nn.Sequential(*layers_list)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after block processing.\n","        \"\"\"\n","        return self.block(x)\n","\n","\n","class BasicCNNModel(nn.Module):\n","    \"\"\"A complete CNN model constructed from multiple BasicCNNBlocks.\n","\n","    This model exemplifies a basic CNN architecture, stacking convolutional blocks,\n","    followed by flattening and a final dense layer for classification.\n","    \"\"\"\n","\n","    def __init__(self, input_shape, output_shape, filters=64, kernel_size=3, stack=2, blocks=3):\n","        \"\"\"Initialises the BasicCNNModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 64.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of Conv-ReLU layers per block. Defaults to 2.\n","            blocks (int, optional): Number of `BasicCNNBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        block_list = []\n","        current_channels = input_shape[0] # Input channels (e.g., 3 for RGB).\n","        current_filters = filters # Starting filters.\n","\n","        # Create stacked blocks, typically doubling filters for each subsequent block.\n","        for b in range(blocks):\n","            block_list.append(BasicCNNBlock(\n","                in_channels=current_channels,\n","                filters=current_filters,\n","                kernel_size=kernel_size,\n","                stack=stack\n","            ))\n","            current_channels = current_filters # Output channels become input for the next block.\n","            current_filters *= 2 # Double filters for the next block.\n","\n","        self.blocks = nn.Sequential(*block_list)\n","\n","        # Dynamically calculate the flattened size for the linear layer.\n","        # A dummy tensor is used to determine the feature map size post-convolution.\n","        with torch.no_grad():\n","            dummy = torch.zeros(1, *input_shape)\n","            dummy_out = self.blocks(dummy)\n","            flatten_size = dummy_out.view(1, -1).shape[1]\n","\n","        self.flatten = nn.Flatten() # Flatten multi-dimensional output to 1D vector.\n","        self.dense = nn.Linear(flatten_size, output_shape) # Final fully connected layer.\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        x = self.blocks(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1) # Convert logits to probabilities."],"metadata":{"id":"aSb4owGstTcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the Basic CNN model\n","basic_cnn = BasicCNNModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(basic_cnn, input_size=input_shape)\n","model_graph = draw_graph(basic_cnn, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n","model_graph.visual_graph"],"metadata":{"id":"CXrRjILF-1-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Global Average Pooling (NiN, 2013)**\n","\n","<img src=\"https://www.researchgate.net/publication/363231491/figure/fig5/AS:11431281179419529@1691187457237/Illustration-of-global-average-pooling-GAP.png\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","\n","*   Replaced Flatten and Dense layers\n","*   Enforced correspondence between feature maps and categories\n","\n","**Key building block:**\n","\n","*   Global spatial average of each feature map\n","*   Direct feature-to-category mapping\n","\n","**Impact:**\n","\n","*   Dramatic parameter reduction\n","*   Better generalization with fewer parameters\n","\n","**üìú Paper:** [\"Network In Network\", Lin et al.](https://arxiv.org/pdf/1312.4400)\n","\n"],"metadata":{"id":"RkqPKmkQra2A"}},{"cell_type":"code","source":["class GAPModel(nn.Module):\n","    \"\"\"Model using Global Average Pooling (GAP) instead of Flatten + Dense.\n","\n","    This architecture replaces the traditional flattening and fully-connected layers\n","    with Global Average Pooling, which reduces parameters and often improves generalisation.\n","    \"\"\"\n","\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3,\n","                 stack=2, blocks=3):\n","        \"\"\"Initialises the GAPModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of Conv-ReLU layers per block. Defaults to 2.\n","            blocks (int, optional): Number of `BasicCNNBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        # Create multiple BasicCNNBlocks, doubling filters for each subsequent block.\n","        for b in range(blocks):\n","            block = BasicCNNBlock(\n","                in_channels=current_channels,\n","                filters=current_filters,\n","                kernel_size=kernel_size,\n","                stack=stack\n","            )\n","            self.blocks_list.append(block)\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        # Global Average Pooling replaces Flatten + Dense for parameter reduction.\n","        self.gap = nn.AdaptiveAvgPool2d(1) # Pools each feature map to a single value.\n","        self.flatten = nn.Flatten() # Flattens the pooled output.\n","        self.dense = nn.Linear(current_channels, output_shape) # Final classification layer.\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1) # Converts logits to probabilities.\n"],"metadata":{"id":"cx3Mww4hqtd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the GAP model\n","gap_model = GAPModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(gap_model, input_size=input_shape)\n","model_graph = draw_graph(gap_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n","model_graph.visual_graph"],"metadata":{"id":"qsRkQodF_K27"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Inception Block (GoogLeNet, 2014)**\n","\n","<img src=\"https://ar5iv.labs.arxiv.org/html/1707.07128/assets/googlenetInception.png\" width=\"800\"/>\n","\n","\n","---\n","**Key Features and Achievements**\n","\n","\n","*   Multi-scale feature processing\n","*   Winner of ILSVRC 2014\n","\n","**Key building block:**\n","\n","*   Parallel paths with different kernels\n","*   1x1 bottleneck for efficiency\n","*   Feature concatenation\n","\n","**Impact:**\n","\n","*   Established multi-path processing\n","*   Introduced 1x1 bottleneck concept\n","\n","**üìú Paper:** [\"Going deeper with convolutions\", Szegedy et al.](https://arxiv.org/pdf/1409.4842)"],"metadata":{"id":"_Cl1WzXxuOLA"}},{"cell_type":"code","source":["class InceptionModule(nn.Module):\n","    \"\"\"Single Inception module with parallel convolution paths.\n","\n","    This module processes input through 1x1, 3x3, 5x5 convolutions and a max-pooling branch,\n","    concatenating their outputs to capture multi-scale features.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, filters):\n","        \"\"\"Initialises the InceptionModule.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Base number of filters for the module's output, divided among branches.\n","        \"\"\"\n","        super().__init__()\n","\n","        # 1x1 convolution path\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels, filters // 4, 1, padding='same'),\n","            nn.ReLU()\n","        )\n","\n","        # 3x3 convolution path with 1x1 reduction (bottleneck)\n","        self.conv3_reduce = nn.Sequential(\n","            nn.Conv2d(in_channels, filters // 8, 1, padding='same'),\n","            nn.ReLU()\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(filters // 8, filters // 4, 3, padding='same'),\n","            nn.ReLU()\n","        )\n","\n","        # 5x5 convolution path with 1x1 reduction (bottleneck)\n","        self.conv5_reduce = nn.Sequential(\n","            nn.Conv2d(in_channels, filters // 12, 1, padding='same'),\n","            nn.ReLU()\n","        )\n","        self.conv5 = nn.Sequential(\n","            nn.Conv2d(filters // 12, filters // 4, 5, padding='same'),\n","            nn.ReLU()\n","        )\n","\n","        # Max pooling path with 1x1 projection\n","        self.pool = nn.MaxPool2d(3, stride=1, padding=1) # MaxPool maintains spatial dimensions here due to padding.\n","        self.pool_proj = nn.Sequential(\n","            nn.Conv2d(in_channels, filters // 4, 1, padding='same'),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after parallel processing and concatenation.\n","        \"\"\"\n","        conv1 = self.conv1(x)\n","        conv3 = self.conv3(self.conv3_reduce(x))\n","        conv5 = self.conv5(self.conv5_reduce(x))\n","        pool_proj = self.pool_proj(self.pool(x))\n","        # Concatenate outputs along the channel dimension.\n","        return torch.cat([conv1, conv3, conv5, pool_proj], dim=1)\n","\n","\n","class InceptionBlock(nn.Module):\n","    \"\"\"An Inception block consisting of stacked InceptionModules followed by MaxPooling.\n","\n","    This block typically reduces spatial dimensions at its end.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, filters, stack=2):\n","        \"\"\"Initialises the InceptionBlock.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Base filters for the InceptionModules.\n","            stack (int, optional): Number of InceptionModule instances to stack. Defaults to 2.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.inception_modules = nn.ModuleList()\n","        current_channels = in_channels\n","\n","        # Stack multiple InceptionModules.\n","        for s in range(stack):\n","            self.inception_modules.append(InceptionModule(current_channels, filters))\n","            current_channels = filters  # Output channels after concatenation within InceptionModule.\n","\n","        # MaxPooling at the end of the block to halve spatial dimensions.\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after Inception modules and pooling.\n","        \"\"\"\n","        for module in self.inception_modules:\n","            x = module(x)\n","        x = self.pool(x)\n","        return x\n","\n","\n","class InceptionModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple InceptionBlocks and Global Average Pooling.\n","\n","    This model exemplifies an Inception-style architecture, suitable for classification tasks.\n","    \"\"\"\n","\n","    def __init__(self, input_shape, output_shape, filters=32, stack=2, blocks=3):\n","        \"\"\"Initialises the InceptionModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            stack (int, optional): Number of InceptionModule instances per block. Defaults to 2.\n","            blocks (int, optional): Number of `InceptionBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        # Stack multiple InceptionBlocks, typically doubling filters for each.\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                InceptionBlock(current_channels, current_filters, stack)\n","            )\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        # Calculate final filters for the linear layer.\n","        final_filters = filters * (2 ** (blocks - 1))\n","\n","        # Global Average Pooling, Flatten, and Dense layer for classification.\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.flatten = nn.Flatten()\n","        self.dense = nn.Linear(final_filters, output_shape)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"hafbI5_Rqta_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the Inception model\n","inception_model = InceptionModel(input_shape, output_shape, filters, stack, blocks).to(device)\n","summary(inception_model, input_size=input_shape)\n","model_graph = draw_graph(inception_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n","model_graph.visual_graph"],"metadata":{"id":"Kpq352hp_nN-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Batch Normalization (Inception Block with BN, 2015)**\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:898/0*pSSzicm1IH4hXOHc.png\" width=\"800\"/>\n","\n","\n","---\n","**Key Features and Achievements**\n","\n","\n","*   Normalized activations in each layer\n","*   Reduced internal covariate shift\n","\n","**Key building block:**\n","\n","*   Normalize: $\\hat{x} = \\frac{x-\\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}}$\n","*   Scale and shift: $y = \\gamma\\hat{x} + \\beta$\n","*   Placed before activation\n","\n","**Impact:**\n","\n","*   Enabled much faster training\n","*   Reduced sensitivity to initialization\n","*   Became standard in modern networks\n","\n","**üìú Paper:** [\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", Ioffe and Szegedy](https://arxiv.org/pdf/1502.03167)"],"metadata":{"id":"IpTP8L0z2O9J"}},{"cell_type":"code","source":["class InceptionBlockBN(nn.Module):\n","    \"\"\"Inception Block with Batch Normalisation.\n","\n","    Stacks Inception Modules, maintaining spatial dimensions, followed by MaxPool2d.\n","    \"\"\"\n","    def __init__(self, in_channels, filters, stack=2):\n","        \"\"\"Initialises the InceptionBlockBN.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Base number of filters for the module's output.\n","            stack (int, optional): Number of Inception Module instances to stack. Defaults to 2.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.inception_modules = nn.ModuleList()\n","        current_channels = in_channels\n","\n","        for s in range(stack):\n","            module = nn.ModuleDict({\n","                # 1x1 path\n","                'conv1': nn.Sequential(\n","                    nn.Conv2d(current_channels, filters // 4, 1, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters // 4),\n","                    nn.ReLU()\n","                ),\n","                # 3x3 path\n","                'conv3': nn.Sequential(\n","                    nn.Conv2d(current_channels, filters // 8, 1, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters // 8),\n","                    nn.ReLU(),\n","                    nn.Conv2d(filters // 8, filters // 4, 3, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters // 4),\n","                    nn.ReLU()\n","                ),\n","                # 5x5 path\n","                'conv5': nn.Sequential(\n","                    nn.Conv2d(current_channels, filters // 12, 1, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters // 12),\n","                    nn.ReLU(),\n","                    nn.Conv2d(filters // 12, filters // 4, 5, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters // 4),\n","                    nn.ReLU()\n","                ),\n","                # Pool path\n","                'pool': nn.Sequential(\n","                    nn.MaxPool2d(3, stride=1, padding=1),\n","                    nn.Conv2d(current_channels, filters // 4, 1, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters // 4),\n","                    nn.ReLU()\n","                )\n","            })\n","            self.inception_modules.append(module)\n","            current_channels = (filters // 4) * 4 # Output channels after concatenation\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after block processing.\n","        \"\"\"\n","        for module in self.inception_modules:\n","            conv1 = module['conv1'](x)\n","            conv3 = module['conv3'](x)\n","            conv5 = module['conv5'](x)\n","            pool = module['pool'](x)\n","            x = torch.cat([conv1, conv3, conv5, pool], dim=1)\n","\n","        x = self.pool(x)\n","        return x\n","\n","class InceptionBNModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple InceptionBlockBN instances and Global Average Pooling.\n","\n","    Suitable for classification tasks, integrating Batch Normalisation within Inception-style blocks.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, stack=2, blocks=3):\n","        \"\"\"Initialises the InceptionBNModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            stack (int, optional): Number of Inception Module instances per block. Defaults to 2.\n","            blocks (int, optional): Number of `InceptionBlockBN` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                InceptionBlockBN(current_channels, current_filters, stack=stack)\n","            )\n","            current_channels = (current_filters // 4) * 4 # Update channels for next block\n","            current_filters *= 2\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.flatten = nn.Flatten()\n","        self.dense = nn.Linear(current_channels, output_shape)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"H1lCnHdi2PG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the Inception with BN model\n","inception_bn_model = InceptionBNModel(input_shape, output_shape, filters, stack, blocks).to(device)\n","summary(inception_bn_model, input_size=input_shape)\n","model_graph = draw_graph(inception_bn_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n","model_graph.visual_graph"],"metadata":{"id":"L0sKNVCi_5vh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Residual Block (ResNet, 2015)**\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png\" width=\"800\"/>\n","\n","\n","---\n","**Key Features and Achievements**\n","\n","\n","*   Enabled 1000+ layer networks\n","*   Winner of ILSVRC 2015\n","\n","**Key building block:**\n","\n","*   Skip connection: F(x) + x\n","*   Two conv layers with BN and ReLU\n","\n","**Impact:**\n","\n","*   Solved deep network degradation\n","*   Revolutionized network design\n","\n","**üìú Paper:** [\"Deep Residual Learning for Image Recognition\", He et al.](https://arxiv.org/pdf/1512.03385)"],"metadata":{"id":"P1bSqyAL0RzV"}},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    \"\"\"Standard Residual Block with stackable units + Final Pooling.\n","    \"\"\"\n","    def __init__(self, in_channels, filters, kernel_size=3, stack=2):\n","        \"\"\"Initialises the ResidualBlock.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Number of output filters.\n","            kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n","            stack (int, optional): Number of residual units to stack. Defaults to 2.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.units = nn.ModuleList()\n","        current_in = in_channels\n","\n","        for s in range(stack):\n","            layers = nn.ModuleDict()\n","\n","            # Standard ResNet Unit: Conv -> BN -> ReLU -> Conv -> BN -> Add -> ReLU\n","            layers['conv1'] = nn.Conv2d(current_in, filters, kernel_size, padding='same', bias=False)\n","            layers['bn1'] = nn.BatchNorm2d(filters)\n","            layers['conv2'] = nn.Conv2d(filters, filters, kernel_size, padding='same', bias=False)\n","            layers['bn2'] = nn.BatchNorm2d(filters)\n","\n","            # Skip connection projection if dimensions mismatch\n","            if current_in != filters:\n","                layers['proj'] = nn.Sequential(\n","                    nn.Conv2d(current_in, filters, 1, padding='same', bias=False),\n","                    nn.BatchNorm2d(filters)\n","                )\n","\n","            self.units.append(layers)\n","            current_in = filters # After first stack, input matches filters\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after block processing.\n","        \"\"\"\n","        for unit in self.units:\n","            residual = x\n","\n","            out = unit['conv1'](x)\n","            out = unit['bn1'](out)\n","            out = F.relu(out)\n","\n","            out = unit['conv2'](out)\n","            out = unit['bn2'](out)\n","\n","            if 'proj' in unit:\n","                residual = unit['proj'](residual)\n","\n","            out += residual\n","            x = F.relu(out)\n","\n","        return self.pool(x)\n","\n","class ResNetModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple ResidualBlocks and Global Average Pooling.\n","\n","    Suitable for classification tasks with deep architectures.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=2, blocks=3):\n","        \"\"\"Initialises the ResNetModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of residual units per block. Defaults to 2.\n","            blocks (int, optional): Number of `ResidualBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        # Stack ResidualBlocks, typically doubling filters for each.\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                ResidualBlock(current_channels, current_filters, kernel_size, stack)\n","            )\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        # Global Average Pooling, Flatten, and Dense layer for classification.\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.flatten = nn.Flatten()\n","        self.dense = nn.Linear(current_channels, output_shape)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"-a4znc8WqtYM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the ResNet model\n","resnet_model = ResNetModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(resnet_model, input_size=input_shape)\n","model_graph = draw_graph(resnet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n","model_graph.visual_graph"],"metadata":{"id":"R51CF_fYAM8X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Depthwise Separable Convolutions (MobileNetV1, 2017)**\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*yG6z6ESzsRW-9q5F_neOsg.png\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","* Drastically reduced computational cost and model size\n","* Enabled efficient deep learning on mobile and embedded devices\n","* Introduced depthwise separable convolutions as core building block\n","\n","**Key building block**\n","\n","* Depthwise convolution: applies single filter per input channel\n","* Pointwise convolution (1x1): combines outputs from depthwise layer\n","* Reduces computation by factor of 8-9x compared to standard convolutions\n","* Each conv followed by BatchNorm and ReLU\n","\n","**Impact**\n","\n","* Pioneered efficient neural network design for resource-constrained devices\n","* Established depthwise separable convolutions as standard for mobile architectures\n","* Inspired subsequent mobile-optimized architectures (MobileNetV2, V3)\n","\n","**üìú Paper:** [\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\", Howard et al.](https://arxiv.org/pdf/1704.04861)"],"metadata":{"id":"cV0BPAU60i_G"}},{"cell_type":"code","source":["class MobileNetBlock(nn.Module):\n","    \"\"\"\n","    MobileNet Block: Stack of Depthwise Separable Convs + Final Pooling.\n","    \"\"\"\n","    def __init__(self, in_channels, filters, kernel_size=3, stack=2):\n","        \"\"\"Initialises the MobileNetBlock.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Number of output filters.\n","            kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n","            stack (int, optional): Number of Depthwise Separable units to stack. Defaults to 2.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.layers = nn.ModuleList()\n","        current_in = in_channels\n","\n","        for s in range(stack):\n","            # Depthwise convolution: applies a single filter per input channel.\n","            dw = nn.Conv2d(current_in, current_in, kernel_size, padding='same', groups=current_in, bias=False)\n","            bn1 = nn.BatchNorm2d(current_in)\n","            # Pointwise convolution (1x1): combines outputs from depthwise layer.\n","            pw = nn.Conv2d(current_in, filters, 1, bias=False)\n","            bn2 = nn.BatchNorm2d(filters)\n","\n","            self.layers.append(nn.Sequential(\n","                dw, bn1, nn.ReLU(),\n","                pw, bn2, nn.ReLU()\n","            ))\n","            current_in = filters\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after block processing.\n","        \"\"\"\n","        for layer in self.layers:\n","            x = layer(x)\n","        return self.pool(x)\n","\n","class MobileNetV1Model(nn.Module):\n","    \"\"\"Complete CNN model using multiple MobileNetBlocks and Global Average Pooling.\n","\n","    Suitable for efficient classification tasks.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=2, blocks=3):\n","        \"\"\"Initialises the MobileNetV1Model.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of Depthwise Separable units per block. Defaults to 2.\n","            blocks (int, optional): Number of `MobileNetBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        # Initial Conv to expand channels usually helps MobileNet\n","        self.init_conv = nn.Sequential(\n","            nn.Conv2d(current_channels, filters, 3, padding='same', bias=False),\n","            nn.BatchNorm2d(filters),\n","            nn.ReLU()\n","        )\n","        current_channels = filters\n","\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                MobileNetBlock(current_channels, current_filters, kernel_size, stack)\n","            )\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.flatten = nn.Flatten()\n","        self.dense = nn.Linear(current_channels, output_shape)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        x = self.init_conv(x)\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"-6349raN0iy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the MobileNetV1 model\n","mobilenet_model = MobileNetV1Model(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(mobilenet_model, input_size=input_shape)\n","model_graph = draw_graph(mobilenet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n","model_graph.visual_graph"],"metadata":{"id":"kqkJZUEx0ok_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Wide Residual Networks (WideResNet, 2016)**\n","\n","<img src=\"https://pytorch.org/wp-content/uploads/2025/01/wide_resnet-1.png\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","* Demonstrated that wider networks can be more effective than deeper ones\n","* Improved performance with fewer layers through increased channel width\n","* Reduced training time while maintaining or improving accuracy\n","\n","**Key building block:**\n","\n","* Widened residual blocks with widening factor k (typically k=2-12)\n","* Two conv layers with BN-ReLU-Conv ordering (pre-activation)\n","* Dropout between convolutions for regularization\n","\n","**Impact:**\n","\n","* Challenged the \"deeper is better\" paradigm\n","* Showed width as important dimension for network capacity\n","* Influenced efficient network design strategies\n","* Demonstrated effectiveness of proper regularization in wide networks\n","\n","**üìú Paper:** [\"Wide Residual Networks\", Zagoruyko and Komodakis](https://arxiv.org/pdf/1605.07146)"],"metadata":{"id":"JV_JM9m41DJI"}},{"cell_type":"code","source":["class WideResBlock(nn.Module):\n","    \"\"\"\n","    Wide Residual Block: Pre-activation BN->ReLU->Conv.\n","    Uses 'widening_factor' to increase internal filter count.\n","    \"\"\"\n","    def __init__(self, in_channels, filters, kernel_size=3, stack=2, k=2, dropout=0.3):\n","        super().__init__()\n","\n","        self.units = nn.ModuleList()\n","        current_in = in_channels\n","        wide_filters = filters * k\n","\n","        for s in range(stack):\n","            unit = nn.ModuleDict()\n","\n","            # Unit 1: BN -> ReLU -> Conv\n","            unit['bn1'] = nn.BatchNorm2d(current_in)\n","            unit['conv1'] = nn.Conv2d(current_in, wide_filters, kernel_size, padding='same', bias=False)\n","\n","            unit['dropout'] = nn.Dropout(dropout)\n","\n","            # Unit 2: BN -> ReLU -> Conv\n","            unit['bn2'] = nn.BatchNorm2d(wide_filters)\n","            unit['conv2'] = nn.Conv2d(wide_filters, filters, kernel_size, padding='same', bias=False)\n","\n","            # Projection\n","            if current_in != filters:\n","                unit['proj'] = nn.Conv2d(current_in, filters, 1, padding='same', bias=False)\n","\n","            self.units.append(unit)\n","            current_in = filters # Output of block matches 'filters' for outer loop structure.\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        for unit in self.units:\n","            residual = x\n","\n","            out = unit['bn1'](x)\n","            out = F.relu(out)\n","            out = unit['conv1'](out)\n","\n","            out = unit['dropout'](out)\n","\n","            out = unit['bn2'](out)\n","            out = F.relu(out)\n","            out = unit['conv2'](out)\n","\n","            if 'proj' in unit:\n","                residual = unit['proj'](residual)\n","\n","            x = out + residual\n","\n","        return self.pool(x)\n","\n","class WideResNetModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple WideResBlocks and Global Average Pooling.\n","\n","    Suitable for classification tasks with increased width.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=2, blocks=3, k=2):\n","        \"\"\"Initialises the WideResNetModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of residual units per block. Defaults to 2.\n","            blocks (int, optional): Number of `WideResBlock` instances to stack. Defaults to 3.\n","            k (int, optional): Widening factor for the internal layers of `WideResBlock`. Defaults to 2.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        # Initial convolutional layer\n","        self.init_conv = nn.Conv2d(input_shape[0], filters, 3, padding='same', bias=False)\n","\n","        current_channels = filters\n","        current_filters = filters\n","\n","        # Stack WideResBlocks, doubling filters for each.\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                WideResBlock(current_channels, current_filters, kernel_size, stack, k=k)\n","            )\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1) # Global Average Pooling\n","        self.flatten = nn.Flatten() # Flatten multi-dimensional output to 1D vector\n","        self.dense = nn.Linear(current_channels, output_shape) # Final fully connected layer\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        x = self.init_conv(x)\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"I-6WXQm80iwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the WideResNet model\n","wideresnet_model = WideResNetModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(wideresnet_model, input_size=input_shape)\n","model_graph = draw_graph(wideresnet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n","model_graph.visual_graph"],"metadata":{"id":"SdJdDZq-0itb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Aggregated Residual Transformations (ResNeXt, 2017)**\n","\n","<img src=\"https://lh4.googleusercontent.com/0KY-EvKoVReJesGJbgfekaYuNZPVXdyxjBMfwKSgUsU5w9Ajp9j-cTdGjD7rubacjiJj9JLsp64DoC-Cdp5qyNQV-PRIR7EnNZ1jKm3UqsGqro3wNEAOtDphGgERgPz8g7OpSau1\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","* Introduced cardinality (number of paths) as new dimension for network design\n","* Achieved better accuracy than ResNet with similar complexity\n","* Won 2nd place in ILSVRC 2016 classification task\n","\n","**Key building block**\n","\n","* Multiple parallel pathways (cardinality) with same topology\n","* Split-transform-merge strategy: split input, apply transformations, aggregate\n","* Grouped convolutions for efficient implementation\n","\n","**Impact**\n","\n","* Established cardinality as important architectural dimension alongside depth and width\n","* Showed that increasing cardinality is more effective than going deeper or wider\n","* Influenced efficient multi-path architectures\n","* Demonstrated grouped convolutions as powerful design pattern\n","\n","**üìú Paper:** [\"Aggregated Residual Transformations for Deep Neural Networks\", Xie et al.](https://arxiv.org/pdf/1611.05431)"],"metadata":{"id":"bzX2sDVS5L0C"}},{"cell_type":"code","source":["class ResNeXtBlock(nn.Module):\n","    \"\"\"\n","    ResNeXt Block with Cardinality (groups).\n","    This block introduces the concept of 'cardinality', allowing for multiple parallel paths.\n","    \"\"\"\n","    def __init__(self, in_channels, filters, kernel_size=3, stack=2, cardinality=8):\n","        \"\"\"Initialises the ResNeXtBlock.\n","\n","        Args:\n","            in_channels (int): Number of input channels.\n","            filters (int): Number of output filters for the block.\n","            kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n","            stack (int, optional): Number of residual units to stack within the block. Defaults to 2.\n","            cardinality (int, optional): The number of independent paths (groups) in the grouped convolution. Defaults to 8.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.units = nn.ModuleList()\n","        current_in = in_channels\n","\n","        # Calculate the width of the bottleneck layers.\n","        # This width ('d') is typically chosen to be divisible by the cardinality.\n","        d = max(cardinality, filters // 2)\n","\n","        for s in range(stack):\n","            unit = nn.ModuleDict()\n","\n","            # First 1x1 Convolution: Reduces dimensions before the grouped convolution.\n","            unit['conv1'] = nn.Conv2d(current_in, d, 1, bias=False)\n","            unit['bn1'] = nn.BatchNorm2d(d)\n","\n","            # Grouped 3x3 Convolution: The core of ResNeXt, applying 'cardinality' parallel transformations.\n","            unit['conv2'] = nn.Conv2d(d, d, kernel_size, padding='same', groups=cardinality, bias=False)\n","            unit['bn2'] = nn.BatchNorm2d(d)\n","\n","            # Second 1x1 Convolution: Expands dimensions back to 'filters'.\n","            unit['conv3'] = nn.Conv2d(d, filters, 1, bias=False)\n","            unit['bn3'] = nn.BatchNorm2d(filters)\n","\n","            # Projection for the shortcut connection if input and output channels differ.\n","            if current_in != filters:\n","                unit['proj'] = nn.Sequential(\n","                    nn.Conv2d(current_in, filters, 1, bias=False),\n","                    nn.BatchNorm2d(filters)\n","                )\n","\n","            self.units.append(unit)\n","            current_in = filters # The output channels become the input for the next unit in the stack.\n","\n","        self.pool = nn.MaxPool2d(2) # Max pooling at the end of the block for spatial downsampling.\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass of the ResNeXtBlock.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor after block processing.\n","        \"\"\"\n","        for unit in self.units:\n","            residual = x\n","\n","            # Bottleneck convolution with BN and ReLU\n","            out = F.relu(unit['bn1'](unit['conv1'](x)))\n","            # Grouped convolution with BN and ReLU\n","            out = F.relu(unit['bn2'](unit['conv2'](out)))\n","            # Expansion convolution with BN\n","            out = unit['bn3'](unit['conv3'](out))\n","\n","            # Apply projection to residual if needed\n","            if 'proj' in unit:\n","                residual = unit['proj'](residual)\n","\n","            # Element-wise addition of residual and block output, followed by final ReLU\n","            x = F.relu(out + residual)\n","\n","        return self.pool(x)\n","\n","class ResNeXtModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple ResNeXtBlocks and Global Average Pooling.\n","\n","    This model integrates the ResNeXt architecture for classification tasks.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=2, blocks=3, cardinality=8):\n","        \"\"\"Initialises the ResNeXtModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of residual units per block. Defaults to 2.\n","            blocks (int, optional): Number of `ResNeXtBlock` instances to stack. Defaults to 3.\n","            cardinality (int, optional): The number of independent paths in the grouped convolution. Defaults to 8.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","\n","        # Initial convolutional layer to process input, ensuring channels meet cardinality requirement.\n","        start_filters = max(filters, cardinality)\n","        self.init_conv = nn.Sequential(\n","            nn.Conv2d(input_shape[0], start_filters, 3, padding='same', bias=False),\n","            nn.BatchNorm2d(start_filters),\n","            nn.ReLU()\n","        )\n","\n","        current_channels = start_filters\n","        current_filters = start_filters\n","\n","        # Stack multiple ResNeXtBlocks, typically doubling filters for each subsequent block.\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                ResNeXtBlock(current_channels, current_filters, kernel_size, stack, cardinality)\n","            )\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1) # Global Average Pooling to reduce spatial dimensions to 1x1.\n","        self.flatten = nn.Flatten() # Flattens the pooled output into a 1D vector.\n","        self.dense = nn.Linear(current_channels, output_shape) # Final fully connected layer for classification.\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass of the ResNeXtModel.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        x = self.init_conv(x)\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"RxpkbJDt0iqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the ResNeXt model\n","resnext_model = ResNeXtModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(resnext_model, input_size=input_shape)\n","model_graph = draw_graph(resnext_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n","model_graph.visual_graph"],"metadata":{"id":"LxoItguX0ioO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Dense Connections (DenseNet, 2017)**\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*9ysRPSExk0KvXR0AhNnlAA.gif\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","* Revolutionary dense connectivity pattern: each layer connected to all previous layers\n","* Achieved state-of-the-art results with fewer parameters\n","* Won best paper award at CVPR 2017\n","\n","**Key building block:**\n","\n","* Dense connectivity: each layer receives feature maps from all preceding layers\n","* Feature concatenation instead of summation\n","* Growth rate k: each layer adds k feature maps\n","* Composite function: BN-ReLU-Conv\n","\n","**Impact:**\n","\n","* Maximized information flow through direct connections\n","* Alleviated vanishing gradient problem through short paths\n","* Encouraged feature reuse, reducing parameter count\n","* Demonstrated effectiveness of dense connectivity patterns\n","\n","**üìú Paper:** [\"Densely Connected Convolutional Networks\", Huang et al.](https://arxiv.org/pdf/1608.06993)"],"metadata":{"id":"2ObugMEO8OrR"}},{"cell_type":"code","source":["class DenseNetBlock(nn.Module):\n","    \"\"\"\n","    DenseBlock + Transition (Pooling + Channel Resizing).\n","    In this custom implementation, the Transition layer forces the output channels\n","    to match the 'next_filters' expected by the outer loop (Double Filters).\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels_target, growth_rate=12, kernel_size=3, stack=2):\n","        super().__init__()\n","\n","        self.layers = nn.ModuleList()\n","        current_in = in_channels\n","\n","        # Dense Sequence: Each layer concatenates its output to the block's input.\n","        for s in range(stack):\n","            self.layers.append(nn.Sequential(\n","                nn.BatchNorm2d(current_in),\n","                nn.ReLU(),\n","                nn.Conv2d(current_in, growth_rate, kernel_size, padding='same', bias=False)\n","            ))\n","            current_in += growth_rate # The number of channels grows with each layer.\n","\n","        # Transition Layer: 1x1 Conv to set channels -> Pool\n","        # Reduces feature map dimensions and prepares for the next DenseNetBlock.\n","        self.transition = nn.Sequential(\n","            nn.BatchNorm2d(current_in),\n","            nn.ReLU(),\n","            nn.Conv2d(current_in, out_channels_target, 1, bias=False),\n","            nn.MaxPool2d(2)\n","        )\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            out = layer(x)\n","            x = torch.cat([x, out], dim=1) # Concatenate output with previous features.\n","        return self.transition(x)\n","\n","class DenseNetModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple DenseNetBlocks and Global Average Pooling.\n","\n","    This model integrates the DenseNet architecture for classification tasks.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=4, blocks=3):\n","        \"\"\"Initialises the DenseNetModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of layers per dense block. Defaults to 4.\n","            blocks (int, optional): Number of `DenseNetBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        # Initial Conv layer.\n","        self.init_conv = nn.Conv2d(current_channels, filters, 3, padding='same', bias=False)\n","        current_channels = filters\n","\n","        # Stack DenseNetBlocks, where each transition layer aims to halve the feature maps.\n","        for b in range(blocks):\n","            # Target is the filter count for the NEXT block input, effectively doubling filters.\n","            target_filters = current_filters * 2\n","\n","            self.blocks_list.append(\n","                DenseNetBlock(current_channels, target_filters, growth_rate=filters//2, kernel_size=kernel_size, stack=stack)\n","            )\n","            current_channels = target_filters # The transition layer outputs this number of channels.\n","            current_filters *= 2 # Double filters for the next block.\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1) # Global Average Pooling to reduce spatial dimensions to 1x1.\n","        self.flatten = nn.Flatten() # Flattens the pooled output into a 1D vector.\n","        self.dense = nn.Linear(current_channels, output_shape) # Final fully connected layer for classification.\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass of the DenseNetModel.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        x = self.init_conv(x)\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"HbLoV13p0ilu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the DenseNet model\n","densenet_model = DenseNetModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(densenet_model, input_size=input_shape)\n","model_graph = draw_graph(densenet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n","model_graph.visual_graph"],"metadata":{"id":"39YPrPOV0ije"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Compound Scaling (EfficientNet, 2019)**\n","\n","<img src=\"https://storage.googleapis.com/gweb-research2023-media/original_images/9ad0a8c7dd04c2b427a537c6229d39f9-image4.png\" width=\"800\"/>\n","\n","---\n","**Key Features and Achievements**\n","\n","* Introduced compound scaling method: jointly scale depth, width, and resolution\n","* Achieved state-of-the-art accuracy with 8.4x fewer parameters than best existing CNN\n","* Used Neural Architecture Search (NAS) to find optimal baseline network\n","\n","**Key building block:**\n","\n","* Mobile Inverted Bottleneck Conv (MBConv) with squeeze-and-excitation\n","* Compound scaling with fixed ratios: depth √ó width¬≤ √ó resolution¬≤ ‚âà 2\n","* Swish activation function for better accuracy\n","\n","**Impact:**\n","\n","* Demonstrated importance of balanced scaling across all dimensions\n","* Established new paradigm for scaling neural networks efficiently\n","* Showed that careful architecture design and scaling can dramatically improve efficiency\n","* Set new standard for accuracy-efficiency trade-offs\n","\n","**üìú Paper:** [\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\", Tan and Le](https://arxiv.org/pdf/1905.11946)"],"metadata":{"id":"j5apTJnN8sXJ"}},{"cell_type":"code","source":["class MBConvBlock(nn.Module):\n","    \"\"\"\n","    MBConv: Expand (1x1) -> Depthwise (3x3) -> SE -> Project (1x1).\n","    Followed by MaxPool at the end of the stack sequence.\n","    \"\"\"\n","    def __init__(self, in_channels, filters, kernel_size=3, stack=2, expansion=4):\n","        super().__init__()\n","\n","        self.units = nn.ModuleList()\n","        current_in = in_channels\n","\n","        for s in range(stack):\n","            unit = nn.ModuleList()\n","            expanded = current_in * expansion\n","\n","            # Expansion Phase (1x1 convolution to expand channels)\n","            if expansion != 1:\n","                unit.append(nn.Sequential(\n","                    nn.Conv2d(current_in, expanded, 1, bias=False),\n","                    nn.BatchNorm2d(expanded),\n","                    nn.SiLU() # Swish activation\n","                ))\n","\n","            # Depthwise Convolution (applies a single filter per input channel)\n","            unit.append(nn.Sequential(\n","                nn.Conv2d(expanded, expanded, kernel_size, padding='same', groups=expanded, bias=False),\n","                nn.BatchNorm2d(expanded),\n","                nn.SiLU()\n","            ))\n","\n","            # Squeeze and Excitation block\n","            se_in = expanded\n","            se_reduced = max(1, int(se_in * 0.25))\n","            unit.append(nn.Sequential(\n","                nn.AdaptiveAvgPool2d(1),\n","                nn.Conv2d(se_in, se_reduced, 1),\n","                nn.SiLU(),\n","                nn.Conv2d(se_reduced, se_in, 1),\n","                nn.Sigmoid()\n","            ))\n","\n","            # Output Projection Phase (1x1 convolution to project channels back)\n","            unit.append(nn.Sequential(\n","                nn.Conv2d(expanded, filters, 1, bias=False),\n","                nn.BatchNorm2d(filters)\n","            ))\n","\n","            self.units.append(unit)\n","\n","            # Update current_in for the next stacked unit\n","            current_in = filters\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        for unit in self.units:\n","            residual = x\n","\n","            # Expand (if expansion factor is not 1)\n","            out = unit[0](x) if len(unit) == 4 else x\n","            # Depthwise (index shifts if expansion is skipped)\n","            dw_idx = 1 if len(unit) == 4 else 0\n","            out = unit[dw_idx](out)\n","\n","            # Squeeze and Excitation\n","            se_w = unit[dw_idx+1](out)\n","            out = out * se_w\n","\n","            # Project\n","            out = unit[dw_idx+2](out)\n","\n","            # Add residual connection if input and output dimensions match\n","            if x.shape == out.shape:\n","                out += x\n","\n","            x = out\n","\n","        return self.pool(x)\n","\n","class EfficientNetModel(nn.Module):\n","    \"\"\"Complete CNN model using multiple MBConvBlocks and Global Average Pooling.\n","\n","    This model integrates the EfficientNet architecture for classification tasks.\n","    \"\"\"\n","    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=2, blocks=3):\n","        \"\"\"Initialises the EfficientNetModel.\n","\n","        Args:\n","            input_shape (tuple): Shape of the input images (C, H, W).\n","            output_shape (int): Number of output classes.\n","            filters (int, optional): Initial number of filters for the first block. Defaults to 32.\n","            kernel_size (int, optional): Kernel size for convolutional layers. Defaults to 3.\n","            stack (int, optional): Number of MBConv units per block. Defaults to 2.\n","            blocks (int, optional): Number of `MBConvBlock` instances to stack. Defaults to 3.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.blocks_list = nn.ModuleList()\n","        current_channels = input_shape[0]\n","        current_filters = filters\n","\n","        # Initial Convolutional layer\n","        self.init_conv = nn.Sequential(\n","            nn.Conv2d(current_channels, filters, 3, padding='same', bias=False),\n","            nn.BatchNorm2d(filters),\n","            nn.SiLU()\n","        )\n","        current_channels = filters\n","\n","        # Stack multiple MBConvBlocks, typically doubling filters for each subsequent block\n","        for b in range(blocks):\n","            self.blocks_list.append(\n","                MBConvBlock(current_channels, current_filters, kernel_size, stack)\n","            )\n","            current_channels = current_filters\n","            current_filters *= 2\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1) # Global Average Pooling\n","        self.flatten = nn.Flatten() # Flatten multi-dimensional output\n","        self.dense = nn.Linear(current_channels, output_shape) # Final fully connected layer\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass of the EfficientNetModel.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output probabilities after Softmax activation.\n","        \"\"\"\n","        x = self.init_conv(x)\n","        for block in self.blocks_list:\n","            x = block(x)\n","        x = self.gap(x)\n","        x = self.flatten(x)\n","        x = self.dense(x)\n","        return F.softmax(x, dim=1)\n"],"metadata":{"id":"ZDxXnfbd0ig4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create and display the EfficientNetV1 model\n","efficientnet_model = EfficientNetModel(input_shape, output_shape, filters, kernel_size, stack, blocks).to(device)\n","summary(efficientnet_model, input_size=input_shape)\n","model_graph = draw_graph(efficientnet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n","model_graph.visual_graph"],"metadata":{"id":"yFThH8mq8Zto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4tWF6oUQFqH"},"source":["#  \n","<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n","\n","##### Connect with us:\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n","\n","##### Contributors:\n","- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n","- **Alberto Archetti**: alberto.archetti@polimi.it\n","- **Roberto Basla**: roberto.basla@polimi.it\n","- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n","\n","```\n","   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n","\n","   Licensed under the Apache License, Version 2.0 (the \"License\");\n","   you may not use this file except in compliance with the License.\n","   You may obtain a copy of the License at\n","\n","       http://www.apache.org/licenses/LICENSE-2.0\n","\n","   Unless required by applicable law or agreed to in writing, software\n","   distributed under the License is distributed on an \"AS IS\" BASIS,\n","   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","   See the License for the specific language governing permissions and\n","   limitations under the License.\n","```"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}