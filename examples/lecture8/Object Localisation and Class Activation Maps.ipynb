{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePTxbWK9kcvk"
   },
   "source": [
    "# **Artificial Neural Networks and Deep Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lecture 8: Object Localisation and Class Activation Maps**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=14qXmXmQHVwDxXJ3DiVhNmMOcnpA6QMiq\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KUi_Me0ktLM"
   },
   "source": [
    "## üåê **Google Drive Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1WXMl12jKDA"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/gdrive\")\n",
    "current_dir = \"/gdrive/My\\\\ Drive/Colab\\\\ Notebooks/[2025-2026]\\\\ AN2DL/Lecture\\\\ 8\"\n",
    "%cd $current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyaqst6KkwFE"
   },
   "source": [
    "## ‚öôÔ∏è **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GwdBPUWmAZ1"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "# Install and import torchview for model visualisation\n",
    "!pip install -q torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "# Configure device and set seeds for CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Setup directories for models and logs\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import csv\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLX7tUdyv4L3"
   },
   "source": [
    "## üì• **Download Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzyYIpkGv5M_"
   },
   "outputs": [],
   "source": [
    "# Download training images if not already present\n",
    "os.environ[\"TRAINING_DATASET_NAME\"] = \"cats_dogs_images_train.zip\"\n",
    "os.environ[\"TRAINING_DATASET_URL\"] = \"1_fGNrYZxs0yzIJQfUmUWHrWnRisVEYaY\"\n",
    "\n",
    "if not os.path.exists(os.environ[\"TRAINING_DATASET_NAME\"]):\n",
    "    print(\"Downloading training images...\")\n",
    "    !gdown -q ${TRAINING_DATASET_URL} -O ${TRAINING_DATASET_NAME}\n",
    "    print(\"Training images downloaded!\")\n",
    "\n",
    "    print(\"Unzipping training images...\")\n",
    "    !unzip -q -o ${TRAINING_DATASET_NAME}\n",
    "    print(\"Training images unzipped!\")\n",
    "else:\n",
    "    print(\"Training images already available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feJR-e5mqpxl"
   },
   "outputs": [],
   "source": [
    "# Download bounding boxes annotations if not already present\n",
    "os.environ[\"TRAINING_DATASET_BOUNDING_BOXES_NAME\"] = \"cats_dogs_images_boxes.csv\"\n",
    "os.environ[\"TRAINING_DATASET_BOUNDING_BOXES_URL\"] = \"1visBcJA_F9oUOAOTNq6R-MTzkFBXa2LY\"\n",
    "\n",
    "if not os.path.exists(os.environ[\"TRAINING_DATASET_BOUNDING_BOXES_NAME\"]):\n",
    "    print(\"Downloading bounding boxes annotations...\")\n",
    "    !gdown -q ${TRAINING_DATASET_BOUNDING_BOXES_URL} -O ${TRAINING_DATASET_BOUNDING_BOXES_NAME}\n",
    "    print(\"Bounding boxes annotations downloaded!\")\n",
    "else:\n",
    "    print(\"Bounding boxes annotations already available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1WFK9ALqqbS"
   },
   "outputs": [],
   "source": [
    "# Download test images if not already present\n",
    "os.environ[\"TEST_DATASET_NAME\"] = \"cats_dogs_images_test.zip\"\n",
    "os.environ[\"TEST_DATASET_URL\"] = \"1RFJwHLkLdj3RVq-xkYtP_8uLkj5K-obn\"\n",
    "\n",
    "if not os.path.exists(os.environ[\"TEST_DATASET_NAME\"]):\n",
    "    print(\"Downloading test images...\")\n",
    "    !gdown -q ${TEST_DATASET_URL} -O ${TEST_DATASET_NAME}\n",
    "    print(\"Test images downloaded!\")\n",
    "\n",
    "    print(\"Unzipping test images...\")\n",
    "    !unzip -q -o ${TEST_DATASET_NAME}\n",
    "    print(\"Test images unzipped!\")\n",
    "else:\n",
    "    print(\"Test images already available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H1jWqrljZgc"
   },
   "outputs": [],
   "source": [
    "# Download test images (multiple targets) if not already present\n",
    "os.environ[\"TEST_MULTIPLE_DATASET_NAME\"] = \"multiple_cats_dogs_images_test.zip\"\n",
    "os.environ[\"TEST_MULTIPLE_DATASET_URL\"] = \"198qHfig8EwdbSmO1ubUiaHrrgVaFf8gx\"\n",
    "\n",
    "if not os.path.exists(os.environ[\"TEST_MULTIPLE_DATASET_NAME\"]):\n",
    "    print(\"Downloading test (multiple targets) images...\")\n",
    "    !gdown -q ${TEST_MULTIPLE_DATASET_URL} -O ${TEST_MULTIPLE_DATASET_NAME}\n",
    "    print(\"Test images downloaded!\")\n",
    "\n",
    "    print(\"Unzipping test (multiple targets) images...\")\n",
    "    !unzip -q -o ${TEST_MULTIPLE_DATASET_NAME}\n",
    "    print(\"Test images (multiple targets) unzipped!\")\n",
    "else:\n",
    "    print(\"Test images (multiple targets) already available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV62IUgkx6md"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=15T4O0D_r2AF3M1FzHaqf1z2y5NXV43t2\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQT-KIn4JwHz"
   },
   "source": [
    "## ‚öôÔ∏è **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXuMIt1EJxrD"
   },
   "outputs": [],
   "source": [
    "# ImageNet normalisation statistics for pre-trained models\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training hyperparameters\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 10\n",
    "EPOCHS = 1000\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Patience: {PATIENCE}\")\n",
    "print(f\"  Max epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTZB5_g9J58I"
   },
   "source": [
    "## üîß **Data Loading Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujjeuN61eVZX"
   },
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, img_dim):\n",
    "    \"\"\"\n",
    "    Load images from folder and preprocess them.\n",
    "\n",
    "    Args:\n",
    "        folder: Path to folder containing images\n",
    "        img_dim: Target dimension for square images\n",
    "\n",
    "    Returns:\n",
    "        Numpy array of preprocessed images\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        # Centre crop to make image square\n",
    "        dim = min(img.shape[:-1])\n",
    "        img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2,\n",
    "                  (img.shape[1]-dim)//2:(img.shape[1]+dim)//2]\n",
    "\n",
    "        # Resize to fixed size\n",
    "        img = cv2.resize(img, (img_dim, img_dim))\n",
    "\n",
    "        # Convert from BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        images.append(img)\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "def preprocess_dataset(image_dir='cats_dogs_images', image_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Preprocess dataset with bounding box annotations.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing images\n",
    "        image_size: Target image size (width, height)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (labels, bounding_boxes, images)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('cats_dogs_images_boxes.csv')\n",
    "\n",
    "    def process_row(row):\n",
    "        \"\"\"Process a single row from the CSV file.\"\"\"\n",
    "        img_path = row[0]\n",
    "        label = int(row[3])\n",
    "        bbox_coords = list(map(float, row[4:8]))\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(os.path.join(image_dir, img_path))\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        # Get original dimensions\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        x1, y1, x2, y2 = bbox_coords\n",
    "\n",
    "        # Normalise bounding box coordinates to [0, 1]\n",
    "        bbox = [x1/orig_w, y1/orig_h, x2/orig_w, y2/orig_h]\n",
    "\n",
    "        # Resize image and convert colour space\n",
    "        img = cv2.resize(img, image_size)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return label, bbox, img\n",
    "\n",
    "    # Process all rows in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_row, df.itertuples(index=False)))\n",
    "\n",
    "    # Filter out None results and separate components\n",
    "    results = [r for r in results if r is not None]\n",
    "    labels, boxes, img_list = zip(*results)\n",
    "\n",
    "    return np.array(list(labels)), np.array(list(boxes)), np.array(list(img_list))\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAgV-mfaqzaO"
   },
   "source": [
    "## üì¶ **Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MSpEd3sq0zv"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for object localisation and classification.\n",
    "\n",
    "    Supports both regression (bounding boxes) and classification (labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, images, labels=None, boxes=None, augmentation=None):\n",
    "        \"\"\"\n",
    "        Initialise the dataset.\n",
    "\n",
    "        Args:\n",
    "            images: Numpy array of images (H, W, C)\n",
    "            labels: Optional numpy array of class labels\n",
    "            boxes: Optional numpy array of bounding boxes (normalised)\n",
    "            augmentation: Optional torchvision transforms for data augmentation\n",
    "        \"\"\"\n",
    "        # Convert images to tensors and normalise to [0, 1]\n",
    "        self.images = torch.from_numpy(images).float().permute(0, 3, 1, 2) / 255.0\n",
    "\n",
    "        # Convert labels and boxes to tensors if provided\n",
    "        self.labels = torch.from_numpy(labels).long() if labels is not None else None\n",
    "        self.boxes = torch.from_numpy(boxes).float() if boxes is not None else None\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        # ImageNet normalisation for pre-trained models\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "\n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (image, label, box) depending on what was provided\n",
    "        \"\"\"\n",
    "        img = self.images[idx].clone()\n",
    "\n",
    "        # Apply augmentation if provided\n",
    "        if self.augmentation:\n",
    "            img = self.augmentation(img)\n",
    "\n",
    "        # Apply ImageNet normalisation\n",
    "        img = self.normalize(img)\n",
    "\n",
    "        # Build return tuple based on available data\n",
    "        items = [img]\n",
    "        if self.labels is not None:\n",
    "            items.append(self.labels[idx])\n",
    "        if self.boxes is not None:\n",
    "            items.append(self.boxes[idx])\n",
    "\n",
    "        # Return tuple if multiple items, else single item\n",
    "        return tuple(items) if len(items) > 1 else items[0]\n",
    "\n",
    "\n",
    "print(\"CustomDataset class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRYV07uVJ_RQ"
   },
   "source": [
    "## üìä **Load and Prepare Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG34H79-q-_Z"
   },
   "outputs": [],
   "source": [
    "# Preprocess dataset with bounding boxes and labels\n",
    "print(\"Processing images and annotations...\")\n",
    "labels, boxes, img_list = preprocess_dataset()\n",
    "\n",
    "# Shuffle the data\n",
    "combined = list(zip(img_list, boxes, labels))\n",
    "random.shuffle(combined)\n",
    "img_list, boxes, labels = zip(*combined)\n",
    "img_list, boxes, labels = np.array(img_list), np.array(boxes), np.array(labels)\n",
    "\n",
    "print(f\"Total images loaded: {len(img_list)}\")\n",
    "print(f\"Image shape: {img_list[0].shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3R5pDOx0rViC"
   },
   "outputs": [],
   "source": [
    "# Define class names\n",
    "num_to_labels = {0: 'cat', 1: 'dog'}\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for class_idx, class_name in num_to_labels.items():\n",
    "    count = np.sum(labels == class_idx)\n",
    "    print(f\"  {class_name}: {count} images ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWFsoAzEylUZ"
   },
   "outputs": [],
   "source": [
    "def visualize_samples_with_boxes(images, labels, boxes, class_names, num_samples=6):\n",
    "    \"\"\"\n",
    "    Visualise sample images with bounding boxes and labels.\n",
    "\n",
    "    Args:\n",
    "        images: Array of images\n",
    "        labels: Array of class labels\n",
    "        boxes: Array of bounding boxes (normalised coordinates)\n",
    "        class_names: Dictionary mapping class indices to names\n",
    "        num_samples: Number of samples to display\n",
    "    \"\"\"\n",
    "    num_samples = min(num_samples, len(images))\n",
    "\n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(images), num_samples, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plt.suptitle(\"Sample Images with Bounding Boxes\",\n",
    "                fontsize=18, fontweight='bold')\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Get image, label, and box\n",
    "        img = images[idx].copy()\n",
    "        label = labels[idx]\n",
    "        box = boxes[idx]\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        # Draw bounding box (green)\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(img,\n",
    "                     (int(x1*w), int(y1*h)),\n",
    "                     (int(x2*w), int(y2*h)),\n",
    "                     (0, 255, 0), 3)\n",
    "\n",
    "        # Display image\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{class_names[label]}\",\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualise sample images with bounding boxes\n",
    "print(\"Visualising sample images with bounding boxes...\")\n",
    "visualize_samples_with_boxes(img_list, labels, boxes, num_to_labels, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qQeQbCjrW6D"
   },
   "source": [
    "## ‚úÇÔ∏è **Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-hDrN0GrZsx"
   },
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "print(\"\\nSplitting data...\")\n",
    "X_train, X_val, y_train, y_val, box_train, box_val = train_test_split(\n",
    "    img_list, labels, boxes,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Dataset split complete:\")\n",
    "print(f\"  Training:   {len(X_train)} images\")\n",
    "print(f\"  Validation: {len(X_val)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4VD54GVrb3Q"
   },
   "source": [
    "## üé® **Create DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nw8kdhA7rd8s"
   },
   "outputs": [],
   "source": [
    "# Define augmentation for training\n",
    "train_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "print(\"Creating DataLoaders...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y7Hp5fJrfHQ"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for bounding box regression\n",
    "train_box_loader = DataLoader(\n",
    "    CustomDataset(X_train, boxes=box_train, augmentation=train_augmentation),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_box_loader = DataLoader(\n",
    "    CustomDataset(X_val, boxes=box_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Regression DataLoaders created:\")\n",
    "print(f\"  Training batches:   {len(train_box_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_box_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cZh2DAhrggs"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for classification\n",
    "train_cls_loader = DataLoader(\n",
    "    CustomDataset(X_train, labels=y_train, augmentation=train_augmentation),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_cls_loader = DataLoader(\n",
    "    CustomDataset(X_val, labels=y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nClassification DataLoaders created:\")\n",
    "print(f\"  Training batches:   {len(train_cls_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_cls_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBNCpP1Brhlc"
   },
   "source": [
    "## üìè **Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmic42rWrj2S"
   },
   "outputs": [],
   "source": [
    "def spearman_rho(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute Spearman's rank correlation coefficient.\n",
    "\n",
    "    Spearman's rho measures the monotonic relationship between\n",
    "    predicted and target values, suitable for regression evaluation.\n",
    "\n",
    "    Args:\n",
    "        predictions: Predicted values (tensor or numpy array)\n",
    "        targets: Ground truth values (tensor or numpy array)\n",
    "\n",
    "    Returns:\n",
    "        float: Spearman's rho correlation coefficient\n",
    "    \"\"\"\n",
    "    # Convert to tensors if needed\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = torch.from_numpy(predictions)\n",
    "    if isinstance(targets, np.ndarray):\n",
    "        targets = torch.from_numpy(targets)\n",
    "\n",
    "    # Flatten to 1D\n",
    "    predictions = predictions.float().flatten()\n",
    "    targets = targets.float().flatten()\n",
    "\n",
    "    # Compute ranks\n",
    "    def rank(x):\n",
    "        \"\"\"Compute ranks of elements in tensor.\"\"\"\n",
    "        sorted_indices = torch.argsort(x)\n",
    "        ranks = torch.argsort(sorted_indices) + 1\n",
    "        return ranks.float()\n",
    "\n",
    "    rank_pred = rank(predictions)\n",
    "    rank_target = rank(targets)\n",
    "\n",
    "    # Compute Pearson correlation of ranks\n",
    "    mean_pred = torch.mean(rank_pred)\n",
    "    mean_target = torch.mean(rank_target)\n",
    "\n",
    "    diff_pred = rank_pred - mean_pred\n",
    "    diff_target = rank_target - mean_target\n",
    "\n",
    "    covariance = torch.mean(diff_pred * diff_target)\n",
    "    std_pred = torch.sqrt(torch.mean(diff_pred ** 2))\n",
    "    std_target = torch.sqrt(torch.mean(diff_target ** 2))\n",
    "\n",
    "    return covariance / (std_pred * std_target + 1e-8)\n",
    "\n",
    "\n",
    "print(\"Spearman's Rho metric function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2CVh0Brrlw_"
   },
   "source": [
    "## üèóÔ∏è **Build Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtAXibW9roKT"
   },
   "source": [
    "### **Bounding Box Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8dCom5drqHY"
   },
   "outputs": [],
   "source": [
    "class BoxRegressorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Bounding box regressor using pre-trained EfficientNetB0.\n",
    "\n",
    "    The model predicts 4 normalised coordinates: (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initialise the regressor.\n",
    "\n",
    "        Args:\n",
    "            dropout_rate: Dropout rate for regularisation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained EfficientNetB0\n",
    "        self.backbone = torchvision.models.efficientnet_b0(\n",
    "            weights=torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "\n",
    "        # Freeze backbone to use as feature extractor\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Get input features for classifier\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "\n",
    "        # Replace classifier with regression head (4 outputs for bounding box)\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, 3, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Predicted bounding boxes of shape (batch, 4)\n",
    "        \"\"\"\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "print(\"BoxRegressorModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od_x48DjrrpP"
   },
   "outputs": [],
   "source": [
    "# Instantiate bounding box regressor\n",
    "box_model = BoxRegressorModel().to(device)\n",
    "\n",
    "print(\"\\nBox Regressor Model Summary:\")\n",
    "print(\"=\"*60)\n",
    "summary(box_model, input_size=(3, IMG_SIZE, IMG_SIZE))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-2pnMLOr1jP"
   },
   "outputs": [],
   "source": [
    "# Visualise model architecture\n",
    "model_graph = draw_graph(\n",
    "    box_model,\n",
    "    input_size=(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE),\n",
    "    expand_nested=True,\n",
    "    depth=6\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2zkFGI2sBlM"
   },
   "outputs": [],
   "source": [
    "def train_box_epoch(model, loader, criterion, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    Train bounding box regressor for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: Regressor model\n",
    "        loader: Training DataLoader\n",
    "        criterion: Loss function (MSE)\n",
    "        optimizer: Optimiser\n",
    "        scaler: Mixed precision scaler\n",
    "\n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    for img, box in loader:\n",
    "        img, box = img.to(device), box.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            pred = model(img)\n",
    "            loss = criterion(pred, box)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += loss.item() * img.size(0)\n",
    "\n",
    "    return loss_sum / len(loader.dataset)\n",
    "\n",
    "\n",
    "def val_box_epoch(model, loader, criterion):\n",
    "    \"\"\"\n",
    "    Validate bounding box regressor for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: Regressor model\n",
    "        loader: Validation DataLoader\n",
    "        criterion: Loss function (MSE)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (average_loss, spearman_correlation)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, box in loader:\n",
    "            img, box = img.to(device), box.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                pred = model(img)\n",
    "                loss = criterion(pred, box)\n",
    "\n",
    "            loss_sum += loss.item() * img.size(0)\n",
    "\n",
    "            # Collect predictions for Spearman correlation\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(box.cpu().numpy())\n",
    "\n",
    "    # Calculate Spearman's rho on entire validation set\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    spearman = spearman_rho(all_preds, all_targets)\n",
    "\n",
    "    return loss_sum / len(loader.dataset), float(spearman)\n",
    "\n",
    "def fit_box_regressor(model, train_loader, val_loader, epochs, criterion, optimizer, scaler,\n",
    "                      patience=10, experiment_name=\"box_regressor\"):\n",
    "    \"\"\"\n",
    "    Complete training loop for bounding box regressor with early stopping.\n",
    "\n",
    "    Args:\n",
    "        model: Regressor model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        epochs: Maximum number of epochs\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimiser\n",
    "        scaler: Mixed precision scaler\n",
    "        patience: Early stopping patience\n",
    "        experiment_name: Name for saving model and logs\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (trained_model, history_dict)\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(f\"./{logs_dir}/{experiment_name}\")\n",
    "\n",
    "    # Initialise history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_spearman': []\n",
    "    }\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f\"Starting training: {experiment_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training phase\n",
    "        train_loss = train_box_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_spearman = val_box_epoch(model, val_loader, criterion)\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_spearman'].append(val_spearman)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Spearman/Val', val_spearman, epoch)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train MSE: {train_loss:.4f} | \"\n",
    "                  f\"Val MSE: {val_loss:.4f} | \"\n",
    "                  f\"Val Spearman: {val_spearman:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"models/{experiment_name}_best.pt\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # Restore best weights\n",
    "    model.load_state_dict(torch.load(f\"models/{experiment_name}_best.pt\"))\n",
    "    print(f\"Best model restored from epoch {best_epoch}\")\n",
    "    print(f\"  Val MSE: {best_loss:.4f}\")\n",
    "    print(f\"  Val Spearman: {history['val_spearman'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model, history\n",
    "\n",
    "\n",
    "print(\"Regression training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySP9kAHCsIve"
   },
   "source": [
    "### üöÄ **Train Bounding Box Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_HjwqOksK92"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Setup training for bounding box regressor\n",
    "box_optimizer = torch.optim.Adam(box_model.parameters(), lr=LEARNING_RATE)\n",
    "box_criterion = nn.MSELoss()\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# Train the model\n",
    "box_model, box_history = fit_box_regressor(\n",
    "    model=box_model,\n",
    "    train_loader=train_box_loader,\n",
    "    val_loader=val_box_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=box_criterion,\n",
    "    optimizer=box_optimizer,\n",
    "    scaler=scaler,\n",
    "    patience=PATIENCE,\n",
    "    experiment_name=\"efficientnet_box_regressor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM99wnbCuIzB"
   },
   "source": [
    "### üìà **Plot Regression Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2M_quFGuLBu"
   },
   "outputs": [],
   "source": [
    "# Plot training history for bounding box regressor\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot MSE Loss\n",
    "ax1.plot(box_history['train_loss'], alpha=0.3, color='#4D61E2', linestyle='--', label='Training')\n",
    "ax1.plot(box_history['val_loss'], alpha=0.8, color='#4D61E2', label='Validation')\n",
    "ax1.set_title('Bounding Box Regression Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot Spearman's Rho\n",
    "ax2.plot(box_history['val_spearman'], alpha=0.8, color='#408537', label='Validation Spearman')\n",
    "ax2.set_title(\"Spearman's Rho Correlation\", fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Rho')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vttSWEgnuVil"
   },
   "source": [
    "### üëÅÔ∏è **Visualise Bounding Box Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irpIX-qluXmi"
   },
   "outputs": [],
   "source": [
    "# Visualise predictions on validation set\n",
    "box_model.eval()\n",
    "\n",
    "# Get a batch from validation loader\n",
    "val_iter = iter(val_box_loader)\n",
    "imgs, gt_boxes = next(val_iter)\n",
    "imgs = imgs.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    pred_boxes = box_model(imgs).cpu().numpy()\n",
    "\n",
    "# Visualise 10 sample predictions\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(min(10, len(imgs))):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Denormalise image for visualisation\n",
    "    img_np = imgs[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    # Convert to uint8 for drawing\n",
    "    img_copy = (img_np * 255).astype(np.uint8).copy()\n",
    "    h, w, _ = img_copy.shape\n",
    "\n",
    "    # Draw ground truth bounding box (green)\n",
    "    gx1, gy1, gx2, gy2 = gt_boxes[i].numpy()\n",
    "    cv2.rectangle(img_copy,\n",
    "                  (int(gx1*w), int(gy1*h)),\n",
    "                  (int(gx2*w), int(gy2*h)),\n",
    "                  (0, 255, 0), 4)\n",
    "\n",
    "    # Draw predicted bounding box (red)\n",
    "    px1, py1, px2, py2 = pred_boxes[i]\n",
    "    cv2.rectangle(img_copy,\n",
    "                  (int(px1*w), int(py1*h)),\n",
    "                  (int(px2*w), int(py2*h)),\n",
    "                  (255, 0, 0), 4)\n",
    "\n",
    "    ax.imshow(img_copy)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Bounding Box Predictions on Validation Set (Green: Ground Truth | Red: Prediction)\",\n",
    "            fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPzTDu3nr31R"
   },
   "source": [
    "### **Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzIRyTwvr9e9"
   },
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary classifier using pre-trained EfficientNetB0.\n",
    "\n",
    "    Classifies images as cat (0) or dog (1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=2, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initialise the classifier.\n",
    "\n",
    "        Args:\n",
    "            num_classes: Number of output classes\n",
    "            dropout_rate: Dropout rate for regularisation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained EfficientNetB0\n",
    "        self.backbone = torchvision.models.efficientnet_b0(\n",
    "            weights=torchvision.models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "\n",
    "        # Freeze backbone initially\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Get input features for classifier\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "\n",
    "        # Replace classifier head\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, 3, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Class logits of shape (batch, num_classes)\n",
    "        \"\"\"\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "print(\"ClassifierModel defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX_eX5tLr9cp"
   },
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "cls_model = ClassifierModel().to(device)\n",
    "\n",
    "print(\"\\nClassifier Model Summary:\")\n",
    "print(\"=\"*60)\n",
    "summary(cls_model, input_size=(3, IMG_SIZE, IMG_SIZE))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d5X8BxMr9aF"
   },
   "outputs": [],
   "source": [
    "# Visualise classifier architecture\n",
    "model_graph = draw_graph(\n",
    "    cls_model,\n",
    "    input_size=(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE),\n",
    "    expand_nested=True,\n",
    "    depth=6\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQ_HGqHir9Xb"
   },
   "outputs": [],
   "source": [
    "def train_cls_epoch(model, loader, criterion, optimizer, scaler):\n",
    "    \"\"\"\n",
    "    Train classifier for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        loader: Training DataLoader\n",
    "        criterion: Loss function (CrossEntropy)\n",
    "        optimizer: Optimiser\n",
    "        scaler: Mixed precision scaler\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (average_loss, average_accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    acc_list = []\n",
    "\n",
    "    for img, lbl in loader:\n",
    "        img, lbl = img.to(device), lbl.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            out = model(img)\n",
    "            loss = criterion(out, lbl)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += loss.item() * img.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        acc_list.append(accuracy_score(lbl.cpu(), preds.cpu()))\n",
    "\n",
    "    return loss_sum / len(loader.dataset), np.mean(acc_list)\n",
    "\n",
    "\n",
    "def val_cls_epoch(model, loader, criterion):\n",
    "    \"\"\"\n",
    "    Validate classifier for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        loader: Validation DataLoader\n",
    "        criterion: Loss function (CrossEntropy)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (average_loss, average_accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    acc_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, lbl in loader:\n",
    "            img, lbl = img.to(device), lbl.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                out = model(img)\n",
    "                loss = criterion(out, lbl)\n",
    "\n",
    "            loss_sum += loss.item() * img.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            acc_list.append(accuracy_score(lbl.cpu(), preds.cpu()))\n",
    "\n",
    "    return loss_sum / len(loader.dataset), np.mean(acc_list)\n",
    "\n",
    "def fit_classifier(model, train_loader, val_loader, epochs, criterion, optimizer, scaler,\n",
    "                   patience=10, experiment_name=\"classifier\"):\n",
    "    \"\"\"\n",
    "    Complete training loop for classifier with early stopping.\n",
    "\n",
    "    Args:\n",
    "        model: Classifier model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        epochs: Maximum number of epochs\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimiser\n",
    "        scaler: Mixed precision scaler\n",
    "        patience: Early stopping patience\n",
    "        experiment_name: Name for saving model and logs\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (trained_model, history_dict)\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(f\"./{logs_dir}/{experiment_name}\")\n",
    "\n",
    "    # Initialise history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    best_acc = float('-inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f\"Starting training: {experiment_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_cls_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = val_cls_epoch(model, val_loader, criterion)\n",
    "\n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Val', val_acc, epoch)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"models/{experiment_name}_best.pt\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # Restore best weights\n",
    "    model.load_state_dict(torch.load(f\"models/{experiment_name}_best.pt\"))\n",
    "    print(f\"Best model restored from epoch {best_epoch}\")\n",
    "    print(f\"  Val Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model, history\n",
    "\n",
    "print(\"Classification training functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efx5GZNWvGTM"
   },
   "source": [
    "### üöÄ **Train Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzmPH4Kfr9Va"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Setup training for classifier\n",
    "cls_optimizer = torch.optim.Adam(cls_model.parameters(), lr=LEARNING_RATE)\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "cls_model, cls_history = fit_classifier(\n",
    "    model=cls_model,\n",
    "    train_loader=train_cls_loader,\n",
    "    val_loader=val_cls_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=cls_criterion,\n",
    "    optimizer=cls_optimizer,\n",
    "    scaler=scaler,\n",
    "    patience=PATIENCE,\n",
    "    experiment_name=\"efficientnet_classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB7Idbsav0DB"
   },
   "source": [
    "### üìà **Plot Classification Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNH9KB9xr9S8"
   },
   "outputs": [],
   "source": [
    "# Plot training history for classifier\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(cls_history['train_loss'], alpha=0.3, color='#ff7f0e', linestyle='--', label='Train')\n",
    "ax1.plot(cls_history['val_loss'], alpha=0.8, color='#ff7f0e', label='Val')\n",
    "ax1.set_title('Cross-Entropy Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(cls_history['train_acc'], alpha=0.3, color='#ff7f0e', linestyle='--', label='Train')\n",
    "ax2.plot(cls_history['val_acc'], alpha=0.8, color='#ff7f0e', label='Val')\n",
    "ax2.set_title('Classification Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvN1ZodLv4pU"
   },
   "source": [
    "### üîß **Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twuiVDK6r9Q4"
   },
   "outputs": [],
   "source": [
    "def unfreeze_and_finetune(model):\n",
    "    \"\"\"\n",
    "    Unfreeze the last 3 blocks of EfficientNet features for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        model: Classifier model with frozen backbone\n",
    "\n",
    "    Returns:\n",
    "        Model with partially unfrozen backbone\n",
    "    \"\"\"\n",
    "    # Unfreeze last 3 blocks of features\n",
    "    for param in model.backbone.features[-3:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    print(\"Last 3 blocks of backbone unfrozen for fine-tuning\")\n",
    "\n",
    "    # Count trainable parameters after unfreezing\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"\\nParameter breakdown after unfreezing:\")\n",
    "    print(f\"  Total parameters:     {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Trainable percentage: {trainable_params/total_params*100:.1f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Apply unfreezing\n",
    "cls_model = unfreeze_and_finetune(cls_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Cqiasmbr9On"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Setup fine-tuning with lower learning rate\n",
    "ft_optimizer = torch.optim.Adam(cls_model.parameters(), lr=1e-4)\n",
    "ft_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nStarting fine-tuning phase...\")\n",
    "\n",
    "# Fine-tune the model\n",
    "cls_model, ft_history = fit_classifier(\n",
    "    model=cls_model,\n",
    "    train_loader=train_cls_loader,\n",
    "    val_loader=val_cls_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=ft_criterion,\n",
    "    optimizer=ft_optimizer,\n",
    "    scaler=scaler,\n",
    "    patience=5,\n",
    "    experiment_name=\"efficientnet_finetuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jbfae0quwFH0"
   },
   "source": [
    "### üìà **Plot Fine-Tuning Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOaRyzp3r9M4"
   },
   "outputs": [],
   "source": [
    "# Plot fine-tuning history\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(ft_history['train_loss'], alpha=0.3, color='#2ca02c', linestyle='--', label='Train')\n",
    "ax1.plot(ft_history['val_loss'], alpha=0.8, color='#2ca02c', label='Val')\n",
    "ax1.set_title('Cross-Entropy Loss (Fine-Tuned)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(ft_history['train_acc'], alpha=0.3, color='#2ca02c', linestyle='--', label='Train')\n",
    "ax2.plot(ft_history['val_acc'], alpha=0.8, color='#2ca02c', label='Val')\n",
    "ax2.set_title('Classification Accuracy (Fine-Tuned)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl-EVx6Lwvfn"
   },
   "source": [
    "### üìä **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWec9KsIr9Kb"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, loader, class_names):\n",
    "    \"\"\"\n",
    "    Compute and plot confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        loader: Validation DataLoader\n",
    "        class_names: Dictionary mapping class indices to names\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Collect predictions\n",
    "    with torch.no_grad():\n",
    "        for img, lbl in loader:\n",
    "            img, lbl = img.to(device), lbl.to(device)\n",
    "            out = model(img)\n",
    "            preds = out.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(lbl.cpu().numpy())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(class_names.values()),\n",
    "                yticklabels=list(class_names.values()),\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.title('Confusion Matrix (Fine-Tuned Model)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate confusion matrix\n",
    "print(\"Generating confusion matrix...\")\n",
    "plot_confusion_matrix(cls_model, val_cls_loader, num_to_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDvWoSWqwzx1"
   },
   "source": [
    "## üß™ **Test Set Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOqt93SLr9Ih"
   },
   "outputs": [],
   "source": [
    "# Load test images\n",
    "test_path = 'localization_test/'\n",
    "print(f\"Loading test images from {test_path}...\")\n",
    "X_test_raw = load_images_from_folder(test_path, IMG_SIZE)\n",
    "\n",
    "print(f\"Loaded {len(X_test_raw)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw9rC55Rw2-0"
   },
   "outputs": [],
   "source": [
    "# Prepare test data transformations\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Apply transformations\n",
    "X_test_tensor = torch.stack([\n",
    "    transform_test(Image.fromarray(img)) for img in X_test_raw\n",
    "]).to(device)\n",
    "\n",
    "print(f\"Test tensor shape: {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBNqnpkcw27m"
   },
   "outputs": [],
   "source": [
    "# Run inference on test set\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "\n",
    "cls_model.eval()\n",
    "box_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Classification predictions\n",
    "    cls_logits = cls_model(X_test_tensor)\n",
    "    cls_probs = torch.softmax(cls_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Bounding box predictions\n",
    "    box_preds = box_model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "print(f\"Inference complete on {len(X_test_raw)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJDywgA7w6io"
   },
   "source": [
    "### üëÅÔ∏è **Visualise Combined Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Dw_MmJWw24u"
   },
   "outputs": [],
   "source": [
    "# Visualise combined classification and localisation results\n",
    "num_img = len(X_test_raw)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 9))\n",
    "plt.suptitle(\"Combined Inference: Classification + Localisation\",\n",
    "            fontsize=18, fontweight='bold')\n",
    "\n",
    "for i in range(min(10, num_img)):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img = X_test_raw[i].copy()\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Draw predicted bounding box (green)\n",
    "    bx = box_preds[i]\n",
    "    cv2.rectangle(img,\n",
    "                  (int(bx[0]*w), int(bx[1]*h)),\n",
    "                  (int(bx[2]*w), int(bx[3]*h)),\n",
    "                  (0, 255, 0), 3)\n",
    "\n",
    "    # Get predicted label and confidence\n",
    "    lbl_idx = np.argmax(cls_probs[i])\n",
    "    conf = cls_probs[i][lbl_idx] * 100\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{num_to_labels[lbl_idx]}: {conf:.1f}%\",\n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu8s_couw_cB"
   },
   "source": [
    "## üî• **Class Activation Maps (CAM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4toGRNSBw227"
   },
   "outputs": [],
   "source": [
    "# @title üëÅÔ∏è **Visualise CAM**\n",
    "def visualize_cam(model, images_tensor, original_images, target_class=None):\n",
    "    \"\"\"\n",
    "    Visualise Class Activation Maps (CAM).\n",
    "\n",
    "    CAM highlights the regions that are most important for classification.\n",
    "\n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        images_tensor: Preprocessed images tensor\n",
    "        original_images: Original images for visualisation\n",
    "        target_class: Optional specific class to visualise (if None, uses predicted class)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook the last feature layer\n",
    "    target_layer = model.backbone.features[-1]\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        \"\"\"Save activations from forward pass.\"\"\"\n",
    "        activations.append(output)\n",
    "\n",
    "    # Register forward hook\n",
    "    handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Get weights from final linear layer\n",
    "    fc_weights = model.backbone.classifier[1].weight.data.cpu().numpy()\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images_tensor)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    # Remove hook\n",
    "    handle.remove()\n",
    "\n",
    "    # Get activations\n",
    "    acts = activations[0].cpu().numpy()\n",
    "\n",
    "    # Visualise CAMs - Modified logic based on input length\n",
    "    num_total_imgs = len(images_tensor)\n",
    "\n",
    "    if num_total_imgs == 10:\n",
    "        num_rows = 2\n",
    "        num_cols = 5\n",
    "        plot_count = 10\n",
    "        figsize = (20, 9)\n",
    "    else:\n",
    "        num_rows = 1\n",
    "        num_cols = 5 # Always create 5 columns for the \"otherwise\" case\n",
    "        plot_count = min(5, num_total_imgs) # Plot only up to 5 images\n",
    "        figsize = (20, 4)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "\n",
    "    # Ensure axes is always a 1D iterable array for consistent access\n",
    "    if num_rows * num_cols == 1:\n",
    "        axes = np.array([axes]) # Make it an array of one element\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "\n",
    "    title_suffix = f\"Class: {num_to_labels[target_class]}\" if target_class is not None else \"Predicted Class\"\n",
    "    plt.suptitle(f'Class Activation Maps - {title_suffix}',\n",
    "                fontsize=18, fontweight='bold')\n",
    "\n",
    "    for i in range(plot_count):\n",
    "        ax = axes[i] # Use flattened axes for consistent access\n",
    "\n",
    "        # Use target class or predicted class\n",
    "        c_idx = target_class if target_class is not None else preds[i]\n",
    "\n",
    "        # Compute weighted CAM\n",
    "        cam = np.zeros(acts.shape[2:], dtype=np.float32)\n",
    "        for w, feat in zip(fc_weights[c_idx], acts[i]):\n",
    "            cam += w * feat\n",
    "\n",
    "        # Resize and normalise\n",
    "        cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n",
    "        cam = np.maximum(cam, 0)  # ReLU activation\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Display original image with CAM overlay\n",
    "        ax.imshow(original_images[i])\n",
    "        ax.imshow(cam, cmap='turbo', alpha=0.5)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{num_to_labels[c_idx]}\", fontsize=13, fontweight='bold')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(plot_count, num_rows * num_cols):\n",
    "        if j < len(axes): # Check if the axis exists before trying to hide it\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate CAMs for predicted classes\n",
    "print(\"Generating Class Activation Maps...\")\n",
    "visualize_cam(cls_model, X_test_tensor, X_test_raw, target_class=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "6afJeRdEw2xl"
   },
   "outputs": [],
   "source": [
    "# @title üì¶ **Bounding Box from CAM**\n",
    "def get_bbox_from_heatmap(heatmap, threshold_percent=0.2):\n",
    "    \"\"\"\n",
    "    Generate bounding box from heatmap using thresholding.\n",
    "\n",
    "    Args:\n",
    "        heatmap: 2D array (H, W) normalised to [0,1]\n",
    "        threshold_percent: Threshold value (0-1) for binary mask\n",
    "\n",
    "    Returns:\n",
    "        Tuple (x, y, w, h) or None if no contours found\n",
    "    \"\"\"\n",
    "    # Apply threshold\n",
    "    threshold_val = np.max(heatmap) * threshold_percent\n",
    "    _, binary_map = cv2.threshold(heatmap, threshold_val, 255, cv2.THRESH_BINARY)\n",
    "    binary_map = binary_map.astype(np.uint8)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) > 0:\n",
    "        # Take the largest contour\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        return x, y, w, h\n",
    "\n",
    "    return None\n",
    "\n",
    "def visualize_cam_with_bbox(model, images_tensor, original_images):\n",
    "    \"\"\"\n",
    "    Visualise CAM and extract bounding boxes from activation maps.\n",
    "\n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        images_tensor: Preprocessed images tensor\n",
    "        original_images: Original images for visualisation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook setup\n",
    "    target_layer = model.backbone.features[-1]\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    handle = target_layer.register_forward_hook(hook_fn)\n",
    "    fc_weights = model.backbone.classifier[1].weight.data.cpu().numpy()\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images_tensor)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    handle.remove()\n",
    "    acts = activations[0].cpu().numpy()\n",
    "\n",
    "    # Visualise\n",
    "    num_imgs = min(10, len(images_tensor))\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 9))\n",
    "    plt.suptitle(\"CAM-based Bounding Box Extraction\",\n",
    "                fontsize=18, fontweight='bold')\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "        c_idx = preds[i]\n",
    "\n",
    "        # Compute CAM\n",
    "        cam = np.zeros(acts.shape[2:], dtype=np.float32)\n",
    "        for w, feat in zip(fc_weights[c_idx], acts[i]):\n",
    "            cam += w * feat\n",
    "        cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Extract bounding box from CAM\n",
    "        img_copy = original_images[i].copy()\n",
    "        bbox = get_bbox_from_heatmap(cam, threshold_percent=0.3)\n",
    "\n",
    "        # Draw bounding box if found\n",
    "        if bbox is not None:\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(img_copy, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "\n",
    "        # Display\n",
    "        ax.imshow(img_copy)\n",
    "        ax.imshow(cam, cmap='turbo', alpha=0.4)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{num_to_labels[c_idx]}\", fontsize=13, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Extract bounding boxes from CAMs\n",
    "print(\"Extracting bounding boxes from CAMs...\")\n",
    "visualize_cam_with_bbox(cls_model, X_test_tensor, X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wA-pcTDecFrF"
   },
   "outputs": [],
   "source": [
    "# @title üëÅÔ∏è **Visualise CAM Channel Decomposition**\n",
    "def visualize_cam_channels(model, image_tensor, original_image, target_class=None, top_n=6):\n",
    "    \"\"\"\n",
    "    Visualise individual feature map channels and their weighted contribution to CAM.\n",
    "\n",
    "    This function shows how the Class Activation Map is computed as a weighted\n",
    "    sum of feature maps from the last convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        image_tensor: Preprocessed image tensor (single image)\n",
    "        original_image: Original image for visualisation\n",
    "        target_class: Target class index (if None, uses predicted class)\n",
    "        top_n: Number of top weighted channels to display\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook the last feature layer\n",
    "    target_layer = model.backbone.features[-1]\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Get weights from final linear layer\n",
    "    fc_weights = model.backbone.classifier[1].weight.data.cpu().numpy()\n",
    "\n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        pred_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    # Get activations\n",
    "    acts = activations[0].cpu().numpy()[0]  # Shape: (num_channels, H, W)\n",
    "\n",
    "    # Use target class or predicted class\n",
    "    c_idx = target_class if target_class is not None else pred_class\n",
    "\n",
    "    # Get weights for target class\n",
    "    weights = fc_weights[c_idx]\n",
    "\n",
    "    # Sort channels by weight value (take only positive contributors)\n",
    "    channel_importance = [(i, weights[i]) for i in range(len(weights)) if weights[i] > 0]\n",
    "    channel_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top N channels\n",
    "    top_channels = channel_importance[:top_n]\n",
    "\n",
    "    # Compute final CAM\n",
    "    cam = np.zeros(acts.shape[1:], dtype=np.float32)\n",
    "    for channel_idx, _ in channel_importance:\n",
    "        cam += weights[channel_idx] * acts[channel_idx]\n",
    "    cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "    # Create visualisation - single row\n",
    "    fig, axes = plt.subplots(1, top_n + 2, figsize=(22, 4))\n",
    "\n",
    "    # Title\n",
    "    fig.suptitle(f'CAM Channel Decomposition - Class: {num_to_labels[c_idx]}',\n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    # Plot original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original Image', fontsize=11, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Collect all feature maps for consistent color scaling\n",
    "    all_maps = []\n",
    "\n",
    "    for i, (channel_idx, weight) in enumerate(top_channels):\n",
    "        # Get feature map\n",
    "        feature_map = acts[channel_idx]\n",
    "        feature_map = cv2.resize(feature_map, (IMG_SIZE, IMG_SIZE))\n",
    "        feature_map = np.maximum(feature_map, 0)\n",
    "\n",
    "        all_maps.append(feature_map)\n",
    "\n",
    "    # Find global min/max for consistent coloring\n",
    "    vmin = min(m.min() for m in all_maps)\n",
    "    vmax = max(m.max() for m in all_maps)\n",
    "\n",
    "    # Plot feature maps with consistent scaling\n",
    "    for i, (channel_idx, weight) in enumerate(top_channels):\n",
    "        feature_map = all_maps[i]\n",
    "\n",
    "        # Plot feature map\n",
    "        ax = axes[i + 1]\n",
    "        ax.imshow(feature_map, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'Channel {channel_idx}\\nw={weight:.4f}',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot final CAM\n",
    "    axes[-1].imshow(original_image)\n",
    "    axes[-1].imshow(cam, cmap='turbo', alpha=0.6)\n",
    "    axes[-1].set_title('Final CAM\\n(Weighted Sum)',\n",
    "                      fontsize=11, fontweight='bold')\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    # Add equation text with proper formula: w √ó Channel\n",
    "    equation_parts = [f'w{i+1} √ó Ch{top_channels[i][0]}' for i in range(len(top_channels))]\n",
    "    equation_text = ' + '.join(equation_parts) + ' + ... = CAM'\n",
    "    fig.text(0.5, -0.05, equation_text, ha='center', fontsize=12,\n",
    "            fontweight='bold', style='italic')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualise CAM channels for test images\n",
    "print(\"Visualising CAM channel decomposition...\")\n",
    "\n",
    "# Select a test image\n",
    "test_idx = 8\n",
    "test_img_tensor = X_test_tensor[test_idx:test_idx+1]\n",
    "test_img_orig = X_test_raw[test_idx]\n",
    "\n",
    "visualize_cam_channels(cls_model, test_img_tensor, test_img_orig, target_class=None, top_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jika2H6Wpxdn"
   },
   "source": [
    "### **Multiple Targets CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-Pa6RtikDEt"
   },
   "outputs": [],
   "source": [
    "# Load test (multiple targets) images\n",
    "test_path_multiple = 'multiple_cats_dogs_images_test/'\n",
    "print(f\"Loading test (multiple targets) images from {test_path}...\")\n",
    "X_test_multiple_raw = load_images_from_folder(test_path_multiple, IMG_SIZE)\n",
    "\n",
    "print(f\"Loaded {len(X_test_multiple_raw)} test images\")\n",
    "\n",
    "# Apply transformations\n",
    "X_test_multiple_tensor = torch.stack([\n",
    "    transform_test(Image.fromarray(img)) for img in X_test_multiple_raw\n",
    "]).to(device)\n",
    "\n",
    "print(f\"Test tensor (multiple targets) shape: {X_test_multiple_tensor.shape}\")\n",
    "\n",
    "# Run inference on test set\n",
    "print(\"Running inference on test set (multiple targets)...\")\n",
    "\n",
    "cls_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Classification predictions\n",
    "    cls_multiple_logits = cls_model(X_test_multiple_tensor)\n",
    "\n",
    "print(f\"Inference complete on {len(X_test_multiple_raw)} test images (multiple targets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kHM15JwKlLJ7"
   },
   "outputs": [],
   "source": [
    "#@title **üê± Generating CAM visualisations for class Cat**\n",
    "print(\"Generating Class Activation Maps...\")\n",
    "visualize_cam(cls_model, X_test_multiple_tensor, X_test_multiple_raw, target_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "l8oaoW8sqsv2"
   },
   "outputs": [],
   "source": [
    "#@title **üê∂ Generating CAM visualisations for class Dog**\n",
    "print(\"Generating Class Activation Maps...\")\n",
    "visualize_cam(cls_model, X_test_multiple_tensor, X_test_multiple_raw, target_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nTmffDWxMa4"
   },
   "source": [
    "## üéØ **Grad-CAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ndwgORkuw2ur"
   },
   "outputs": [],
   "source": [
    "# @title üëÅÔ∏è **Visualise Grad-CAM**\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM (Gradient-weighted Class Activation Mapping) implementation.\n",
    "\n",
    "    Grad-CAM uses gradients flowing into the final convolutional layer\n",
    "    to produce a coarse localisation map highlighting important regions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        \"\"\"\n",
    "        Initialise Grad-CAM.\n",
    "\n",
    "        Args:\n",
    "            model: Trained classifier\n",
    "            target_layer: Target convolutional layer for CAM generation\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.handles = []\n",
    "\n",
    "        # Register forward and backward hooks\n",
    "        self.handles.append(\n",
    "            target_layer.register_forward_hook(self.save_activation)\n",
    "        )\n",
    "        self.handles.append(\n",
    "            target_layer.register_full_backward_hook(self.save_gradient)\n",
    "        )\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        \"\"\"Save activations from forward pass.\"\"\"\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        \"\"\"Save gradients from backward pass.\"\"\"\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove registered hooks.\"\"\"\n",
    "        for h in self.handles:\n",
    "            h.remove()\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM heatmap.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor (single image)\n",
    "            class_idx: Target class index (if None, uses predicted class)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (normalised_cam, class_index)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "\n",
    "        # Backward pass for target class\n",
    "        output[0, class_idx].backward()\n",
    "\n",
    "        # Compute importance weights (average gradients)\n",
    "        grads = self.gradients.cpu().numpy()[0]\n",
    "        acts = self.activations.detach().cpu().numpy()[0]\n",
    "        weights = np.mean(grads, axis=(1, 2))\n",
    "\n",
    "        # Generate weighted CAM\n",
    "        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * acts[i]\n",
    "\n",
    "        # Apply ReLU and resize\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (x.shape[3], x.shape[2]))\n",
    "\n",
    "        # Normalise\n",
    "        return cam / (cam.max() + 1e-8), class_idx\n",
    "\n",
    "\n",
    "print(\"Generating Grad-CAM visualisations...\")\n",
    "\n",
    "# Initialise Grad-CAM on last feature layer\n",
    "gcam = GradCAM(cls_model, cls_model.backbone.features[-1])\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 9))\n",
    "plt.suptitle(\"Grad-CAM: Predicted Class Visualisation\",\n",
    "            fontsize=18, fontweight='bold')\n",
    "\n",
    "for i in range(min(10, len(X_test_raw))):\n",
    "    # Prepare single image tensor with gradients enabled\n",
    "    img_t = X_test_tensor[i:i+1]\n",
    "    img_t.requires_grad = True\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    mask, c_idx = gcam(img_t)\n",
    "\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(X_test_raw[i])\n",
    "    ax.imshow(mask, cmap='jet', alpha=0.6)\n",
    "    ax.set_title(num_to_labels[c_idx], fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cleanup\n",
    "gcam.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "T6lT-sSvw2rX"
   },
   "outputs": [],
   "source": [
    "#@title üì¶ **Bounding Box from Grad-CAM**\n",
    "def visualize_gradcam_with_bbox(model, images_tensor, original_images):\n",
    "    \"\"\"\n",
    "    Visualise Grad-CAM and extract bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        images_tensor: Preprocessed images tensor\n",
    "        original_images: Original images for visualisation\n",
    "    \"\"\"\n",
    "    # Initialise Grad-CAM\n",
    "    gcam_tool = GradCAM(model, model.backbone.features[-1])\n",
    "\n",
    "    num_imgs = min(10, len(images_tensor))\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 9))\n",
    "    plt.suptitle(\"Grad-CAM-based Bounding Box Extraction\",\n",
    "                fontsize=18, fontweight='bold')\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        ax = axes[i // 5, i % 5]\n",
    "\n",
    "        # Prepare image with gradients\n",
    "        img_t = images_tensor[i:i+1]\n",
    "        img_t.requires_grad = True\n",
    "\n",
    "        # Generate Grad-CAM heatmap\n",
    "        heatmap, pred_c = gcam_tool(img_t)\n",
    "\n",
    "        # Extract bounding box\n",
    "        img_copy = original_images[i].copy()\n",
    "        bbox = get_bbox_from_heatmap(heatmap, threshold_percent=0.3)\n",
    "\n",
    "        # Draw bounding box if found\n",
    "        if bbox is not None:\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(img_copy, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "\n",
    "        # Display\n",
    "        ax.imshow(img_copy)\n",
    "        ax.imshow(heatmap, cmap='jet', alpha=0.4)\n",
    "        ax.set_title(f\"{num_to_labels[pred_c]}\", fontsize=13, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Cleanup\n",
    "    gcam_tool.cleanup()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Extract bounding boxes from Grad-CAMs\n",
    "print(\"Extracting bounding boxes from Grad-CAMs...\")\n",
    "visualize_gradcam_with_bbox(cls_model, X_test_tensor, X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0ZXPeG6ScriL"
   },
   "outputs": [],
   "source": [
    "#@title üëÅÔ∏è **Visualise Grad-CAM Channel Decomposition**\n",
    "def visualize_gradcam_channels(model, image_tensor, original_image, target_class=None, top_n=6):\n",
    "    \"\"\"\n",
    "    Visualise individual feature map channels and their gradient-weighted contribution to Grad-CAM.\n",
    "\n",
    "    This function shows how Grad-CAM is computed as a gradient-weighted\n",
    "    sum of feature maps from the last convolutional layer.\n",
    "\n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        image_tensor: Preprocessed image tensor (single image, requires_grad=True)\n",
    "        original_image: Original image for visualisation\n",
    "        target_class: Target class index (if None, uses predicted class)\n",
    "        top_n: Number of top weighted channels to display\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook setup\n",
    "    target_layer = model.backbone.features[-1]\n",
    "    activations = []\n",
    "    gradients = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients.append(grad_output[0])\n",
    "\n",
    "    handle_forward = target_layer.register_forward_hook(forward_hook)\n",
    "    handle_backward = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    model.zero_grad()\n",
    "    image_tensor.requires_grad = True\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    # Get predicted class\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    c_idx = target_class if target_class is not None else pred_class\n",
    "\n",
    "    # Backward pass\n",
    "    output[0, c_idx].backward()\n",
    "\n",
    "    # Remove hooks\n",
    "    handle_forward.remove()\n",
    "    handle_backward.remove()\n",
    "\n",
    "    # Get activations and gradients\n",
    "    acts = activations[0].cpu().detach().numpy()[0]  # Shape: (num_channels, H, W)\n",
    "    grads = gradients[0].cpu().numpy()[0]\n",
    "\n",
    "    # Compute importance weights (global average pooling of gradients)\n",
    "    weights = np.mean(grads, axis=(1, 2))\n",
    "\n",
    "    # Sort channels by weight value (take only positive contributors)\n",
    "    channel_importance = [(i, weights[i]) for i in range(len(weights)) if weights[i] > 0]\n",
    "    channel_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top N channels\n",
    "    top_channels = channel_importance[:top_n]\n",
    "\n",
    "    # Compute final Grad-CAM\n",
    "    cam = np.zeros(acts.shape[1:], dtype=np.float32)\n",
    "    for i in range(len(weights)):\n",
    "        cam += weights[i] * acts[i]\n",
    "    cam = cv2.resize(cam, (IMG_SIZE, IMG_SIZE))\n",
    "    cam = np.maximum(cam, 0)  # ReLU\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "    # Create visualisation - single row\n",
    "    fig, axes = plt.subplots(1, top_n + 2, figsize=(22, 4))\n",
    "\n",
    "    # Title\n",
    "    fig.suptitle(f'Grad-CAM Channel Decomposition - Class: {num_to_labels[c_idx]}',\n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    # Plot original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original Image', fontsize=11, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Collect all feature maps for consistent color scaling\n",
    "    all_maps = []\n",
    "\n",
    "    for i, (channel_idx, weight) in enumerate(top_channels):\n",
    "        # Get feature map\n",
    "        feature_map = acts[channel_idx]\n",
    "        feature_map = cv2.resize(feature_map, (IMG_SIZE, IMG_SIZE))\n",
    "        feature_map = np.maximum(feature_map, 0)\n",
    "\n",
    "        all_maps.append(feature_map)\n",
    "\n",
    "    # Find global min/max for consistent coloring\n",
    "    vmin = min(m.min() for m in all_maps)\n",
    "    vmax = max(m.max() for m in all_maps)\n",
    "\n",
    "    # Plot feature maps with consistent scaling\n",
    "    for i, (channel_idx, weight) in enumerate(top_channels):\n",
    "        feature_map = all_maps[i]\n",
    "\n",
    "        # Plot feature map\n",
    "        ax = axes[i + 1]\n",
    "        ax.imshow(feature_map, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'Channel {channel_idx}\\nŒ±={weight:.4f}',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot final Grad-CAM\n",
    "    axes[-1].imshow(original_image)\n",
    "    axes[-1].imshow(cam, cmap='jet', alpha=0.6)\n",
    "    axes[-1].set_title('Final Grad-CAM\\n(Weighted Sum)',\n",
    "                      fontsize=11, fontweight='bold')\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "    # Add equation text with proper formula: Œ± √ó Channel\n",
    "    equation_parts = [f'Œ±{i+1} √ó Ch{top_channels[i][0]}' for i in range(len(top_channels))]\n",
    "    equation_text = ' + '.join(equation_parts) + ' + ... = Grad-CAM'\n",
    "    fig.text(0.5, -0.05, equation_text, ha='center', fontsize=12,\n",
    "            fontweight='bold', style='italic')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualise Grad-CAM channels for test images\n",
    "print(\"Visualising Grad-CAM channel decomposition...\")\n",
    "\n",
    "# Select a test image\n",
    "test_idx = 8\n",
    "test_img_tensor = X_test_tensor[test_idx:test_idx+1].clone()\n",
    "test_img_tensor.requires_grad = True\n",
    "test_img_orig = X_test_raw[test_idx]\n",
    "\n",
    "visualize_gradcam_channels(cls_model, test_img_tensor, test_img_orig, target_class=None, top_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dhQlh03ew2oP"
   },
   "outputs": [],
   "source": [
    "#@title üîç **Multi-Layer Grad-CAM Comparison**\n",
    "# Compare Grad-CAM at different layers\n",
    "layers_to_viz = [\n",
    "    cls_model.backbone.features[-5],\n",
    "    cls_model.backbone.features[-3],\n",
    "    cls_model.backbone.features[-1]\n",
    "]\n",
    "layer_names = [\"Layer -5 (Early)\", \"Layer -3 (Mid)\", \"Layer -1 (Late)\"]\n",
    "\n",
    "num_imgs = min(10, len(X_test_raw))\n",
    "fig, axes = plt.subplots(num_imgs, 4, figsize=(16, 4 * num_imgs))\n",
    "\n",
    "plt.suptitle(\"Multi-Layer Grad-CAM Comparison\",\n",
    "            fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx in range(num_imgs):\n",
    "    # Column 0: Original image\n",
    "    axes[idx, 0].imshow(X_test_raw[idx])\n",
    "    axes[idx, 0].set_title(\"Original\", fontsize=12, fontweight='bold')\n",
    "    axes[idx, 0].axis('off')\n",
    "\n",
    "    # Columns 1-3: Grad-CAMs for different layers\n",
    "    for l_idx, layer in enumerate(layers_to_viz):\n",
    "        gcam_tool = GradCAM(cls_model, layer)\n",
    "\n",
    "        img_t = X_test_tensor[idx:idx+1]\n",
    "        img_t.requires_grad = True\n",
    "\n",
    "        mask, pred_c = gcam_tool(img_t)\n",
    "\n",
    "        ax = axes[idx, l_idx + 1]\n",
    "        ax.imshow(X_test_raw[idx])\n",
    "        ax.imshow(mask, cmap='jet', alpha=0.6)\n",
    "        ax.set_title(f\"{layer_names[l_idx]}\\n{num_to_labels[pred_c]}\",\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "        gcam_tool.cleanup()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vlCuzc5PlzS-"
   },
   "outputs": [],
   "source": [
    "#@title **üê± Generating Grad-CAM visualisations for class Cat**\n",
    "\n",
    "# Choose target class\n",
    "TARGET_CLASS = 0  # 0 = 'cat', 1 = 'dog'\n",
    "\n",
    "# Initialise Grad-CAM on last feature layer\n",
    "gcam = GradCAM(cls_model, cls_model.backbone.features[-1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4)) # Changed to 1 row, 5 columns, adjusted figsize\n",
    "plt.suptitle(f\"Grad-CAM: Forced Class '{num_to_labels[TARGET_CLASS]}', first 5 images\", # Added 'first 5 images'\n",
    "            fontsize=18, fontweight='bold')\n",
    "\n",
    "for i in range(min(5, len(X_test_multiple_raw))): # Changed range to 5 to fit 5 columns\n",
    "    # Prepare single image tensor with gradients enabled\n",
    "    img_t = X_test_multiple_tensor[i:i+1]\n",
    "    img_t.requires_grad = True\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    mask, c_idx = gcam(img_t, class_idx=TARGET_CLASS)\n",
    "\n",
    "    ax = axes[i % 5] # Adjusted indexing for a single row\n",
    "    ax.imshow(X_test_multiple_raw[i])\n",
    "    ax.imshow(mask, cmap='jet', alpha=0.6)\n",
    "    ax.set_title(num_to_labels[c_idx], fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cleanup\n",
    "gcam.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "95ajF-c5n5Ac"
   },
   "outputs": [],
   "source": [
    "#@title **üê∂ Generating Grad-CAM visualisations for class Dog**\n",
    "\n",
    "# Choose target class\n",
    "TARGET_CLASS = 1  # 0 = 'cat', 1 = 'dog'\n",
    "\n",
    "# Initialise Grad-CAM on last feature layer\n",
    "gcam = GradCAM(cls_model, cls_model.backbone.features[-1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4)) # Changed to 1 row, 5 columns, adjusted figsize\n",
    "plt.suptitle(f\"Grad-CAM: Forced Class '{num_to_labels[TARGET_CLASS]}', first 5 images\", # Added 'first 5 images'\n",
    "            fontsize=18, fontweight='bold')\n",
    "\n",
    "for i in range(min(5, len(X_test_multiple_raw))): # Changed range to 5 to fit 5 columns\n",
    "    # Prepare single image tensor with gradients enabled\n",
    "    img_t = X_test_multiple_tensor[i:i+1]\n",
    "    img_t.requires_grad = True\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    mask, c_idx = gcam(img_t, class_idx=TARGET_CLASS)\n",
    "\n",
    "    ax = axes[i % 5] # Adjusted indexing for a single row\n",
    "    ax.imshow(X_test_multiple_raw[i])\n",
    "    ax.imshow(mask, cmap='jet', alpha=0.6)\n",
    "    ax.set_title(num_to_labels[c_idx], fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cleanup\n",
    "gcam.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M13VY1FknD7"
   },
   "source": [
    "#  \n",
    "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
    "\n",
    "##### Connect with us:\n",
    "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n",
    "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n",
    "\n",
    "##### Contributors:\n",
    "- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n",
    "- **Alberto Archetti**: alberto.archetti@polimi.it\n",
    "- **Roberto Basla**: roberto.basla@polimi.it\n",
    "- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n",
    "\n",
    "```\n",
    "   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
