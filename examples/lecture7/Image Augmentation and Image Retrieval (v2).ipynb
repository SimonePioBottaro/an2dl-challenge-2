{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWbrjTk9ILxx"
   },
   "source": [
    "# **Artificial Neural Networks and Deep Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lecture 7: Image Augmentation and Image Retrieval**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1idXlqipXxn5yDono2mWxxZ94TxSkjiZ4\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wiFI7GaIbna"
   },
   "source": [
    "## üåê **Google Drive Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rOS1_jPUIIQA"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3668161216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcurrent_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/gdrive/My\\\\ Drive/Colab\\\\ Notebooks/[2025-2026]\\\\ AN2DL/Lecture\\\\ 7\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$current_dir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/gdrive\")\n",
    "current_dir = \"/gdrive/My\\\\ Drive/Colab\\\\ Notebooks/[2025-2026]\\\\ AN2DL/Lecture\\\\ 7\"\n",
    "%cd $current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i74fcelIeDa"
   },
   "source": [
    "## ‚öôÔ∏è **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWxWVeN9I7XI"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "!pip install torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "!pkill -f tensorboard\n",
    "%load_ext tensorboard\n",
    "!mkdir -p models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5x8J_1MJB54"
   },
   "source": [
    "## ‚è≥ **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbwoRdJDI-8F"
   },
   "outputs": [],
   "source": [
    "# Set environment variables for Animals dataset\n",
    "os.environ[\"ANIMALS_DATASET_NAME\"] = \"animals.zip\"\n",
    "os.environ[\"ANIMALS_DATASET_URL\"] = \"1nlTR-mwPLc05vxaOncUhXu82NR8gbx63\"\n",
    "\n",
    "# Check if Animals dataset exists, download and unzip if not\n",
    "if not os.path.exists(os.environ[\"ANIMALS_DATASET_NAME\"]):\n",
    "    print(\"Downloading Animals dataset...\")\n",
    "    !gdown -q ${ANIMALS_DATASET_URL} -O ${ANIMALS_DATASET_NAME}\n",
    "    print(\"Animals dataset downloaded!\")\n",
    "\n",
    "    print(\"Unzipping Animals dataset...\")\n",
    "    !unzip -o ${ANIMALS_DATASET_NAME}\n",
    "    print(\"Animals dataset unzipped!\")\n",
    "else:\n",
    "    print(\"Animals dataset already downloaded and unzipped. Using cached data.\")\n",
    "\n",
    "# Set environment variables for Items dataset\n",
    "os.environ[\"ITEMS_DATASET_NAME\"] = \"items.zip\"\n",
    "os.environ[\"ITEMS_DATASET_URL\"] = \"1tcDVgQYuMnISgFCjaxinXSryB0CAZYHP\"\n",
    "\n",
    "# Check if Items dataset exists, download and unzip if not\n",
    "if not os.path.exists(os.environ[\"ITEMS_DATASET_NAME\"]):\n",
    "    print(\"Downloading Items dataset...\")\n",
    "    !gdown -q ${ITEMS_DATASET_URL} -O ${ITEMS_DATASET_NAME}\n",
    "    print(\"Items dataset downloaded!\")\n",
    "\n",
    "    print(\"Unzipping Items dataset...\")\n",
    "    !unzip -o ${ITEMS_DATASET_NAME}\n",
    "    print(\"Items dataset unzipped!\")\n",
    "else:\n",
    "    print(\"Items dataset already downloaded and unzipped. Using cached data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQzqSBYBJFYw"
   },
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    \"\"\"\n",
    "    Load and preprocess images from a specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Path to the folder containing images\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of preprocessed images with shape (N, H, W, C)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "\n",
    "    # Iterate through files in the specified folder\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "\n",
    "        # Normalize image pixel values to a float range [0, 1]\n",
    "        img = (img / 255).astype(np.float32)\n",
    "\n",
    "        # Convert image from BGR to RGB\n",
    "        img = img[...,::-1]\n",
    "\n",
    "        # Make the image dataset squared\n",
    "        dim = min(img.shape[:-1])\n",
    "        img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2,\n",
    "                  (img.shape[1]-dim)//2:(img.shape[1]+dim)//2, :]\n",
    "\n",
    "        # Resize the image to 224x224 pixels\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "# Load images from the 'animals/' folder\n",
    "animals_path = 'animals/'\n",
    "animals = load_images_from_folder(animals_path)\n",
    "\n",
    "# Load images from the 'items/' folder\n",
    "items_path = 'items/'\n",
    "items = load_images_from_folder(items_path)\n",
    "\n",
    "print(f\"Loaded {len(animals)} animal images\")\n",
    "print(f\"Loaded {len(items)} item images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sibgfmyKJNHh"
   },
   "source": [
    "## üîé **Exploration and Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3tIg18TJQeP"
   },
   "outputs": [],
   "source": [
    "# Number of images to display\n",
    "num_img = 10\n",
    "\n",
    "# Create subplots for displaying items\n",
    "fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
    "for i in range(num_img):\n",
    "    ax = axes[i%2, i%num_img//2]\n",
    "    ax.imshow(np.clip(items[i], 0, 1))  # Display clipped item images\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create subplots for displaying animals\n",
    "fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
    "for i in range(num_img):\n",
    "    ax = axes[i%2, i%num_img//2]\n",
    "    ax.imshow(np.clip(animals[i], 0, 1))  # Display clipped animal images\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUMnZpPGJWpK"
   },
   "source": [
    "## üîÑ **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ldh38ipsJXLQ"
   },
   "outputs": [],
   "source": [
    "# Concatenate 'animals' and 'items' arrays along axis 0\n",
    "X = np.concatenate([animals, items], axis=0)\n",
    "\n",
    "# Create labels: 1 for 'animals', 0 for 'items'\n",
    "y = np.concatenate([np.ones(len(animals)), np.zeros(len(items))], axis=0)\n",
    "\n",
    "# Reshape labels to column vectors (N, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Split data into train_val and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, random_state=SEED, test_size=30, stratify=y\n",
    ")\n",
    "\n",
    "# Further split train_val into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, random_state=SEED, test_size=len(X_test), stratify=y_train_val\n",
    ")\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Training Label Shape: {y_train.shape}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape}\")\n",
    "print(f\"Validation Label Shape: {y_val.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}\")\n",
    "print(f\"Test Label Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LucihcJkJa-N"
   },
   "outputs": [],
   "source": [
    "# Define the input shape based on the training data\n",
    "input_shape = (X_train.shape[3], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxi7njBsJcZw"
   },
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch datasets (without augmentation for now)\n",
    "train_ds = TensorDataset(\n",
    "    torch.from_numpy(X_train).permute(0, 3, 1, 2),\n",
    "    torch.from_numpy(y_train).squeeze().long()\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    torch.from_numpy(X_val).permute(0, 3, 1, 2),\n",
    "    torch.from_numpy(y_val).squeeze().long()\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.from_numpy(X_test).permute(0, 3, 1, 2),\n",
    "    torch.from_numpy(y_test).squeeze().long()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVpmaMPaJdwA"
   },
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBsGCy6fJfWr"
   },
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader with optimized settings.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): PyTorch Dataset object\n",
    "        batch_size (int): Number of samples per batch\n",
    "        shuffle (bool): Whether to shuffle data at each epoch\n",
    "        drop_last (bool): Whether to drop last incomplete batch\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: Configured DataLoader instance\n",
    "    \"\"\"\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI50l9mRJg6B"
   },
   "outputs": [],
   "source": [
    "# Create data loaders with different settings for each phase\n",
    "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKQy2glSJh4Y"
   },
   "outputs": [],
   "source": [
    "# Get one batch from the training data loader\n",
    "for xb, yb in train_loader:\n",
    "    print(\"Features batch shape:\", xb.shape)\n",
    "    print(\"Labels batch shape:\", yb.shape)\n",
    "    break  # Stop after getting one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaLLXlXPJlZ3"
   },
   "source": [
    "## üßÆ **Network Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJhjhy_6Jl38"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 100\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.2         # Dropout probability\n",
    "L1_LAMBDA = 0              # L1 penalty\n",
    "L2_LAMBDA = 0              # L2 penalty\n",
    "\n",
    "# Set up loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", EPOCHS)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Learning Rate:\", LEARNING_RATE)\n",
    "print(\"Dropout Rate:\", DROPOUT_RATE)\n",
    "print(\"L1 Penalty:\", L1_LAMBDA)\n",
    "print(\"L2 Penalty:\", L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2mJVzycJpih"
   },
   "source": [
    "## üõ†Ô∏è **Build the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xRCZEDGJvCO"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Convolutional Neural Network for binary classification.\n",
    "\n",
    "    Architecture:\n",
    "    - 4 convolutional blocks with increasing channels (16->32->64->128)\n",
    "    - ReLU activations and MaxPooling for spatial reduction\n",
    "    - Dropout and fully connected layer for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=(3, 224, 224), num_classes=2, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # First convolutional block: 16 filters\n",
    "        self.conv0 = nn.Conv2d(input_shape[0], 16, kernel_size=3, padding='same')\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.mp0 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional block: 32 filters\n",
    "        self.conv1 = nn.Conv2d(16, 32, kernel_size=3, padding='same')\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolutional block: 64 filters\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fourth convolutional block: 128 filters\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Calculate flattened size after all blocks using a dummy forward pass\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            dummy_output = self._forward_features(dummy_input)\n",
    "            flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(flattened_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"Forward pass through convolutional layers only.\"\"\"\n",
    "        x = self.mp0(self.relu0(self.conv0(x)))\n",
    "        x = self.mp1(self.relu1(self.conv1(x)))\n",
    "        x = self.mp2(self.relu2(self.conv2(x)))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the entire network.\"\"\"\n",
    "        x = self._forward_features(x)\n",
    "        x = self.classifier_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tf6By8uJxoy"
   },
   "outputs": [],
   "source": [
    "# Instantiate CNN model and move to computing device (CPU/GPU)\n",
    "vanilla_model = SimpleCNN(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ").to(device)\n",
    "\n",
    "# Display model architecture summary\n",
    "summary(vanilla_model, input_size=input_shape)\n",
    "model_graph = draw_graph(vanilla_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy4hmKNQKqQe"
   },
   "outputs": [],
   "source": [
    "# Set up TensorBoard logging and save model architecture\n",
    "experiment_name = \"vanilla_cnn\"\n",
    "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
    "x = torch.randn(1, input_shape[0], input_shape[1], input_shape[2]).to(device)\n",
    "writer.add_graph(vanilla_model, x)\n",
    "\n",
    "# Define optimizer with L2 regularization\n",
    "optimizer = torch.optim.Adam(vanilla_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Enable mixed precision training for GPU acceleration\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXhX1gtiKuoT"
   },
   "source": [
    "## üß† **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_ZLl3P5Lkre"
   },
   "outputs": [],
   "source": [
    "# Initialize best model tracking variables\n",
    "best_model = None\n",
    "best_performance = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK2hHxtRLlsT"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VrjLQ-sLmwL"
   },
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform one complete validation epoch through the entire validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        criterion (nn.Module): Loss function used to calculate validation loss\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n",
    "\n",
    "    Note:\n",
    "        This function automatically sets the model to evaluation mode and disables\n",
    "        gradient computation for efficiency during validation.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision (if CUDA available)\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cB_2JRS2Lnuh"
   },
   "outputs": [],
   "source": [
    "def log_metrics_to_tensorboard(writer, epoch, train_loss, train_f1, val_loss, val_f1, model):\n",
    "    \"\"\"\n",
    "    Log training metrics and model parameters to TensorBoard for visualization.\n",
    "\n",
    "    Args:\n",
    "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
    "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
    "        train_loss (float): Training loss for this epoch\n",
    "        train_f1 (float): Training f1 score for this epoch\n",
    "        val_loss (float): Validation loss for this epoch\n",
    "        val_f1 (float): Validation f1 score for this epoch\n",
    "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
    "\n",
    "    Note:\n",
    "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
    "        parameters and gradients, which helps monitor training progress and detect\n",
    "        issues like vanishing/exploding gradients.\n",
    "    \"\"\"\n",
    "    # Log scalar metrics\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "    writer.add_scalar('F1/Validation', val_f1, epoch)\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Check if the tensor is not empty before adding a histogram\n",
    "            if param.numel() > 0:\n",
    "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
    "            if param.grad is not None:\n",
    "                # Check if the gradient tensor is not empty before adding a histogram\n",
    "                if param.grad.numel() > 0:\n",
    "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
    "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XDEpZyFLo1J"
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history) - Trained model and metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_f1': [], 'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Evaluate model on validation data without updating weights\n",
    "        val_loss, val_f1 = validate_one_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['val_loss'].append(val_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "        training_history['val_f1'].append(val_f1)\n",
    "\n",
    "        # Write metrics to TensorBoard for visualization\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, val_loss, val_f1, model\n",
    "            )\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, F1 Score={val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX-ENoAqLqwW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model and track training history\n",
    "vanilla_model, vanilla_history = fit(\n",
    "    model=vanilla_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=1,\n",
    "    experiment_name=\"vanilla_cnn\",\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "# Update best model if current performance is superior\n",
    "if vanilla_history['val_f1'][-1] > best_performance:\n",
    "    best_model = vanilla_model\n",
    "    best_performance = vanilla_history['val_f1'][-1]\n",
    "\n",
    "# Calculate and print the final validation F1 score\n",
    "final_val_f1 = round(max(vanilla_history['val_f1']) * 100, 2)\n",
    "print(f'Final validation F1 score: {final_val_f1}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cr-50R2HRs_O"
   },
   "outputs": [],
   "source": [
    "# @title Plot History\n",
    "# Create a figure with two side-by-side subplots\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(vanilla_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(vanilla_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Categorical Crossentropy')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation F1 score on the second axis\n",
    "ax2.plot(vanilla_history['train_f1'], label='Training F1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(vanilla_history['val_f1'], label='Validation F1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W72szA9lRv6P"
   },
   "source": [
    "## üí™ **Image Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66JOp5s1RyRV"
   },
   "outputs": [],
   "source": [
    "# Define the URL for the image\n",
    "url = \"https://static.wikia.nocookie.net/jujutsu-kaisen/images/6/66/Gojo_reveals_his_Six_Eyes_%28Anime%29.png/revision/latest?cb=20201114064149\"\n",
    "\n",
    "# Send a GET request to the URL and retrieve the image content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Load the image and normalise pixel values\n",
    "img = np.array(Image.open(BytesIO(response.content))) / 255\n",
    "\n",
    "# Make the image squared by cropping equally from left and right\n",
    "dim = min(img.shape[:-1])\n",
    "img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2,\n",
    "          (img.shape[1]-dim)//2:(img.shape[1]+dim)//2, :]\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrfToA_0SPs_"
   },
   "source": [
    "### **Geometric - Random Flip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh3_JnfzSSJj"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random horizontal and vertical flipping\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_NwUY4MVtOQ"
   },
   "source": [
    "### **Geometric - Random Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPgqFOnjVvsD"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random translation\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=None),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Jzzkj-V5La"
   },
   "source": [
    "### **Geometric - Random Rotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD5uJQNcYY9C"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random rotation\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=72, translate=None, scale=None),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCJk1sN5acF9"
   },
   "source": [
    "### **Geometric - Random Zoom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UBaYCZDadf8"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random zoom\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=None, scale=(0.8, 1.2)),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvPw6vY-nqxv"
   },
   "source": [
    " **Geometric - All Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILi7rlAbnrEj"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random zoom\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=72, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiSuQr31n94q"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random zoom\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=72, translate=(0.2, 0.2), scale=(0.8, 1.2), fill=128),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4MPx4e8l8Wd"
   },
   "source": [
    "### **Photometric - Random Brightness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6c6TnngmjfV"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random brightness\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dFnHHOzm0Sh"
   },
   "source": [
    "### **Photometric - Random Contrast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecTf1VI_m0fz"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random contrast\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.ColorJitter(contrast=0.75),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBtBZbzjqOGR"
   },
   "source": [
    "### **Photometric - Random Saturation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-9JONfzqOV9"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random saturation\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.ColorJitter(saturation=0.75),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EVSuBPHp28G"
   },
   "source": [
    " ### **All Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tWJknSVpyZr"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random zoom\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.75, saturation=0.75),\n",
    "    transforms.RandomAffine(degrees=72, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpTaMfuWrwz6"
   },
   "source": [
    "## üí™ **Advanced Image Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZX_dgYar31S"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random erasing (cutout)\n",
    "# RandomErasing randomly selects a rectangle region and erases its pixels\n",
    "# Note: RandomErasing works on tensors, not PIL images\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomErasing(p=1.0, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kE8Ls0fJsOsd"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random erasing (cutout)\n",
    "# RandomErasing randomly selects a rectangle region and erases its pixels\n",
    "# Note: RandomErasing works on tensors, not PIL images\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomErasing(p=1.0, scale=(0.02, 0.33), value='random'),\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12AES3VksU0X"
   },
   "outputs": [],
   "source": [
    "# Define image augmentation with random erasing (cutout)\n",
    "# RandomErasing randomly selects a rectangle region and erases its pixels\n",
    "# Note: RandomErasing works on tensors, not PIL images\n",
    "k_cutout = 5\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomErasing(p=1.0, scale=(0.01, 0.033), value='random')\n",
    "    for _ in range(k_cutout)\n",
    "])\n",
    "\n",
    "# Convert numpy image to PIL for transforms\n",
    "img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "\n",
    "# Set up the figure and grid layout for displaying images\n",
    "fig = plt.figure(constrained_layout=True, figsize=(12, 3))\n",
    "gs = gridspec.GridSpec(1, 4, figure=fig, width_ratios=[1, 1, 1, 1], wspace=0.1)\n",
    "\n",
    "# Display the original image\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Apply augmentation and display the first augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax2.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the second augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax3 = fig.add_subplot(gs[2])\n",
    "ax3.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax3.axis('off')\n",
    "\n",
    "# Apply augmentation again and display the third augmented image\n",
    "augmented_img = np.array(augmentation(img_pil)) / 255\n",
    "ax4 = fig.add_subplot(gs[3])\n",
    "ax4.imshow(np.clip(augmented_img, 0., 1.))\n",
    "ax4.axis('off')\n",
    "\n",
    "# Show the figure with all images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z799SUAeyfLv"
   },
   "source": [
    "## üõ†Ô∏è **Build the Model with Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0lWuVR3yh6J"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class that applies transforms v2 on-the-fly\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that applies data augmentation transforms using transforms v2.\n",
    "\n",
    "    Following the recommended approach from torchvision documentation:\n",
    "    - Use ToImage() to convert PIL to tensor\n",
    "    - Use ToDtype(torch.float32, scale=True) to convert to float and scale to [0, 1]\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input images with shape (N, H, W, C)\n",
    "        labels (np.ndarray): Labels with shape (N,)\n",
    "        transform (callable, optional): Transform to apply to images\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Base transform: convert to tensor (following v2 guidelines)\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and label\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert numpy to PIL Image\n",
    "        image_pil = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "        # Convert to tensor using v2 recommended approach\n",
    "        image_tensor = self.to_tensor(image_pil)\n",
    "\n",
    "        # Apply additional transforms if provided\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "\n",
    "        return image_tensor, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_z9AY2XyiT9"
   },
   "outputs": [],
   "source": [
    "# Define data augmentation pipeline for training using transforms v2\n",
    "train_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomErasing(p=1.0, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0)\n",
    "])\n",
    "\n",
    "# Create augmented datasets\n",
    "train_aug_ds = AugmentedDataset(X_train, y_train.squeeze(), transform=train_augmentation)\n",
    "val_aug_ds = AugmentedDataset(X_val, y_val.squeeze(), transform=None)\n",
    "test_aug_ds = AugmentedDataset(X_test, y_test.squeeze(), transform=None)\n",
    "\n",
    "# Create data loaders for augmented datasets\n",
    "train_aug_loader = make_loader(train_aug_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_aug_loader = make_loader(val_aug_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_aug_loader = make_loader(test_aug_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt8sZs7Lzk8Q"
   },
   "outputs": [],
   "source": [
    "# Instantiate CNN model with augmentation and move to computing device (CPU/GPU)\n",
    "aug_model = SimpleCNN(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ").to(device)\n",
    "\n",
    "# Display model architecture summary\n",
    "summary(aug_model, input_size=input_shape)\n",
    "model_graph = draw_graph(aug_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQg0sM50zmI7"
   },
   "outputs": [],
   "source": [
    "# Set up TensorBoard logging and save model architecture\n",
    "experiment_name = \"augmented_cnn\"\n",
    "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
    "x = torch.randn(1, input_shape[0], input_shape[1], input_shape[2]).to(device)\n",
    "writer.add_graph(aug_model, x)\n",
    "\n",
    "# Define optimizer with L2 regularization\n",
    "optimizer = torch.optim.Adam(aug_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Enable mixed precision training for GPU acceleration\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lRPVFS6zqMv"
   },
   "source": [
    "### üß† **Train the Model with Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90rbxlWvzo23"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train model with augmentation and track training history\n",
    "aug_model, aug_history = fit(\n",
    "    model=aug_model,\n",
    "    train_loader=train_aug_loader,\n",
    "    val_loader=val_aug_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=1,\n",
    "    experiment_name=\"augmented_cnn\",\n",
    "    patience=PATIENCE\n",
    ")\n",
    "\n",
    "# Update best model if current performance is superior\n",
    "if aug_history['val_f1'][-1] > best_performance:\n",
    "    best_model = aug_model\n",
    "    best_performance = aug_history['val_f1'][-1]\n",
    "\n",
    "# Calculate and print the final validation F1 score\n",
    "final_val_f1 = round(max(aug_history['val_f1']) * 100, 2)\n",
    "print(f'Final validation F1 score: {final_val_f1}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Iehq19wRzui3"
   },
   "outputs": [],
   "source": [
    "# @title Plot History\n",
    "# Create a figure with two side-by-side subplots\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(aug_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax1.plot(aug_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n",
    "ax1.set_title('Categorical Crossentropy')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation F1 score on the second axis\n",
    "ax2.plot(aug_history['train_f1'], label='Training F1', alpha=0.3, color='#ff7f0e', linestyle='--')\n",
    "ax2.plot(aug_history['val_f1'], label='Validation F1', alpha=0.9, color='#ff7f0e')\n",
    "ax2.set_title('F1 Score')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjE0NC0ZGmzR"
   },
   "outputs": [],
   "source": [
    "# Copy TensorBoard logs to accessible location for Colab\n",
    "!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n",
    "\n",
    "# Launch TensorBoard interface\n",
    "%tensorboard --logdir \"/content/\"$logs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7Jj3UigG3rX"
   },
   "source": [
    "## üïπÔ∏è **Use the Model - Make Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOKgxQDgG4kp"
   },
   "outputs": [],
   "source": [
    "# Collect predictions and ground truth labels using augmented test loader\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for xb, yb in test_aug_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # Forward pass: get model predictions\n",
    "        logits = best_model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Store batch results\n",
    "        test_preds.append(preds)\n",
    "        test_targets.append(yb.numpy())\n",
    "\n",
    "# Combine all batches into single arrays\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test_targets = np.concatenate(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-tTgrNqG5wc"
   },
   "outputs": [],
   "source": [
    "# Calculate overall test metrics\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_prec = precision_score(test_targets, test_preds, average='weighted')\n",
    "test_rec = recall_score(test_targets, test_preds, average='weighted')\n",
    "test_f1 = f1_score(test_targets, test_preds, average='weighted')\n",
    "print(f\"Accuracy over the test set: {test_acc:.4f}\")\n",
    "print(f\"Precision over the test set: {test_prec:.4f}\")\n",
    "print(f\"Recall over the test set: {test_rec:.4f}\")\n",
    "print(f\"F1 score over the test set: {test_f1:.4f}\")\n",
    "\n",
    "# Generate confusion matrix for detailed error analysis\n",
    "cm = confusion_matrix(test_targets, test_preds)\n",
    "\n",
    "# Create numeric labels for heatmap annotation\n",
    "labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
    "\n",
    "# Visualise confusion matrix\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=labels, fmt='',\n",
    "            xticklabels=['Item','Animal'],\n",
    "            yticklabels=['Item','Animal'],\n",
    "            cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix ‚Äì Test Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3JAQeQEZFJA"
   },
   "source": [
    "## **üîÑ Test Time Augmentation (TTA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt3k5ggNZSo2"
   },
   "outputs": [],
   "source": [
    "# First, let's establish baseline performance without TTA\n",
    "print(\"Computing baseline test performance (no augmentation)...\")\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "test_preds_baseline = []\n",
    "test_targets_baseline = []\n",
    "test_probs_baseline = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_aug_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # Forward pass: get model predictions\n",
    "        logits = best_model(xb)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        # Store results\n",
    "        test_probs_baseline.append(probs.cpu().numpy())\n",
    "        test_preds_baseline.append(preds.cpu().numpy())\n",
    "        test_targets_baseline.append(yb.numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "test_probs_baseline = np.concatenate(test_probs_baseline)\n",
    "test_preds_baseline = np.concatenate(test_preds_baseline)\n",
    "test_targets_baseline = np.concatenate(test_targets_baseline)\n",
    "\n",
    "# Calculate baseline metrics\n",
    "baseline_acc = accuracy_score(test_targets_baseline, test_preds_baseline)\n",
    "baseline_f1 = f1_score(test_targets_baseline, test_preds_baseline, average='weighted')\n",
    "\n",
    "print(f\"\\nBaseline Performance (No TTA):\")\n",
    "print(f\"  Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"  F1 Score: {baseline_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL3Ik3XXZKKW"
   },
   "outputs": [],
   "source": [
    "# Define deterministic horizontal flip transformation\n",
    "horizontal_flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=1.0)  # Always flip (p=1.0)\n",
    "])\n",
    "\n",
    "print(\"\\nApplying Test Time Augmentation with horizontal flip...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fEho-g0ZLxG"
   },
   "outputs": [],
   "source": [
    "# Get predictions for original images (already computed above)\n",
    "print(\"Step 1/3: Using original predictions...\")\n",
    "probs_original = test_probs_baseline\n",
    "\n",
    "# Get predictions for horizontally flipped images\n",
    "print(\"Step 2/3: Computing predictions with horizontal flip...\")\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "test_probs_flipped = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_aug_loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # Apply horizontal flip\n",
    "        xb_flipped = horizontal_flip(xb)\n",
    "\n",
    "        # Forward pass with flipped images\n",
    "        logits_flipped = best_model(xb_flipped)\n",
    "        probs_flipped = torch.softmax(logits_flipped, dim=1)\n",
    "\n",
    "        # Store results\n",
    "        test_probs_flipped.append(probs_flipped.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "test_probs_flipped = np.concatenate(test_probs_flipped)\n",
    "\n",
    "print(f\"Predictions computed for {len(test_probs_flipped)} flipped test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NriFbiCWZ244"
   },
   "outputs": [],
   "source": [
    "# Average the soft labels (probabilities) from original and flipped predictions\n",
    "print(\"Step 3/3: Averaging soft labels from both predictions...\")\n",
    "\n",
    "probs_tta = (probs_original + test_probs_flipped) / 2.0\n",
    "\n",
    "# Get final predictions from averaged probabilities\n",
    "preds_tta = np.argmax(probs_tta, axis=1)\n",
    "\n",
    "print(f\"\\nTTA ensemble complete:\")\n",
    "print(f\"  Original predictions shape: {probs_original.shape}\")\n",
    "print(f\"  Flipped predictions shape:  {test_probs_flipped.shape}\")\n",
    "print(f\"  Averaged predictions shape: {probs_tta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vyfPIs9Z42J"
   },
   "outputs": [],
   "source": [
    "# Calculate TTA metrics\n",
    "tta_acc = accuracy_score(test_targets_baseline, preds_tta)\n",
    "tta_prec = precision_score(test_targets_baseline, preds_tta, average='weighted')\n",
    "tta_rec = recall_score(test_targets_baseline, preds_tta, average='weighted')\n",
    "tta_f1 = f1_score(test_targets_baseline, preds_tta, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST TIME AUGMENTATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TTA Accuracy:  {tta_acc:.4f}\")\n",
    "print(f\"TTA Precision: {tta_prec:.4f}\")\n",
    "print(f\"TTA Recall:    {tta_rec:.4f}\")\n",
    "print(f\"TTA F1 Score:  {tta_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyZ11G6mZ8rO"
   },
   "source": [
    "### üìä **Performance Comparison: Baseline vs TTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCg1T2hvZ_BY"
   },
   "outputs": [],
   "source": [
    "# Compare baseline and TTA performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score'],\n",
    "    'Baseline': [f'{baseline_acc:.4f}', f'{baseline_f1:.4f}'],\n",
    "    'TTA (Horizontal Flip)': [f'{tta_acc:.4f}', f'{tta_f1:.4f}'],\n",
    "    'Improvement': [\n",
    "        f'{(tta_acc - baseline_acc):.4f} ({(tta_acc - baseline_acc)/baseline_acc*100:+.2f}%)',\n",
    "        f'{(tta_f1 - baseline_f1):.4f} ({(tta_f1 - baseline_f1)/baseline_f1*100:+.2f}%)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON: BASELINE VS TTA\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjX7Tls-aBHY"
   },
   "outputs": [],
   "source": [
    "# Generate confusion matrix for TTA predictions\n",
    "cm_tta = confusion_matrix(test_targets_baseline, preds_tta)\n",
    "\n",
    "# Visualise TTA confusion matrix\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm_tta, annot=True, fmt='d',\n",
    "            xticklabels=['Item', 'Animal'],\n",
    "            yticklabels=['Item', 'Animal'],\n",
    "            cmap='Greens')\n",
    "plt.xlabel('Predicted labels', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True labels', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix - Test Set with TTA', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWBfT8qHG7uZ"
   },
   "source": [
    "## ü™Ñ **Image Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26kTmttfG8gf"
   },
   "outputs": [],
   "source": [
    "# Create an embedding model using the existing _forward_features method\n",
    "class EmbeddingModel(nn.Module):\n",
    "    \"\"\"Wrapper that extracts flattened features using the model's feature extractor.\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        # Create a deep copy to avoid modifying the original best_model\n",
    "        self.base_model = copy.deepcopy(base_model)\n",
    "        # Truncate the classifier_head to only include the Flatten layer\n",
    "        self.base_model.classifier_head = nn.Sequential(self.base_model.classifier_head[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The _forward_features method already handles the convolutional layers\n",
    "        x = self.base_model._forward_features(x)\n",
    "        # The truncated classifier_head now only contains the Flatten layer\n",
    "        x = self.base_model.classifier_head(x)\n",
    "        return x\n",
    "\n",
    "# Create embedding model and move to device\n",
    "embedding = EmbeddingModel(best_model).to(device)\n",
    "embedding.eval()\n",
    "\n",
    "# Display the embedding model architecture\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5n1oG4jG-WD"
   },
   "outputs": [],
   "source": [
    "# Define the conversion transform following v2 guidelines\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "# Extract and preprocess a single image for feature extraction\n",
    "index = 10\n",
    "\n",
    "# Prepare the query image\n",
    "image_np = X[index]\n",
    "image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
    "image_tensor = to_tensor(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "# Predict the features of the selected image using the embedding model\n",
    "with torch.no_grad():\n",
    "    image_features = embedding(image_tensor).cpu().numpy()\n",
    "\n",
    "# Display the selected image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image_np)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Extract features from the entire dataset using the embedding model\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X), 32):  # Process in batches\n",
    "        batch = X[i:i+32]\n",
    "        batch_tensors = []\n",
    "        for img in batch:\n",
    "            img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "            img_tensor = to_tensor(img_pil)\n",
    "            batch_tensors.append(img_tensor)\n",
    "        batch_tensor = torch.stack(batch_tensors).to(device)\n",
    "        features = embedding(batch_tensor).cpu().numpy()\n",
    "        all_features.append(features)\n",
    "\n",
    "dataset_features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "# Compute the distances between the selected image's features and the entire dataset's features\n",
    "distances = np.mean(np.square(dataset_features - image_features), axis=-1)\n",
    "\n",
    "# Sort images by their distances (similarity to the selected image)\n",
    "ordered_indices = distances.argsort()\n",
    "ordered_images = X[ordered_indices]\n",
    "\n",
    "# Display the top 10 most similar images\n",
    "num_img = 10\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
    "for i in range(num_img):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(ordered_images[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdZ7EllNJn86"
   },
   "source": [
    "### **Use Pretrained Models as Image Search Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXIgA_GmJhA6"
   },
   "outputs": [],
   "source": [
    "# Load the EfficientNetB0 model pre-trained on ImageNet, without the top classification layer\n",
    "pretrained_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "pretrained_model = torchvision.models.efficientnet_b0(weights=pretrained_weights)\n",
    "\n",
    "# Remove the classifier head to use as feature extractor\n",
    "pretrained_model.classifier = nn.Identity()\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Display pretrained model summary\n",
    "summary(pretrained_model, input_size=input_shape)\n",
    "model_graph = draw_graph(pretrained_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=6)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IoGd_WDMksa"
   },
   "outputs": [],
   "source": [
    "# Get the default preprocessing transforms for EfficientNetB0\n",
    "efficientnet_transform = pretrained_weights.transforms()\n",
    "print(efficientnet_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd1QpOzmJpbS"
   },
   "outputs": [],
   "source": [
    "# Extract and preprocess a single image for feature extraction using EfficientNetB0\n",
    "index = 10\n",
    "\n",
    "# Prepare the query image\n",
    "image_np = X[index]\n",
    "image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
    "# Apply the EfficientNet preprocessing transforms and add a batch dimension\n",
    "image_tensor = efficientnet_transform(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "# Predict the features of the selected image using the pre-trained EfficientNetB0 model\n",
    "with torch.no_grad():\n",
    "    image_features = pretrained_model(image_tensor).cpu().numpy()\n",
    "\n",
    "# Display the selected query image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image_np)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Extract features from all images in the dataset using the pre-trained EfficientNetB0 model\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    # Process images in batches to manage memory and potentially speed up computation\n",
    "    for i in range(0, len(X), 32):\n",
    "        batch = X[i:i+32]\n",
    "        batch_tensors = []\n",
    "        for img in batch:\n",
    "            img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "            # Apply the EfficientNet preprocessing transforms to each image\n",
    "            img_tensor = efficientnet_transform(img_pil)\n",
    "            batch_tensors.append(img_tensor)\n",
    "        # Stack the processed image tensors into a single batch tensor\n",
    "        batch_tensor = torch.stack(batch_tensors).to(device)\n",
    "        # Get features for the current batch\n",
    "        features = pretrained_model(batch_tensor).cpu().numpy()\n",
    "        all_features.append(features)\n",
    "\n",
    "# Concatenate features from all batches into a single array\n",
    "dataset_features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "# Compute the squared Euclidean distances between the query image's features\n",
    "# and the features of all images in the dataset. This measures similarity.\n",
    "distances = np.mean(np.square(dataset_features - image_features), axis=-1)\n",
    "\n",
    "# Sort the dataset images by their distances (smallest distance means most similar)\n",
    "ordered_indices = distances.argsort()\n",
    "ordered_images = X[ordered_indices]\n",
    "\n",
    "# Display the top 10 most similar images to the query image\n",
    "num_img = 10\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
    "for i in range(num_img):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(ordered_images[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wETzo3KKHAU"
   },
   "source": [
    "#  \n",
    "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
    "\n",
    "##### Connect with us:\n",
    "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n",
    "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n",
    "\n",
    "##### Contributors:\n",
    "- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n",
    "- **Alberto Archetti**: alberto.archetti@polimi.it\n",
    "- **Roberto Basla**: roberto.basla@polimi.it\n",
    "- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n",
    "\n",
    "```\n",
    "   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNnMfmpa0IAEmj0u0ypY3Hz",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
