{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfe375a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfe375a9",
        "outputId": "f23665a8-2610-473c-b22a-cce338a12710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "be49326d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49326d",
        "outputId": "f51763fc-cc7e-400c-903a-a393ca7fc5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchview\n",
            "  Downloading torchview-0.2.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchview) (0.21)\n",
            "Downloading torchview-0.2.7-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.7\n",
            "PyTorch version: 2.9.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "!pip install torchview\n",
        "from torchview import draw_graph\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46b14fe3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46b14fe3",
        "outputId": "ce9e66f9-e228-4bf7-b912-41b3f85f3565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample ignorati (logico): 248\n",
            "Sample rimanenti nel dataset: 1164\n",
            "Number of classes: 4\n",
            "Train samples: 931, Val samples: 233\n",
            "Loading training data...\n",
            "Loading validation data...\n"
          ]
        }
      ],
      "source": [
        "current_dir = \"/gdrive/My Drive/[2025-2026] AN2DL/Challenge 2/dataset\"\n",
        "train_data_dir = f\"{current_dir}/train_data\"\n",
        "test_data_dir = f\"{current_dir}/test_data\"\n",
        "csv_path = f\"{current_dir}/train_labels.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============================================================\n",
        "# IGNORA MANUALMENTE ALCUNI SAMPLE\n",
        "# ============================================================\n",
        "\n",
        "# >>> MODIFICA SOLO QUESTA LISTA <<<\n",
        "# Usa sempre il nome immagine completo: \"img_XXXX.png\"\n",
        "SAMPLES_TO_IGNORE = [\n",
        "    \"img_0102.png\", \"img_0104.png\", \"img_0108.png\", \"img_0109.png\", \"img_0112.png\",\n",
        "    \"img_0130.png\", \"img_0147.png\", \"img_0152.png\", \"img_0153.png\", \"img_0168.png\",\n",
        "    \"img_0173.png\", \"img_0176.png\", \"img_0182.png\", \"img_0189.png\", \"img_0193.png\",\n",
        "    \"img_0203.png\", \"img_0213.png\", \"img_0218.png\", \"img_0223.png\", \"img_0228.png\",\n",
        "    \"img_0232.png\", \"img_0237.png\", \"img_0239.png\", \"img_0249.png\", \"img_0250.png\",\n",
        "    \"img_0256.png\", \"img_0264.png\", \"img_0269.png\", \"img_0270.png\", \"img_0271.png\",\n",
        "    \"img_0276.png\", \"img_0277.png\", \"img_0282.png\", \"img_0290.png\", \"img_0291.png\",\n",
        "    \"img_0304.png\", \"img_0308.png\", \"img_0318.png\", \"img_0322.png\", \"img_0323.png\",\n",
        "    \"img_0328.png\", \"img_0336.png\", \"img_0342.png\", \"img_0348.png\", \"img_0357.png\",\n",
        "    \"img_0365.png\", \"img_0368.png\", \"img_0369.png\", \"img_0370.png\", \"img_0379.png\",\n",
        "    \"img_0384.png\", \"img_0386.png\", \"img_0390.png\", \"img_0391.png\", \"img_0394.png\",\n",
        "    \"img_0404.png\", \"img_0406.png\", \"img_0411.png\", \"img_0413.png\", \"img_0418.png\",\n",
        "    \"img_0422.png\", \"img_0426.png\", \"img_0428.png\", \"img_0430.png\", \"img_0436.png\",\n",
        "    \"img_0438.png\", \"img_0442.png\", \"img_0446.png\", \"img_0447.png\", \"img_0448.png\",\n",
        "    \"img_0451.png\", \"img_0454.png\", \"img_0455.png\", \"img_0456.png\", \"img_0469.png\",\n",
        "    \"img_0471.png\", \"img_0478.png\", \"img_0480.png\", \"img_0481.png\", \"img_0487.png\",\n",
        "    \"img_0489.png\", \"img_0492.png\", \"img_0493.png\", \"img_0495.png\", \"img_0503.png\",\n",
        "    \"img_0505.png\", \"img_0509.png\", \"img_0511.png\", \"img_0512.png\", \"img_0514.png\",\n",
        "    \"img_0516.png\", \"img_0518.png\", \"img_0520.png\", \"img_0521.png\", \"img_0526.png\",\n",
        "    \"img_0527.png\", \"img_0529.png\", \"img_0536.png\", \"img_0554.png\", \"img_0555.png\",\n",
        "    \"img_0559.png\", \"img_0572.png\", \"img_0574.png\", \"img_0586.png\", \"img_0589.png\",\n",
        "    \"img_0592.png\", \"img_0594.png\", \"img_0597.png\", \"img_0600.png\", \"img_0606.png\",\n",
        "    \"img_0608.png\", \"img_0612.png\", \"img_0629.png\", \"img_0631.png\", \"img_0648.png\",\n",
        "    \"img_0650.png\", \"img_0652.png\", \"img_0653.png\", \"img_0655.png\", \"img_0665.png\",\n",
        "    \"img_0673.png\", \"img_0681.png\", \"img_0687.png\", \"img_0703.png\", \"img_0714.png\",\n",
        "    \"img_0731.png\", \"img_0733.png\", \"img_0735.png\", \"img_0748.png\", \"img_0753.png\",\n",
        "    \"img_0755.png\", \"img_0758.png\", \"img_0767.png\", \"img_0771.png\", \"img_0796.png\",\n",
        "    \"img_0800.png\", \"img_0804.png\", \"img_0813.png\", \"img_0817.png\", \"img_0819.png\",\n",
        "    \"img_0822.png\", \"img_0825.png\", \"img_0826.png\", \"img_0829.png\", \"img_0832.png\",\n",
        "    \"img_0866.png\", \"img_0868.png\", \"img_0892.png\", \"img_0893.png\", \"img_0898.png\",\n",
        "    \"img_0903.png\", \"img_0906.png\", \"img_0907.png\", \"img_0910.png\", \"img_0913.png\",\n",
        "    \"img_0918.png\", \"img_0919.png\", \"img_0924.png\", \"img_0930.png\", \"img_0935.png\",\n",
        "    \"img_0936.png\", \"img_0937.png\", \"img_0941.png\", \"img_0944.png\", \"img_0958.png\",\n",
        "    \"img_0960.png\", \"img_0963.png\", \"img_0973.png\", \"img_0982.png\", \"img_0991.png\",\n",
        "    \"img_0992.png\", \"img_0999.png\", \"img_1004.png\", \"img_1005.png\", \"img_1006.png\",\n",
        "    \"img_1009.png\", \"img_1011.png\", \"img_1015.png\", \"img_1017.png\", \"img_1021.png\",\n",
        "    \"img_1023.png\", \"img_1035.png\", \"img_1037.png\", \"img_1039.png\", \"img_1041.png\",\n",
        "    \"img_1046.png\", \"img_1062.png\", \"img_1086.png\", \"img_1108.png\", \"img_1109.png\",\n",
        "    \"img_1114.png\", \"img_1124.png\", \"img_1125.png\", \"img_1133.png\", \"img_1145.png\",\n",
        "    \"img_1146.png\", \"img_1150.png\", \"img_1156.png\", \"img_1177.png\", \"img_1184.png\",\n",
        "    \"img_1186.png\", \"img_1202.png\", \"img_1206.png\", \"img_1212.png\", \"img_1217.png\",\n",
        "    \"img_1218.png\", \"img_1220.png\", \"img_1223.png\", \"img_1229.png\", \"img_1232.png\",\n",
        "    \"img_1236.png\", \"img_1241.png\", \"img_1245.png\", \"img_1258.png\", \"img_1261.png\",\n",
        "    \"img_1263.png\", \"img_1267.png\", \"img_1271.png\", \"img_1274.png\", \"img_1277.png\",\n",
        "    \"img_1280.png\", \"img_1281.png\", \"img_1298.png\", \"img_1300.png\", \"img_1312.png\",\n",
        "    \"img_1317.png\", \"img_1320.png\", \"img_1326.png\", \"img_1327.png\", \"img_1329.png\",\n",
        "    \"img_1332.png\", \"img_1334.png\", \"img_1335.png\", \"img_1338.png\", \"img_1340.png\",\n",
        "    \"img_1360.png\", \"img_1361.png\", \"img_1362.png\", \"img_1367.png\", \"img_1369.png\",\n",
        "    \"img_1375.png\", \"img_1376.png\", \"img_1377.png\", \"img_1381.png\", \"img_1385.png\",\n",
        "    \"img_1388.png\", \"img_1389.png\", \"img_1392.png\",\n",
        "]\n",
        "\n",
        "df = pd.read_csv(csv_path, header=None, names=[\"sample_index\", \"label\"])\n",
        "df = df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "# Numero iniziale di righe\n",
        "n_before = len(df)\n",
        "\n",
        "# Filtro logico del dataframe\n",
        "df = df[~df[\"sample_index\"].isin(SAMPLES_TO_IGNORE)].reset_index(drop=True)\n",
        "\n",
        "# Report\n",
        "n_after = len(df)\n",
        "print(f\"Sample ignorati (logico): {n_before - n_after}\")\n",
        "print(f\"Sample rimanenti nel dataset: {n_after}\")\n",
        "\n",
        "# Check di sicurezza\n",
        "assert not df[\"sample_index\"].isin(SAMPLES_TO_IGNORE).any(), \\\n",
        "    \"Errore: alcuni sample ignorati sono ancora presenti nel dataframe\"\n",
        "\n",
        "\n",
        "# Label mapping\n",
        "class_names = sorted(df[\"label\"].unique())\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "df[\"label_index\"] = df[\"label\"].map(label_to_index)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Train/validation split (stratified)\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"],\n",
        "    random_state=SEED\n",
        ")\n",
        "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n",
        "\n",
        "\n",
        "\n",
        "class MaskedCropDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None, padding=10):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.padding = padding\n",
        "\n",
        "    def _get_bbox_from_mask(self, mask):\n",
        "        ys, xs = np.where(mask > 0)\n",
        "        if len(xs) == 0 or len(ys) == 0:\n",
        "            return None\n",
        "        x1, x2 = xs.min(), xs.max()\n",
        "        y1, y2 = ys.min(), ys.max()\n",
        "        return x1, y1, x2, y2\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_name = row[\"sample_index\"]\n",
        "\n",
        "        img_path  = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask  = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "\n",
        "        bbox = self._get_bbox_from_mask(mask)\n",
        "\n",
        "        if bbox is not None:\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            h, w = image.shape[:2]\n",
        "            x1 = max(0, x1 - self.padding)\n",
        "            y1 = max(0, y1 - self.padding)\n",
        "            x2 = min(w, x2 + self.padding)\n",
        "            y2 = min(h, y2 + self.padding)\n",
        "            image = image[y1:y2, x1:x2]\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = row[\"label_index\"]\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "# Transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = MaskedCropDataset(train_df, train_data_dir, transform=data_transforms)\n",
        "val_dataset   = MaskedCropDataset(val_df, train_data_dir, transform=data_transforms)\n",
        "\n",
        "\n",
        "def dataset_to_numpy(ds):\n",
        "    loader = DataLoader(ds, batch_size=32, shuffle=False)\n",
        "    n = len(ds)\n",
        "    X = torch.zeros((n, 3, 224, 224), dtype=torch.float32)\n",
        "    y = torch.zeros(n, dtype=torch.int64)\n",
        "    idx_offset = 0\n",
        "    for images, labels in loader:\n",
        "        bs = images.shape[0]\n",
        "        X[idx_offset:idx_offset+bs] = images\n",
        "        y[idx_offset:idx_offset+bs] = labels\n",
        "        idx_offset += bs\n",
        "    return X.numpy(), y.numpy()\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "X_train, y_train = dataset_to_numpy(train_dataset)\n",
        "print(\"Loading validation data...\")\n",
        "X_val, y_val = dataset_to_numpy(val_dataset)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "5e056077",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e056077",
        "outputId": "356ecc8f-b9d3-4850-e29a-90d98a91afb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (931, 3, 224, 224), y_train shape: (931,)\n",
            "X_val shape: (233, 3, 224, 224), y_val shape: (233,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "5b22415d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b22415d",
        "outputId": "1593c151-49f2-4685-99be-d82601583ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: (3, 224, 224)\n",
            "Number of Classes: 4\n"
          ]
        }
      ],
      "source": [
        "# Define the input shape and number of classes\n",
        "input_shape = (3, 224, 224)\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\"Input Shape:\", input_shape)\n",
        "print(\"Number of Classes:\", num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "9d51c1a0",
      "metadata": {
        "id": "9d51c1a0"
      },
      "outputs": [],
      "source": [
        "# Define the batch size\n",
        "BATCH_SIZE = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "7a30c53a",
      "metadata": {
        "id": "7a30c53a"
      },
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "f98905a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98905a6",
        "outputId": "e071fc92-7085-4fba-c8ae-825cff097845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 200\n",
            "Batch Size: 32\n",
            "Learning Rate: 0.0001\n",
            "Dropout Rate: 0.5\n",
            "Patience: 20\n"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "DROPOUT_RATE = 0.5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Epochs:\", EPOCHS)\n",
        "print(\"Batch Size:\", BATCH_SIZE)\n",
        "print(\"Learning Rate:\", LEARNING_RATE)\n",
        "print(\"Dropout Rate:\", DROPOUT_RATE)\n",
        "print(\"Patience:\", PATIENCE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd872b8a",
      "metadata": {
        "id": "fd872b8a"
      },
      "source": [
        "Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "5956abc5",
      "metadata": {
        "id": "5956abc5"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, targets)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_acc = accuracy_score(np.concatenate(all_targets), np.concatenate(all_predictions))\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, targets)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_acc = accuracy_score(np.concatenate(all_targets), np.concatenate(all_predictions))\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        patience=0, evaluation_metric=\"val_acc\", mode='max',\n",
        "        restore_best_weights=True, writer=None, verbose=1, experiment_name=\"\"):\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    best_metric = float('-inf') if mode == 'max' else float('inf')\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
        "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/Training', train_loss, epoch)\n",
        "            writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "            writer.add_scalar('Accuracy/Training', train_acc, epoch)\n",
        "            writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
        "\n",
        "        if verbose > 0 and (epoch % verbose == 0 or epoch == 1):\n",
        "            print(f\"Epoch {epoch:3d}/{epochs} | Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | Val: Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "\n",
        "        current_metric = history[evaluation_metric][-1]\n",
        "        is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
        "        if is_improvement:\n",
        "            best_metric = current_metric\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience and patience > 0:\n",
        "                print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                break\n",
        "\n",
        "    if restore_best_weights and patience > 0:\n",
        "        model.load_state_dict(torch.load(\"models/\"+experiment_name+'_model.pt'))\n",
        "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "    elif patience == 0:\n",
        "        torch.save(model.state_dict(), \"models/\"+experiment_name+'_model.pt')\n",
        "\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d2ad0c",
      "metadata": {
        "id": "29d2ad0c"
      },
      "source": [
        "## Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "3f17a8ac",
      "metadata": {
        "id": "3f17a8ac"
      },
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    \"\"\"ResNet18 pretrained su ImageNet, con head custom per classificazione.\"\"\"\n",
        "    def __init__(self, num_classes, dropout_rate=0.3, freeze_backbone=True):\n",
        "        super().__init__()\n",
        "        self.backbone = torchvision.models.resnet18(\n",
        "            weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
        "        )\n",
        "\n",
        "        # Congelo tutto il backbone se richiesto\n",
        "        if freeze_backbone:\n",
        "            for p in self.backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        # Sostituisco la fully connected finale\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "3fd5b617",
      "metadata": {
        "id": "3fd5b617"
      },
      "outputs": [],
      "source": [
        "## Transfer Learning con ResNet18\n",
        "\n",
        "tl_model = ResNet18(\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    freeze_backbone=True\n",
        ").to(device)\n",
        "\n",
        "train_augmentation = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "train_tl_ds = MaskedCropDataset(train_df, train_data_dir, transform=data_transforms)\n",
        "val_tl_ds   = MaskedCropDataset(val_df,   train_data_dir, transform=data_transforms)\n",
        "\n",
        "train_tl_loader = make_loader(train_tl_ds, BATCH_SIZE, shuffle=True,  drop_last=False)\n",
        "val_tl_loader   = make_loader(val_tl_ds,   BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "experiment_name = \"resnet18_transfer_learning\"\n",
        "writer = SummaryWriter(\"./\"+logs_dir+\"/\"+experiment_name)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    tl_model.parameters(),  # alleniamo SOLO la nuova fc perché il backbone è congelato\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "si9DNlteE_UK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si9DNlteE_UK",
        "outputId": "34f95b8e-37d3-4464-f2ec-d650dce9b339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=1.5135, Acc=0.2975 | Val: Loss=1.4068, Acc=0.3047\n",
            "Epoch   5/200 | Train: Loss=1.4808, Acc=0.3190 | Val: Loss=1.3882, Acc=0.2833\n",
            "Epoch  10/200 | Train: Loss=1.4329, Acc=0.3201 | Val: Loss=1.3840, Acc=0.2876\n",
            "Epoch  15/200 | Train: Loss=1.4136, Acc=0.3340 | Val: Loss=1.3817, Acc=0.2918\n",
            "Epoch  20/200 | Train: Loss=1.4124, Acc=0.3362 | Val: Loss=1.3763, Acc=0.2575\n",
            "Early stopping triggered after 21 epochs.\n",
            "Best model restored from epoch 1 with val_acc 0.3047\n",
            "Final validation accuracy: 30.47%\n",
            "CPU times: user 22.4 s, sys: 9.11 s, total: 31.5 s\n",
            "Wall time: 8min 28s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Train with transfer learning\n",
        "tl_model, tl_history = fit(\n",
        "    model=tl_model,\n",
        "    train_loader=train_tl_loader,\n",
        "    val_loader=val_tl_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=5,\n",
        "    experiment_name=experiment_name,\n",
        "    patience=PATIENCE\n",
        ")\n",
        "\n",
        "final_val_acc = round(max(tl_history['val_acc']) * 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_acc}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f3f7ec",
      "metadata": {
        "id": "66f3f7ec"
      },
      "source": [
        "## Fine-Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "25029e59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25029e59",
        "outputId": "a736d735-586b-43d8-c832-68f8053ff8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 11,178,564\n",
            "Trainable parameters: 11,178,564\n",
            "Frozen parameters: 0\n"
          ]
        }
      ],
      "source": [
        "# Carico il modello fine-tuning\n",
        "ft_model = ResNet18(\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    freeze_backbone=False\n",
        ").to(device)\n",
        "\n",
        "# Carico i pesi del transfer learning\n",
        "ft_model.load_state_dict(\n",
        "    torch.load(\"models/resnet18_transfer_learning_model.pt\")\n",
        ")\n",
        "\n",
        "# 1️⃣ Congelo TUTTO\n",
        "for param in ft_model.backbone.parameters():\n",
        "    param.requires_grad = True                                   ###palese fa cagare\n",
        "\"\"\"\n",
        "2️⃣ Sblocco SOLO layer4\n",
        "for param in ft_model.backbone.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in ft_model.backbone.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\"\"\"\n",
        "# 3️⃣ Sblocco SEMPRE la fully connected finale\n",
        "for param in ft_model.backbone.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Debug: contiamo i parametri\n",
        "total_params = sum(p.numel() for p in ft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "e8eab149",
      "metadata": {
        "id": "e8eab149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baddf708-c82f-4293-c149-0e07bd0c78ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 11,178,564\n",
            "Trainable parameters: 8,395,780\n",
            "Frozen parameters: 2,782,784\n"
          ]
        }
      ],
      "source": [
        "## Fine-Tuning con ResNet18 (sblocco parziale corretto)\n",
        "\n",
        "# 1️⃣ Istanzia il modello\n",
        "ft_model = ResNet18(\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    freeze_backbone=False\n",
        ").to(device)\n",
        "\n",
        "# 2️⃣ Carica i pesi della fase di transfer learning\n",
        "ft_model.load_state_dict(\n",
        "    torch.load(\"models/resnet18_transfer_learning_model.pt\", map_location=device)\n",
        ")\n",
        "\n",
        "# 3️⃣ Congela TUTTO il backbone\n",
        "for param in ft_model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 4️⃣ Sblocca SOLO quello che davvero vuoi fine-tunare\n",
        "# ⚠️ Visto che stai overfittando, layer3 è rischioso\n",
        "# Se vuoi provarlo, fallo DOPO aver stabilizzato layer4\n",
        "for param in ft_model.backbone.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Fully connected SEMPRE sbloccata\n",
        "for param in ft_model.backbone.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 5️⃣ Report parametri\n",
        "total_params = sum(p.numel() for p in ft_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "\n",
        "train_ft_loader = train_tl_loader\n",
        "val_ft_loader   = val_tl_loader\n",
        "\n",
        "# 6️⃣ Optimizer (LR diversi)\n",
        "experiment_name = \"resnet18_fine_tuning\"\n",
        "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + experiment_name)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    [\n",
        "        {\"params\": ft_model.backbone.layer4.parameters(), \"lr\": 1e-5},\n",
        "        {\"params\": ft_model.backbone.fc.parameters(),     \"lr\": 5e-4},\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 7️⃣ Mixed precision (ok)\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "22dec830",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dec830",
        "outputId": "2903dcaf-7041-44ff-d060-b198ab6e841b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 200 epochs...\n",
            "Epoch   1/200 | Train: Loss=1.5009, Acc=0.2911 | Val: Loss=1.3941, Acc=0.2661\n",
            "Epoch   5/200 | Train: Loss=1.2909, Acc=0.3942 | Val: Loss=1.4122, Acc=0.2876\n",
            "Epoch  10/200 | Train: Loss=1.1074, Acc=0.4973 | Val: Loss=1.4502, Acc=0.2618\n",
            "Epoch  15/200 | Train: Loss=0.9094, Acc=0.6434 | Val: Loss=1.5051, Acc=0.3004\n",
            "Epoch  20/200 | Train: Loss=0.7037, Acc=0.7411 | Val: Loss=1.6395, Acc=0.3090\n",
            "Early stopping triggered after 24 epochs.\n",
            "Best model restored from epoch 4 with val_acc 0.3219\n",
            "Final validation accuracy: 32.19%\n",
            "CPU times: user 31.1 s, sys: 10.6 s, total: 41.8 s\n",
            "Wall time: 9min 45s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "ft_model, ft_history = fit(\n",
        "    model=ft_model,\n",
        "    train_loader=train_ft_loader,\n",
        "    val_loader=val_ft_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    writer=writer,\n",
        "    verbose=5,\n",
        "    experiment_name=experiment_name,\n",
        "    patience=PATIENCE\n",
        ")\n",
        "\n",
        "final_val_acc = round(max(ft_history['val_acc']) * 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_acc}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365f9814",
      "metadata": {
        "id": "365f9814"
      },
      "source": [
        "Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "6dae20b4",
      "metadata": {
        "id": "6dae20b4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate_macro_f1(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "            y_pred.extend(preds.cpu().tolist())\n",
        "    return f1_score(y_true, y_pred, average=\"macro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "5c4fc853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4fc853",
        "outputId": "91fb6830-b9d6-446f-8290-61e9639619cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18(\n",
              "  (backbone): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=512, out_features=4, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "best_model = ResNet18(num_classes, DROPOUT_RATE, freeze_backbone=False).to(device)\n",
        "best_model.load_state_dict(torch.load(\"models/resnet18_fine_tuning_model.pt\"))\n",
        "best_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "c9964ad6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9964ad6",
        "outputId": "1c3958b9-3ab3-4768-ba3b-5a31fe8c52db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 TRAIN (macro): 0.3004\n",
            "F1 VAL   (macro): 0.1755\n"
          ]
        }
      ],
      "source": [
        "train_f1 = evaluate_macro_f1(best_model, train_ft_loader, device)\n",
        "val_f1   = evaluate_macro_f1(best_model, val_ft_loader, device)\n",
        "print(f\"F1 TRAIN (macro): {train_f1:.4f}\")\n",
        "print(f\"F1 VAL   (macro): {val_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f7d2d7",
      "metadata": {
        "id": "a3f7d2d7"
      },
      "source": [
        "Inference on test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "83d202eb",
      "metadata": {
        "id": "83d202eb"
      },
      "outputs": [],
      "source": [
        "class InferenceImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        files = sorted(os.listdir(img_dir))\n",
        "        seen = set()\n",
        "        self.files = []\n",
        "        for f in files:\n",
        "            base = os.path.splitext(f)[0]\n",
        "            if base not in seen:\n",
        "                seen.add(base)\n",
        "                self.files.append(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.files[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.files[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "6429a3aa",
      "metadata": {
        "id": "6429a3aa"
      },
      "outputs": [],
      "source": [
        "class MaskedInferenceDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None, padding=10):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.padding = padding\n",
        "\n",
        "        # Prendo solo le immagini img_xxxx.png\n",
        "        self.images = sorted([\n",
        "            f for f in os.listdir(img_dir)\n",
        "            if f.startswith(\"img_\")\n",
        "        ])\n",
        "\n",
        "    def _get_bbox_from_mask(self, mask):\n",
        "        ys, xs = np.where(mask > 0)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "        return xs.min(), ys.min(), xs.max(), ys.max()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        mask_name = img_name.replace(\"img_\", \"mask_\")\n",
        "\n",
        "        img_path  = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, mask_name)\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask  = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "\n",
        "        bbox = self._get_bbox_from_mask(mask)\n",
        "        if bbox is not None:\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            h, w = image.shape[:2]\n",
        "            image = image[\n",
        "                max(0, y1 - self.padding):min(h, y2 + self.padding),\n",
        "                max(0, x1 - self.padding):min(w, x2 + self.padding)\n",
        "            ]\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "5780d454",
      "metadata": {
        "id": "5780d454"
      },
      "outputs": [],
      "source": [
        "test_dataset = MaskedInferenceDataset(\n",
        "    img_dir=test_data_dir,\n",
        "    transform=data_transforms\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "\n",
        "best_model = ResNet18(\n",
        "    num_classes=num_classes,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    freeze_backbone=False\n",
        ").to(device)\n",
        "\n",
        "best_model.load_state_dict(\n",
        "    torch.load(\"models/resnet18_fine_tuning_model.pt\")\n",
        ")\n",
        "\n",
        "best_model.eval()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = best_model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        predictions.extend(preds.cpu().tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "652168da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "652168da",
        "outputId": "b4e98e12-f0a2-4483-fa94-64e07721173d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "954 righe ? OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sample_index      label\n",
              "0  img_0000.png  Luminal B\n",
              "1  img_0001.png  Luminal B\n",
              "2  img_0002.png  Luminal B\n",
              "3  img_0003.png  Luminal B\n",
              "4  img_0004.png  Luminal B"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30011fe3-9d40-4c07-b5d5-01ede09305b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img_0000.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img_0001.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img_0002.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>img_0003.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>img_0004.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30011fe3-9d40-4c07-b5d5-01ede09305b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-30011fe3-9d40-4c07-b5d5-01ede09305b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-30011fe3-9d40-4c07-b5d5-01ede09305b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-51bf9122-877e-4cc6-b5b7-48822da25908\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51bf9122-877e-4cc6-b5b7-48822da25908')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-51bf9122-877e-4cc6-b5b7-48822da25908 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "submission_df",
              "summary": "{\n  \"name\": \"submission_df\",\n  \"rows\": 954,\n  \"fields\": [\n    {\n      \"column\": \"sample_index\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 954,\n        \"samples\": [\n          \"img_0199.png\",\n          \"img_0422.png\",\n          \"img_0695.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Luminal B\",\n          \"Luminal A\",\n          \"HER2(+)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    \"sample_index\": test_dataset.images,\n",
        "    \"label\": [class_names[p] for p in predictions]\n",
        "})\n",
        "\n",
        "os.makedirs(os.path.join(current_dir, \"submission\"), exist_ok=True)\n",
        "submission_df.to_csv(\n",
        "    os.path.join(current_dir, \"submission\", \"fixed_input_shape.csv\"),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(len(submission_df), \"righe ? OK\")\n",
        "submission_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}