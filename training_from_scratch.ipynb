{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2e021f",
   "metadata": {},
   "source": [
    "# **AN2DL Challenge 2 - Image Classification**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d28923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment\n",
    "isColab = False\n",
    "isKaggle = False\n",
    "isWsl = True\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaefbbd",
   "metadata": {},
   "source": [
    "## **Loading Enviroment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d315fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esecuzione su WSL. Directory corrente impostata a: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
      "Changed directory to: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory di default\n",
    "current_dir = os.getcwd()   \n",
    "\n",
    "if isColab:\n",
    "    from google.colab import drive\n",
    "    \n",
    "    drive.mount(\"/gdrive\")\n",
    "    current_dir = \"/gdrive/My\\\\ Drive/Colab\\\\ Notebooks/[2025-2026]\\\\ AN2DL/AN2DL-challenge-2\"\n",
    "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
    "    %cd $current_dir\n",
    "elif isKaggle:\n",
    "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
    "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
    "    current_dir = kaggle_work_dir\n",
    "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
    "    os.chdir(current_dir)\n",
    "elif isWsl:\n",
    "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
    "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
    "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
    "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
    "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
    "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "print(f\"Changed directory to: {current_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70e1ac",
   "metadata": {},
   "source": [
    "## **Import Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e975c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchview in /home/berto/an2dl-challenge-2/.venv/lib/python3.12/site-packages (0.2.7)\n",
      "Requirement already satisfied: graphviz in /home/berto/an2dl-challenge-2/.venv/lib/python3.12/site-packages (from torchview) (0.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "PyTorch version: 2.9.1+cu130\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "%pip install torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "if isColab or isKaggle:\n",
    "    !pkill -f tensorboard\n",
    "    !mkdir -p models\n",
    "    print(\"Killed existing TensorBoard instances and created models directory.\")\n",
    "else:\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    \n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954322c",
   "metadata": {},
   "source": [
    "## **Dataset Downloading**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad20c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present in local environment. Skipping download.\n",
      "Dataset directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset\n",
      "Train set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_data\n",
      "Test set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/test_data\n",
      "Label file: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
    "\n",
    "if isColab:\n",
    "    # Clean up path for Python usage (remove shell escapes)\n",
    "    clean_current_dir = current_dir.replace('\\\\ ', ' ')\n",
    "    drive_dataset_dir = os.path.join(clean_current_dir, \"dataset\")\n",
    "    local_dataset_dir = \"/content/dataset\"\n",
    "    \n",
    "    if not os.path.exists(local_dataset_dir):\n",
    "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
    "        try:\n",
    "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
    "            print(\"Copy complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying dataset: {e}\")\n",
    "            print(\"Falling back to Drive dataset (slow).\")\n",
    "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
    "            dataset_dir = drive_dataset_dir\n",
    "    else:\n",
    "        print(\"Dataset already copied to local runtime.\")\n",
    "    \n",
    "    # If copy succeeded (or already existed), use local path\n",
    "    if os.path.exists(local_dataset_dir):\n",
    "        dataset_dir = local_dataset_dir\n",
    "\n",
    "elif isKaggle:\n",
    "    # Nothing to do, dataset is already available in Kaggle environment\n",
    "    print(\"Running on Kaggle. Dataset is assumed to be already available.\")\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "else:\n",
    "    # Check if dataset is already downloaded\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "    if not os.listdir(dataset_dir):\n",
    "        print(\"Downloading dataset from Kaggle in local environment...\")\n",
    "        os.chdir(dataset_dir)\n",
    "        !kaggle competitions download -c an2dl2526c2\n",
    "        zip_file = \"an2dl2526c2.zip\"\n",
    "        shutil.unpack_archive(zip_file, extract_dir=\".\")\n",
    "        os.remove(zip_file)\n",
    "        os.chdir(current_dir)\n",
    "    else:\n",
    "        print(\"Dataset already present in local environment. Skipping download.\")\n",
    "\n",
    "# Define absolute paths\n",
    "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
    "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
    "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
    "        \n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Train set directory: {train_set_dir}\")\n",
    "print(f\"Test set directory: {test_set_dir}\")\n",
    "print(f\"Label file: {label_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c7507",
   "metadata": {},
   "source": [
    "## ‚è≥ **Data Loading**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca3c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader parameters\n",
    "APPLY_MASK = False\n",
    "BATCH_SIZE = 32\n",
    "LOADER_SHUFFLE = False\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "LABEL_MAP = {\"Luminal A\": 0, \"Luminal B\": 1, \"HER2(+)\": 2, \"Triple negative\": 3}\n",
    "IMG_RESIZE = (224, 224)\n",
    "input_shape = (3, *IMG_RESIZE)\n",
    "num_classes = len(LABEL_MAP)\n",
    "\n",
    "TEST_SET_SIZE = 0.2\n",
    "VAL_SET_SIZE = 0.2\n",
    "TRAIN_SET_SIZE = 1.0 - TEST_SET_SIZE - VAL_SET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cf0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# No augmentation for validation/test\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMG_RESIZE, antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcb081",
   "metadata": {},
   "source": [
    "### **Definitions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00064880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IronGutsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for the Iron-Guts (Breast Cancer) competition.\n",
    "\n",
    "    Features:\n",
    "    1. Lazy Loading: Reads images from disk on-the-fly to save RAM.\n",
    "    2. Integrity Checks: Ensures every image has a corresponding mask.\n",
    "    3. Mask Gating: Uses the binary mask to suppress background noise (setting it to pure black).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        data,\n",
    "        transform=None,\n",
    "        augmentation=None,\n",
    "        target_transform=None,\n",
    "        apply_mask=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all images and masks.\n",
    "            data (str or pd.DataFrame): Path to the CSV file with annotations or the DataFrame itself.\n",
    "            transform (callable, optional): Transform to be applied on the image (e.g., Resize, ToTensor).\n",
    "            augmentation (callable, optional): Augmentation transforms to be applied on the image.\n",
    "            target_transform (callable, optional): Transform to be applied on the label.\n",
    "            apply_mask (bool, optional): Whether to apply mask gating. Default: True.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.target_transform = target_transform\n",
    "        self.apply_mask = apply_mask\n",
    "\n",
    "        # Load the CSV\n",
    "        if isinstance(data, str):\n",
    "            self.annotations = pd.read_csv(data)\n",
    "            print(f\"Loaded {len(self.annotations)} annotations from {data}\")\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            self.annotations = data\n",
    "            print(f\"Loaded {len(self.annotations)} annotations from DataFrame\")\n",
    "        else:\n",
    "            raise ValueError(\"data must be a file path or a pandas DataFrame\")\n",
    "\n",
    "        # Define class mapping based on the biological subtypes\n",
    "        # ['Triple negative' 'Luminal A' 'Luminal B' 'HER2(+)']\n",
    "        self.label_map = LABEL_MAP\n",
    "\n",
    "        # Validation: Check that dataset is not empty\n",
    "        if len(self.annotations) == 0:\n",
    "            raise RuntimeError(\"Dataset CSV is empty.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # 1. Parse File Paths\n",
    "        # The CSV contains 'sample_index' like 'img_5', we need to append extensions.\n",
    "        img_id = self.annotations.iloc[idx][\"sample_index\"]\n",
    "        img_name = f\"{img_id}\"\n",
    "        mask_name = f\"{img_id.replace('img_', 'mask_')}\"\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        mask_path = os.path.join(self.root_dir, mask_name)\n",
    "\n",
    "        # 2. Load Data (Lazy Operation)\n",
    "        # We convert image to RGB (3 channels) and mask to L (grayscale/binary)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.apply_mask:\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Missing file pair: {img_name} or {mask_name}\")\n",
    "\n",
    "        # 3. Apply Mask Gating (Background Suppression) - only if flag is enabled\n",
    "        if self.apply_mask:\n",
    "            # Tissue is signal, background is noise.\n",
    "            # We multiply the image by the mask to force background to absolute 0.\n",
    "            image_np = np.array(image)\n",
    "            mask_np = np.array(mask)\n",
    "\n",
    "            # Ensure mask is binary (0 or 1) for broadcasting\n",
    "            # Any pixel > 0 in the mask is considered tissue\n",
    "            binary_mask = (mask_np > 0).astype(np.uint8)\n",
    "\n",
    "            # Expand dimensions of mask to match image (H, W, 1) for broadcasting\n",
    "            binary_mask = np.expand_dims(binary_mask, axis=-1)\n",
    "\n",
    "            # Apply gating: Image * Mask\n",
    "            masked_image_np = image_np * binary_mask\n",
    "\n",
    "            # Convert back to PIL for standard PyTorch transforms\n",
    "            masked_image = Image.fromarray(masked_image_np)\n",
    "        else:\n",
    "            masked_image = image\n",
    "\n",
    "        # 4. Apply Transforms (e.g., Resize, ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            masked_image = self.transform(masked_image)\n",
    "\n",
    "        if self.augmentation:\n",
    "            masked_image = self.augmentation(masked_image)\n",
    "\n",
    "        # 5. Handle Labels\n",
    "        label_str = self.annotations.iloc[idx][\"label\"]\n",
    "        label = self.label_map[str(label_str)]\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return masked_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3077a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, cpu_cores)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers if isColab or isKaggle or isWsl else 0,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4 if isColab or isKaggle or isWsl else None,\n",
    "        persistent_workers=(isColab or isKaggle or isWsl) and num_workers > 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4966b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_TO_IGNORE = [\n",
    "    \"img_0102.png\", \"img_0104.png\", \"img_0108.png\", \"img_0109.png\", \"img_0112.png\",\n",
    "    \"img_0130.png\", \"img_0147.png\", \"img_0152.png\", \"img_0153.png\", \"img_0168.png\",\n",
    "    \"img_0173.png\", \"img_0176.png\", \"img_0182.png\", \"img_0189.png\", \"img_0193.png\",\n",
    "    \"img_0203.png\", \"img_0213.png\", \"img_0218.png\", \"img_0223.png\", \"img_0228.png\",\n",
    "    \"img_0232.png\", \"img_0237.png\", \"img_0239.png\", \"img_0249.png\", \"img_0250.png\",\n",
    "    \"img_0256.png\", \"img_0264.png\", \"img_0269.png\", \"img_0270.png\", \"img_0271.png\",\n",
    "    \"img_0276.png\", \"img_0277.png\", \"img_0282.png\", \"img_0290.png\", \"img_0291.png\",\n",
    "    \"img_0304.png\", \"img_0308.png\", \"img_0318.png\", \"img_0322.png\", \"img_0323.png\",\n",
    "    \"img_0328.png\", \"img_0336.png\", \"img_0342.png\", \"img_0348.png\", \"img_0357.png\",\n",
    "    \"img_0365.png\", \"img_0368.png\", \"img_0369.png\", \"img_0370.png\", \"img_0379.png\",\n",
    "    \"img_0384.png\", \"img_0386.png\", \"img_0390.png\", \"img_0391.png\", \"img_0394.png\",\n",
    "    \"img_0404.png\", \"img_0406.png\", \"img_0411.png\", \"img_0413.png\", \"img_0418.png\",\n",
    "    \"img_0422.png\", \"img_0426.png\", \"img_0428.png\", \"img_0430.png\", \"img_0436.png\",\n",
    "    \"img_0438.png\", \"img_0442.png\", \"img_0446.png\", \"img_0447.png\", \"img_0448.png\",\n",
    "    \"img_0451.png\", \"img_0454.png\", \"img_0455.png\", \"img_0456.png\", \"img_0469.png\",\n",
    "    \"img_0471.png\", \"img_0478.png\", \"img_0480.png\", \"img_0481.png\", \"img_0487.png\",\n",
    "    \"img_0489.png\", \"img_0492.png\", \"img_0493.png\", \"img_0495.png\", \"img_0503.png\",\n",
    "    \"img_0505.png\", \"img_0509.png\", \"img_0511.png\", \"img_0512.png\", \"img_0514.png\",\n",
    "    \"img_0516.png\", \"img_0518.png\", \"img_0520.png\", \"img_0521.png\", \"img_0526.png\",\n",
    "    \"img_0527.png\", \"img_0529.png\", \"img_0536.png\", \"img_0554.png\", \"img_0555.png\",\n",
    "    \"img_0559.png\", \"img_0572.png\", \"img_0574.png\", \"img_0586.png\", \"img_0589.png\",\n",
    "    \"img_0592.png\", \"img_0594.png\", \"img_0597.png\", \"img_0600.png\", \"img_0606.png\",\n",
    "    \"img_0608.png\", \"img_0612.png\", \"img_0629.png\", \"img_0631.png\", \"img_0648.png\",\n",
    "    \"img_0650.png\", \"img_0652.png\", \"img_0653.png\", \"img_0655.png\", \"img_0665.png\",\n",
    "    \"img_0673.png\", \"img_0681.png\", \"img_0687.png\", \"img_0703.png\", \"img_0714.png\",\n",
    "    \"img_0731.png\", \"img_0733.png\", \"img_0735.png\", \"img_0748.png\", \"img_0753.png\",\n",
    "    \"img_0755.png\", \"img_0758.png\", \"img_0767.png\", \"img_0771.png\", \"img_0796.png\",\n",
    "    \"img_0800.png\", \"img_0804.png\", \"img_0813.png\", \"img_0817.png\", \"img_0819.png\",\n",
    "    \"img_0822.png\", \"img_0825.png\", \"img_0826.png\", \"img_0829.png\", \"img_0832.png\",\n",
    "    \"img_0866.png\", \"img_0868.png\", \"img_0892.png\", \"img_0893.png\", \"img_0898.png\",\n",
    "    \"img_0903.png\", \"img_0906.png\", \"img_0907.png\", \"img_0910.png\", \"img_0913.png\",\n",
    "    \"img_0918.png\", \"img_0919.png\", \"img_0924.png\", \"img_0930.png\", \"img_0935.png\",\n",
    "    \"img_0936.png\", \"img_0937.png\", \"img_0941.png\", \"img_0944.png\", \"img_0958.png\",\n",
    "    \"img_0960.png\", \"img_0963.png\", \"img_0973.png\", \"img_0982.png\", \"img_0991.png\",\n",
    "    \"img_0992.png\", \"img_0999.png\", \"img_1004.png\", \"img_1005.png\", \"img_1006.png\",\n",
    "    \"img_1009.png\", \"img_1011.png\", \"img_1015.png\", \"img_1017.png\", \"img_1021.png\",\n",
    "    \"img_1023.png\", \"img_1035.png\", \"img_1037.png\", \"img_1039.png\", \"img_1041.png\",\n",
    "    \"img_1046.png\", \"img_1062.png\", \"img_1086.png\", \"img_1108.png\", \"img_1109.png\",\n",
    "    \"img_1114.png\", \"img_1124.png\", \"img_1125.png\", \"img_1133.png\", \"img_1145.png\",\n",
    "    \"img_1146.png\", \"img_1150.png\", \"img_1156.png\", \"img_1177.png\", \"img_1184.png\",\n",
    "    \"img_1186.png\", \"img_1202.png\", \"img_1206.png\", \"img_1212.png\", \"img_1217.png\",\n",
    "    \"img_1218.png\", \"img_1220.png\", \"img_1223.png\", \"img_1229.png\", \"img_1232.png\",\n",
    "    \"img_1236.png\", \"img_1241.png\", \"img_1245.png\", \"img_1258.png\", \"img_1261.png\",\n",
    "    \"img_1263.png\", \"img_1267.png\", \"img_1271.png\", \"img_1274.png\", \"img_1277.png\",\n",
    "    \"img_1280.png\", \"img_1281.png\", \"img_1298.png\", \"img_1300.png\", \"img_1312.png\",\n",
    "    \"img_1317.png\", \"img_1320.png\", \"img_1326.png\", \"img_1327.png\", \"img_1329.png\",\n",
    "    \"img_1332.png\", \"img_1334.png\", \"img_1335.png\", \"img_1338.png\", \"img_1340.png\",\n",
    "    \"img_1360.png\", \"img_1361.png\", \"img_1362.png\", \"img_1367.png\", \"img_1369.png\",\n",
    "    \"img_1375.png\", \"img_1376.png\", \"img_1377.png\", \"img_1381.png\", \"img_1385.png\",\n",
    "    \"img_1388.png\", \"img_1389.png\", \"img_1392.png\", \"img_0846.png\",\"img_1192.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a6de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 697\n",
      "Validation set size: 372\n",
      "Test set size: 93\n",
      "Loaded 697 annotations from DataFrame\n",
      "Loaded 372 annotations from DataFrame\n",
      "Loaded 93 annotations from DataFrame\n",
      "Batch of images shape: torch.Size([32, 3, 224, 224])\n",
      "Batch of labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the full dataframe\n",
    "full_df = pd.read_csv(label_file)\n",
    "\n",
    "# Remove cursed images\n",
    "full_df = full_df[~full_df['sample_index'].isin(SAMPLES_TO_IGNORE)].reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    full_df,\n",
    "    test_size=(TEST_SET_SIZE + VAL_SET_SIZE),\n",
    "    stratify=full_df[\"label\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=VAL_SET_SIZE,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Create Datasets\n",
    "train_set = IronGutsDataset(\n",
    "    root_dir=train_set_dir,\n",
    "    data=train_df,\n",
    "    transform=transform,\n",
    "    augmentation=train_augmentation,\n",
    "    apply_mask=APPLY_MASK,\n",
    ")\n",
    "\n",
    "val_set = IronGutsDataset(\n",
    "    root_dir=train_set_dir,\n",
    "    data=val_df,\n",
    "    transform=transform,\n",
    "    apply_mask=APPLY_MASK,\n",
    ")\n",
    "\n",
    "test_set = IronGutsDataset(\n",
    "    root_dir=train_set_dir,\n",
    "    data=test_df,\n",
    "    transform=transform,\n",
    "    apply_mask=APPLY_MASK,\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = make_loader(\n",
    "    ds=train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = make_loader(\n",
    "    ds=val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = make_loader(\n",
    "    ds=test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "# Check a batch\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc4a5c",
   "metadata": {},
   "source": [
    "## üßÆ **Network Parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "\n",
    "# Regularization\n",
    "DROPOUT_RATE = 0.3\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# Set up loss function\n",
    "# Manage Class imbalance\n",
    "class_counts = full_df[\"label\"].value_counts().sort_index()\n",
    "total_samples = len(full_df)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "# Convert to tensor for PyTorch usage later\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=LABEL_SMOOTHING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa95a8",
   "metadata": {},
   "source": [
    "## üß† **Training Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f349447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(\n",
    "            device_type=device.type, enabled=(device.type == \"cuda\")\n",
    "        ):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = accuracy_score(\n",
    "        np.concatenate(all_targets), np.concatenate(all_predictions)\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(\n",
    "                device_type=device.type, enabled=(device.type == \"cuda\")\n",
    "            ):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = accuracy_score(\n",
    "        np.concatenate(all_targets), np.concatenate(all_predictions)\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d487b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    device,\n",
    "    patience=0,\n",
    "    evaluation_metric=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True,\n",
    "    writer=None,\n",
    "    verbose=1,\n",
    "    experiment_name=\"\",\n",
    "):\n",
    "    \"\"\"Train the neural network model.\"\"\"\n",
    "\n",
    "    training_history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "    }\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float(\"-inf\") if mode == \"max\" else float(\"inf\")\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        training_history[\"train_loss\"].append(train_loss)\n",
    "        training_history[\"val_loss\"].append(val_loss)\n",
    "        training_history[\"train_acc\"].append(train_acc)\n",
    "        training_history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"Loss/Training\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Training\", train_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(\n",
    "                    f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, Acc={val_acc:.4f}\"\n",
    "                )\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (\n",
    "                (current_metric > best_metric)\n",
    "                if mode == \"max\"\n",
    "                else (current_metric < best_metric)\n",
    "            )\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(\n",
    "                    model.state_dict(), \"models/\" + experiment_name + \"_model.pt\"\n",
    "                )\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\" + experiment_name + \"_model.pt\"))\n",
    "        print(\n",
    "            f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\"\n",
    "        )\n",
    "\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed09202",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Train EfficientNetB0 from Scratch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744a6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0FromScratch(nn.Module):\n",
    "    \"\"\"EfficientNet-B0 trained from scratch (Random weights).\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load architecture with NO pretrained weights\n",
    "        self.backbone = torchvision.models.efficientnet_b0(weights=None)\n",
    "\n",
    "        # Re-build classifier head\n",
    "        # EfficientNet classifier is a Sequential; the Linear layer is the last one [-1]\n",
    "        in_features = self.backbone.classifier[-1].in_features\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(in_features, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79b47cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              SiLU-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "              SiLU-6         [-1, 32, 112, 112]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             264\n",
      "              SiLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-11             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 32, 112, 112]               0\n",
      "           Conv2d-13         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-14         [-1, 16, 112, 112]              32\n",
      "           MBConv-15         [-1, 16, 112, 112]               0\n",
      "           Conv2d-16         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-17         [-1, 96, 112, 112]             192\n",
      "             SiLU-18         [-1, 96, 112, 112]               0\n",
      "           Conv2d-19           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-20           [-1, 96, 56, 56]             192\n",
      "             SiLU-21           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0\n",
      "           Conv2d-23              [-1, 4, 1, 1]             388\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-26             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-27           [-1, 96, 56, 56]               0\n",
      "           Conv2d-28           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-29           [-1, 24, 56, 56]              48\n",
      "           MBConv-30           [-1, 24, 56, 56]               0\n",
      "           Conv2d-31          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-32          [-1, 144, 56, 56]             288\n",
      "             SiLU-33          [-1, 144, 56, 56]               0\n",
      "           Conv2d-34          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-35          [-1, 144, 56, 56]             288\n",
      "             SiLU-36          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0\n",
      "           Conv2d-38              [-1, 6, 1, 1]             870\n",
      "             SiLU-39              [-1, 6, 1, 1]               0\n",
      "           Conv2d-40            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-41            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-42          [-1, 144, 56, 56]               0\n",
      "           Conv2d-43           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-44           [-1, 24, 56, 56]              48\n",
      "  StochasticDepth-45           [-1, 24, 56, 56]               0\n",
      "           MBConv-46           [-1, 24, 56, 56]               0\n",
      "           Conv2d-47          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-48          [-1, 144, 56, 56]             288\n",
      "             SiLU-49          [-1, 144, 56, 56]               0\n",
      "           Conv2d-50          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-51          [-1, 144, 28, 28]             288\n",
      "             SiLU-52          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n",
      "           Conv2d-54              [-1, 6, 1, 1]             870\n",
      "             SiLU-55              [-1, 6, 1, 1]               0\n",
      "           Conv2d-56            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-57            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-58          [-1, 144, 28, 28]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "           MBConv-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-63          [-1, 240, 28, 28]             480\n",
      "             SiLU-64          [-1, 240, 28, 28]               0\n",
      "           Conv2d-65          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-66          [-1, 240, 28, 28]             480\n",
      "             SiLU-67          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0\n",
      "           Conv2d-69             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-70             [-1, 10, 1, 1]               0\n",
      "           Conv2d-71            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-72            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-73          [-1, 240, 28, 28]               0\n",
      "           Conv2d-74           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-75           [-1, 40, 28, 28]              80\n",
      "  StochasticDepth-76           [-1, 40, 28, 28]               0\n",
      "           MBConv-77           [-1, 40, 28, 28]               0\n",
      "           Conv2d-78          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-79          [-1, 240, 28, 28]             480\n",
      "             SiLU-80          [-1, 240, 28, 28]               0\n",
      "           Conv2d-81          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-82          [-1, 240, 14, 14]             480\n",
      "             SiLU-83          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0\n",
      "           Conv2d-85             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-86             [-1, 10, 1, 1]               0\n",
      "           Conv2d-87            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-88            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-89          [-1, 240, 14, 14]               0\n",
      "           Conv2d-90           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-91           [-1, 80, 14, 14]             160\n",
      "           MBConv-92           [-1, 80, 14, 14]               0\n",
      "           Conv2d-93          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-94          [-1, 480, 14, 14]             960\n",
      "             SiLU-95          [-1, 480, 14, 14]               0\n",
      "           Conv2d-96          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-97          [-1, 480, 14, 14]             960\n",
      "             SiLU-98          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0\n",
      "          Conv2d-100             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-101             [-1, 20, 1, 1]               0\n",
      "          Conv2d-102            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-103            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-104          [-1, 480, 14, 14]               0\n",
      "          Conv2d-105           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-106           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-107           [-1, 80, 14, 14]               0\n",
      "          MBConv-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      "            SiLU-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "            SiLU-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Conv2d-116             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-117             [-1, 20, 1, 1]               0\n",
      "          Conv2d-118            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-119            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-123           [-1, 80, 14, 14]               0\n",
      "          MBConv-124           [-1, 80, 14, 14]               0\n",
      "          Conv2d-125          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-126          [-1, 480, 14, 14]             960\n",
      "            SiLU-127          [-1, 480, 14, 14]               0\n",
      "          Conv2d-128          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-129          [-1, 480, 14, 14]             960\n",
      "            SiLU-130          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0\n",
      "          Conv2d-132             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-133             [-1, 20, 1, 1]               0\n",
      "          Conv2d-134            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-135            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-136          [-1, 480, 14, 14]               0\n",
      "          Conv2d-137          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-138          [-1, 112, 14, 14]             224\n",
      "          MBConv-139          [-1, 112, 14, 14]               0\n",
      "          Conv2d-140          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-142          [-1, 672, 14, 14]               0\n",
      "          Conv2d-143          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-144          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-145          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0\n",
      "          Conv2d-147             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-148             [-1, 28, 1, 1]               0\n",
      "          Conv2d-149            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-150            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-151          [-1, 672, 14, 14]               0\n",
      "          Conv2d-152          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-153          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-154          [-1, 112, 14, 14]               0\n",
      "          MBConv-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-157          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-158          [-1, 672, 14, 14]               0\n",
      "          Conv2d-159          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-161          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n",
      "          Conv2d-163             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-164             [-1, 28, 1, 1]               0\n",
      "          Conv2d-165            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-166            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-167          [-1, 672, 14, 14]               0\n",
      "          Conv2d-168          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-170          [-1, 112, 14, 14]               0\n",
      "          MBConv-171          [-1, 112, 14, 14]               0\n",
      "          Conv2d-172          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-174          [-1, 672, 14, 14]               0\n",
      "          Conv2d-175            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-176            [-1, 672, 7, 7]           1,344\n",
      "            SiLU-177            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0\n",
      "          Conv2d-179             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-180             [-1, 28, 1, 1]               0\n",
      "          Conv2d-181            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-182            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-183            [-1, 672, 7, 7]               0\n",
      "          Conv2d-184            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-185            [-1, 192, 7, 7]             384\n",
      "          MBConv-186            [-1, 192, 7, 7]               0\n",
      "          Conv2d-187           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-189           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-190           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-191           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-192           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-194             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-195             [-1, 48, 1, 1]               0\n",
      "          Conv2d-196           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-197           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-198           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-199            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-200            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-201            [-1, 192, 7, 7]               0\n",
      "          MBConv-202            [-1, 192, 7, 7]               0\n",
      "          Conv2d-203           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-204           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-205           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-206           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-208           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-210             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-211             [-1, 48, 1, 1]               0\n",
      "          Conv2d-212           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-213           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-214           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-215            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-216            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-217            [-1, 192, 7, 7]               0\n",
      "          MBConv-218            [-1, 192, 7, 7]               0\n",
      "          Conv2d-219           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-220           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-221           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-222           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-224           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-226             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-227             [-1, 48, 1, 1]               0\n",
      "          Conv2d-228           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-229           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-230           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-231            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-232            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-233            [-1, 192, 7, 7]               0\n",
      "          MBConv-234            [-1, 192, 7, 7]               0\n",
      "          Conv2d-235           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-236           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-237           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-238           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-240           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-242             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-243             [-1, 48, 1, 1]               0\n",
      "          Conv2d-244           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-245           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-246           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-247            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-248            [-1, 320, 7, 7]             640\n",
      "          MBConv-249            [-1, 320, 7, 7]               0\n",
      "          Conv2d-250           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-251           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-252           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0\n",
      "         Dropout-254                 [-1, 1280]               0\n",
      "          Linear-255                    [-1, 4]           5,124\n",
      "    EfficientNet-256                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 4,012,672\n",
      "Trainable params: 4,012,672\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 173.64\n",
      "Params size (MB): 15.31\n",
      "Estimated Total Size (MB): 189.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "scratch_model = EfficientNetB0FromScratch(len(LABEL_MAP), DROPOUT_RATE).to(device)\n",
    "\n",
    "# Visualize structure\n",
    "summary(scratch_model, input_size=input_shape)\n",
    "try:\n",
    "    from torchview import draw_graph\n",
    "\n",
    "    model_graph = draw_graph(\n",
    "        scratch_model,\n",
    "        input_size=(BATCH_SIZE,) + input_shape,\n",
    "        expand_nested=True,\n",
    "        depth=6,\n",
    "    )\n",
    "    model_graph.visual_graph\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize model graph: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cc450ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Training\n",
    "experiment_name = \"from_scratch\"\n",
    "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + experiment_name)\n",
    "optimizer = torch.optim.Adam(\n",
    "    scratch_model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6101ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 200 epochs...\n",
      "CPU times: user 8.88 s, sys: 2.55 s, total: 11.4 s\n",
      "Wall time: 30.6 s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got weight is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Train the model from scratch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mscratch_model, history = fit(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    model=scratch_model,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    train_loader=train_loader,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    val_loader=val_loader,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    epochs=EPOCHS,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    criterion=criterion,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    optimizer=optimizer,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    scaler=scaler,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    device=device,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    writer=writer,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    verbose=5,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    patience=PATIENCE,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    experiment_name=experiment_name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/IPython/core/magics/execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:2\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device, patience, evaluation_metric, mode, restore_best_weights, writer, verbose, experiment_name)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n\u001b[32m     41\u001b[39m     training_history[\u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m].append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, scaler, device)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\n\u001b[32m     15\u001b[39m     device_type=device.type, enabled=(device.type == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m ):\n\u001b[32m     17\u001b[39m     logits = model(inputs)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m scaler.scale(loss).backward()\n\u001b[32m     21\u001b[39m scaler.step(optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/an2dl-challenge-2/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got weight is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model from scratch\n",
    "scratch_model, history = fit(\n",
    "    model=scratch_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=5,\n",
    "    patience=PATIENCE,\n",
    "    experiment_name=experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot History\n",
    "# Create a figure with two side-by-side subplots\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(\n",
    "    history[\"train_loss\"],\n",
    "    label=\"Training loss\",\n",
    "    alpha=0.3,\n",
    "    color=\"#ff7f0e\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax1.plot(history[\"val_loss\"], label=\"Validation loss\", alpha=0.9, color=\"#ff7f0e\")\n",
    "ax1.set_title(\"Categorical Crossentropy\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation Accuracy on the second axis\n",
    "ax2.plot(\n",
    "    history[\"train_acc\"],\n",
    "    label=\"Training Accuracy\",\n",
    "    alpha=0.3,\n",
    "    color=\"#ff7f0e\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax2.plot(history[\"val_acc\"], label=\"Validation Accuracy\", alpha=0.9, color=\"#ff7f0e\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769add2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
