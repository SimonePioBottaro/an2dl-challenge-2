{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2e021f",
   "metadata": {},
   "source": [
    "# **AN2DL Challenge 2 - Image Classification**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d28923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment\n",
    "isColab = False\n",
    "isKaggle = False\n",
    "isWsl = False\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaefbbd",
   "metadata": {},
   "source": [
    "## **Loading Enviroment**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d315fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory di default\n",
    "current_dir = os.getcwd()   \n",
    "\n",
    "if isColab:\n",
    "    from google.colab import drive\n",
    "    \n",
    "    drive.mount(\"/gdrive\")\n",
    "    current_dir = \"/gdrive/My\\\\ Drive/Colab\\\\ Notebooks/[2025-2026]\\\\ AN2DL/AN2DL-challenge-2\"\n",
    "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
    "    %cd $current_dir\n",
    "elif isKaggle:\n",
    "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
    "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
    "    current_dir = kaggle_work_dir\n",
    "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
    "    os.chdir(current_dir)\n",
    "elif isWsl:\n",
    "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
    "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
    "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
    "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
    "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
    "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "print(f\"Changed directory to: {current_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70e1ac",
   "metadata": {},
   "source": [
    "## **Import Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "%pip install torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "# Configurazione di TensorBoard e directory\n",
    "logs_dir = \"tensorboard\"\n",
    "if isColab or isKaggle:\n",
    "    !pkill -f tensorboard\n",
    "    !mkdir -p models\n",
    "    print(\"Killed existing TensorBoard instances and created models directory.\")\n",
    "else:\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    \n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import cv2\n",
    "import copy\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954322c",
   "metadata": {},
   "source": [
    "## **Dataset Downloading**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
    "\n",
    "if isColab:\n",
    "    # Clean up path for Python usage (remove shell escapes)\n",
    "    clean_current_dir = current_dir.replace('\\\\ ', ' ')\n",
    "    drive_dataset_dir = os.path.join(clean_current_dir, \"dataset\")\n",
    "    local_dataset_dir = \"/content/dataset\"\n",
    "    \n",
    "    if not os.path.exists(local_dataset_dir):\n",
    "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
    "        try:\n",
    "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
    "            print(\"Copy complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying dataset: {e}\")\n",
    "            print(\"Falling back to Drive dataset (slow).\")\n",
    "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
    "            dataset_dir = drive_dataset_dir\n",
    "    else:\n",
    "        print(\"Dataset already copied to local runtime.\")\n",
    "    \n",
    "    # If copy succeeded (or already existed), use local path\n",
    "    if os.path.exists(local_dataset_dir):\n",
    "        dataset_dir = local_dataset_dir\n",
    "\n",
    "elif isKaggle:\n",
    "    # Nothing to do, dataset is already available in Kaggle environment\n",
    "    print(\"Running on Kaggle. Dataset is assumed to be already available.\")\n",
    "    print(f\"Dataset directory: {dataset_dir}\")\n",
    "else:\n",
    "    # Check if dataset is already downloaded\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "    if not os.listdir(dataset_dir):\n",
    "        print(\"Downloading dataset from Kaggle in local environment...\")\n",
    "        os.chdir(dataset_dir)\n",
    "        !kaggle competitions download -c an2dl2526c2\n",
    "        zip_file = \"an2dl2526c2.zip\"\n",
    "        shutil.unpack_archive(zip_file, extract_dir=\".\")\n",
    "        os.remove(zip_file)\n",
    "        os.chdir(current_dir)\n",
    "    else:\n",
    "        print(\"Dataset already present in local environment. Skipping download.\")\n",
    "\n",
    "# Define absolute paths\n",
    "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
    "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
    "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
    "        \n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Train set directory: {train_set_dir}\")\n",
    "print(f\"Test set directory: {test_set_dir}\")\n",
    "print(f\"Label file: {label_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c7507",
   "metadata": {},
   "source": [
    "## ‚è≥ **Data Loading**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader parameters\n",
    "APPLY_MASK = False\n",
    "BATCH_SIZE = 32\n",
    "LOADER_SHUFFLE = False\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "LABEL_MAP = {\"Luminal A\": 0, \"Luminal B\": 1, \"HER2(+)\": 2, \"Triple negative\": 3}\n",
    "IMG_RESIZE = (224, 224)\n",
    "input_shape = (3, *IMG_RESIZE)\n",
    "num_classes = len(LABEL_MAP)\n",
    "\n",
    "TEST_SET_SIZE = 0.2\n",
    "VAL_SET_SIZE = 0.2\n",
    "TRAIN_SET_SIZE = 1.0 - TEST_SET_SIZE - VAL_SET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# No augmentation for validation/test\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMG_RESIZE, antialias=True),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcb081",
   "metadata": {},
   "source": [
    "### **Definitions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00064880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IronGutsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for the Iron-Guts (Breast Cancer) competition.\n",
    "\n",
    "    Features:\n",
    "    1. Lazy Loading: Reads images from disk on-the-fly to save RAM.\n",
    "    2. Integrity Checks: Ensures every image has a corresponding mask.\n",
    "    3. Mask Gating: Uses the binary mask to suppress background noise (setting it to pure black).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        data,\n",
    "        transform=None,\n",
    "        augmentation=None,\n",
    "        target_transform=None,\n",
    "        apply_mask=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all images and masks.\n",
    "            data (str or pd.DataFrame): Path to the CSV file with annotations or the DataFrame itself.\n",
    "            transform (callable, optional): Transform to be applied on the image (e.g., Resize, ToTensor).\n",
    "            augmentation (callable, optional): Augmentation transforms to be applied on the image.\n",
    "            target_transform (callable, optional): Transform to be applied on the label.\n",
    "            apply_mask (bool, optional): Whether to apply mask gating. Default: True.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.target_transform = target_transform\n",
    "        self.apply_mask = apply_mask\n",
    "\n",
    "        # Load the CSV\n",
    "        if isinstance(data, str):\n",
    "            self.annotations = pd.read_csv(data)\n",
    "            print(f\"Loaded {len(self.annotations)} annotations from {data}\")\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            self.annotations = data\n",
    "            print(f\"Loaded {len(self.annotations)} annotations from DataFrame\")\n",
    "        else:\n",
    "            raise ValueError(\"data must be a file path or a pandas DataFrame\")\n",
    "\n",
    "        # Define class mapping based on the biological subtypes\n",
    "        # ['Triple negative' 'Luminal A' 'Luminal B' 'HER2(+)']\n",
    "        self.label_map = LABEL_MAP\n",
    "\n",
    "        # Validation: Check that dataset is not empty\n",
    "        if len(self.annotations) == 0:\n",
    "            raise RuntimeError(\"Dataset CSV is empty.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # 1. Parse File Paths\n",
    "        # The CSV contains 'sample_index' like 'img_5', we need to append extensions.\n",
    "        img_id = self.annotations.iloc[idx][\"sample_index\"]\n",
    "        img_name = f\"{img_id}\"\n",
    "        mask_name = f\"{img_id.replace('img_', 'mask_')}\"\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        mask_path = os.path.join(self.root_dir, mask_name)\n",
    "\n",
    "        # 2. Load Data (Lazy Operation)\n",
    "        # We convert image to RGB (3 channels) and mask to L (grayscale/binary)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.apply_mask:\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Missing file pair: {img_name} or {mask_name}\")\n",
    "\n",
    "        # 3. Apply Mask Gating (Background Suppression) - only if flag is enabled\n",
    "        if self.apply_mask:\n",
    "            # Tissue is signal, background is noise.\n",
    "            # We multiply the image by the mask to force background to absolute 0.\n",
    "            image_np = np.array(image)\n",
    "            mask_np = np.array(mask)\n",
    "\n",
    "            # Ensure mask is binary (0 or 1) for broadcasting\n",
    "            # Any pixel > 0 in the mask is considered tissue\n",
    "            binary_mask = (mask_np > 0).astype(np.uint8)\n",
    "\n",
    "            # Expand dimensions of mask to match image (H, W, 1) for broadcasting\n",
    "            binary_mask = np.expand_dims(binary_mask, axis=-1)\n",
    "\n",
    "            # Apply gating: Image * Mask\n",
    "            masked_image_np = image_np * binary_mask\n",
    "\n",
    "            # Convert back to PIL for standard PyTorch transforms\n",
    "            masked_image = Image.fromarray(masked_image_np)\n",
    "        else:\n",
    "            masked_image = image\n",
    "\n",
    "        # 4. Apply Transforms (e.g., Resize, ToTensor, Normalize)\n",
    "        if self.transform:\n",
    "            masked_image = self.transform(masked_image)\n",
    "\n",
    "        if self.augmentation:\n",
    "            masked_image = self.augmentation(masked_image)\n",
    "\n",
    "        # 5. Handle Labels\n",
    "        label_str = self.annotations.iloc[idx][\"label\"]\n",
    "        label = self.label_map[str(label_str)]\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return masked_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, cpu_cores)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers if isColab or isKaggle or isWsl else 0,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4 if isColab or isKaggle or isWsl else None,\n",
    "        persistent_workers=(isColab or isKaggle or isWsl) and num_workers > 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_TO_IGNORE = [\n",
    "    \"img_0102.png\", \"img_0104.png\", \"img_0108.png\", \"img_0109.png\", \"img_0112.png\",\n",
    "    \"img_0130.png\", \"img_0147.png\", \"img_0152.png\", \"img_0153.png\", \"img_0168.png\",\n",
    "    \"img_0173.png\", \"img_0176.png\", \"img_0182.png\", \"img_0189.png\", \"img_0193.png\",\n",
    "    \"img_0203.png\", \"img_0213.png\", \"img_0218.png\", \"img_0223.png\", \"img_0228.png\",\n",
    "    \"img_0232.png\", \"img_0237.png\", \"img_0239.png\", \"img_0249.png\", \"img_0250.png\",\n",
    "    \"img_0256.png\", \"img_0264.png\", \"img_0269.png\", \"img_0270.png\", \"img_0271.png\",\n",
    "    \"img_0276.png\", \"img_0277.png\", \"img_0282.png\", \"img_0290.png\", \"img_0291.png\",\n",
    "    \"img_0304.png\", \"img_0308.png\", \"img_0318.png\", \"img_0322.png\", \"img_0323.png\",\n",
    "    \"img_0328.png\", \"img_0336.png\", \"img_0342.png\", \"img_0348.png\", \"img_0357.png\",\n",
    "    \"img_0365.png\", \"img_0368.png\", \"img_0369.png\", \"img_0370.png\", \"img_0379.png\",\n",
    "    \"img_0384.png\", \"img_0386.png\", \"img_0390.png\", \"img_0391.png\", \"img_0394.png\",\n",
    "    \"img_0404.png\", \"img_0406.png\", \"img_0411.png\", \"img_0413.png\", \"img_0418.png\",\n",
    "    \"img_0422.png\", \"img_0426.png\", \"img_0428.png\", \"img_0430.png\", \"img_0436.png\",\n",
    "    \"img_0438.png\", \"img_0442.png\", \"img_0446.png\", \"img_0447.png\", \"img_0448.png\",\n",
    "    \"img_0451.png\", \"img_0454.png\", \"img_0455.png\", \"img_0456.png\", \"img_0469.png\",\n",
    "    \"img_0471.png\", \"img_0478.png\", \"img_0480.png\", \"img_0481.png\", \"img_0487.png\",\n",
    "    \"img_0489.png\", \"img_0492.png\", \"img_0493.png\", \"img_0495.png\", \"img_0503.png\",\n",
    "    \"img_0505.png\", \"img_0509.png\", \"img_0511.png\", \"img_0512.png\", \"img_0514.png\",\n",
    "    \"img_0516.png\", \"img_0518.png\", \"img_0520.png\", \"img_0521.png\", \"img_0526.png\",\n",
    "    \"img_0527.png\", \"img_0529.png\", \"img_0536.png\", \"img_0554.png\", \"img_0555.png\",\n",
    "    \"img_0559.png\", \"img_0572.png\", \"img_0574.png\", \"img_0586.png\", \"img_0589.png\",\n",
    "    \"img_0592.png\", \"img_0594.png\", \"img_0597.png\", \"img_0600.png\", \"img_0606.png\",\n",
    "    \"img_0608.png\", \"img_0612.png\", \"img_0629.png\", \"img_0631.png\", \"img_0648.png\",\n",
    "    \"img_0650.png\", \"img_0652.png\", \"img_0653.png\", \"img_0655.png\", \"img_0665.png\",\n",
    "    \"img_0673.png\", \"img_0681.png\", \"img_0687.png\", \"img_0703.png\", \"img_0714.png\",\n",
    "    \"img_0731.png\", \"img_0733.png\", \"img_0735.png\", \"img_0748.png\", \"img_0753.png\",\n",
    "    \"img_0755.png\", \"img_0758.png\", \"img_0767.png\", \"img_0771.png\", \"img_0796.png\",\n",
    "    \"img_0800.png\", \"img_0804.png\", \"img_0813.png\", \"img_0817.png\", \"img_0819.png\",\n",
    "    \"img_0822.png\", \"img_0825.png\", \"img_0826.png\", \"img_0829.png\", \"img_0832.png\",\n",
    "    \"img_0866.png\", \"img_0868.png\", \"img_0892.png\", \"img_0893.png\", \"img_0898.png\",\n",
    "    \"img_0903.png\", \"img_0906.png\", \"img_0907.png\", \"img_0910.png\", \"img_0913.png\",\n",
    "    \"img_0918.png\", \"img_0919.png\", \"img_0924.png\", \"img_0930.png\", \"img_0935.png\",\n",
    "    \"img_0936.png\", \"img_0937.png\", \"img_0941.png\", \"img_0944.png\", \"img_0958.png\",\n",
    "    \"img_0960.png\", \"img_0963.png\", \"img_0973.png\", \"img_0982.png\", \"img_0991.png\",\n",
    "    \"img_0992.png\", \"img_0999.png\", \"img_1004.png\", \"img_1005.png\", \"img_1006.png\",\n",
    "    \"img_1009.png\", \"img_1011.png\", \"img_1015.png\", \"img_1017.png\", \"img_1021.png\",\n",
    "    \"img_1023.png\", \"img_1035.png\", \"img_1037.png\", \"img_1039.png\", \"img_1041.png\",\n",
    "    \"img_1046.png\", \"img_1062.png\", \"img_1086.png\", \"img_1108.png\", \"img_1109.png\",\n",
    "    \"img_1114.png\", \"img_1124.png\", \"img_1125.png\", \"img_1133.png\", \"img_1145.png\",\n",
    "    \"img_1146.png\", \"img_1150.png\", \"img_1156.png\", \"img_1177.png\", \"img_1184.png\",\n",
    "    \"img_1186.png\", \"img_1202.png\", \"img_1206.png\", \"img_1212.png\", \"img_1217.png\",\n",
    "    \"img_1218.png\", \"img_1220.png\", \"img_1223.png\", \"img_1229.png\", \"img_1232.png\",\n",
    "    \"img_1236.png\", \"img_1241.png\", \"img_1245.png\", \"img_1258.png\", \"img_1261.png\",\n",
    "    \"img_1263.png\", \"img_1267.png\", \"img_1271.png\", \"img_1274.png\", \"img_1277.png\",\n",
    "    \"img_1280.png\", \"img_1281.png\", \"img_1298.png\", \"img_1300.png\", \"img_1312.png\",\n",
    "    \"img_1317.png\", \"img_1320.png\", \"img_1326.png\", \"img_1327.png\", \"img_1329.png\",\n",
    "    \"img_1332.png\", \"img_1334.png\", \"img_1335.png\", \"img_1338.png\", \"img_1340.png\",\n",
    "    \"img_1360.png\", \"img_1361.png\", \"img_1362.png\", \"img_1367.png\", \"img_1369.png\",\n",
    "    \"img_1375.png\", \"img_1376.png\", \"img_1377.png\", \"img_1381.png\", \"img_1385.png\",\n",
    "    \"img_1388.png\", \"img_1389.png\", \"img_1392.png\", \"img_0846.png\",\"img_1192.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the full dataframe\n",
    "full_df = pd.read_csv(label_file)\n",
    "\n",
    "# Remove cursed images\n",
    "full_df = full_df[~full_df['sample_index'].isin(SAMPLES_TO_IGNORE)].reset_index(drop=True)\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    full_df,\n",
    "    test_size=(TEST_SET_SIZE + VAL_SET_SIZE),\n",
    "    stratify=full_df[\"label\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=VAL_SET_SIZE,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Create Datasets\n",
    "train_set = IronGutsDataset(\n",
    "    root_dir=train_set_dir,\n",
    "    data=train_df,\n",
    "    transform=transform,\n",
    "    augmentation=train_augmentation,\n",
    "    apply_mask=APPLY_MASK,\n",
    ")\n",
    "\n",
    "val_set = IronGutsDataset(\n",
    "    root_dir=train_set_dir,\n",
    "    data=val_df,\n",
    "    transform=transform,\n",
    "    apply_mask=APPLY_MASK,\n",
    ")\n",
    "\n",
    "test_set = IronGutsDataset(\n",
    "    root_dir=train_set_dir,\n",
    "    data=test_df,\n",
    "    transform=transform,\n",
    "    apply_mask=APPLY_MASK,\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = make_loader(\n",
    "    ds=train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = make_loader(\n",
    "    ds=val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = make_loader(\n",
    "    ds=test_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "# Check a batch\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc4a5c",
   "metadata": {},
   "source": [
    "## üßÆ **Network Parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "\n",
    "# Regularization\n",
    "DROPOUT_RATE = 0.3\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# Set up loss function\n",
    "# Manage Class imbalance\n",
    "class_counts = full_df[\"label\"].value_counts().sort_index()\n",
    "total_samples = len(full_df)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "# Convert to tensor for PyTorch usage later\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=LABEL_SMOOTHING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa95a8",
   "metadata": {},
   "source": [
    "## üß† **Training Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(\n",
    "            device_type=device.type, enabled=(device.type == \"cuda\")\n",
    "        ):\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = accuracy_score(\n",
    "        np.concatenate(all_targets), np.concatenate(all_predictions)\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            with torch.amp.autocast(\n",
    "                device_type=device.type, enabled=(device.type == \"cuda\")\n",
    "            ):\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = logits.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = accuracy_score(\n",
    "        np.concatenate(all_targets), np.concatenate(all_predictions)\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d487b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    device,\n",
    "    patience=0,\n",
    "    evaluation_metric=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    restore_best_weights=True,\n",
    "    writer=None,\n",
    "    verbose=1,\n",
    "    experiment_name=\"\",\n",
    "):\n",
    "    \"\"\"Train the neural network model.\"\"\"\n",
    "\n",
    "    training_history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "    }\n",
    "\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float(\"-inf\") if mode == \"max\" else float(\"inf\")\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        training_history[\"train_loss\"].append(train_loss)\n",
    "        training_history[\"val_loss\"].append(val_loss)\n",
    "        training_history[\"train_acc\"].append(train_acc)\n",
    "        training_history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"Loss/Training\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Training\", train_acc, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(\n",
    "                    f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                    f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "                    f\"Val: Loss={val_loss:.4f}, Acc={val_acc:.4f}\"\n",
    "                )\n",
    "\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (\n",
    "                (current_metric > best_metric)\n",
    "                if mode == \"max\"\n",
    "                else (current_metric < best_metric)\n",
    "            )\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(\n",
    "                    model.state_dict(), \"models/\" + experiment_name + \"_model.pt\"\n",
    "                )\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\" + experiment_name + \"_model.pt\"))\n",
    "        print(\n",
    "            f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\"\n",
    "        )\n",
    "\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\" + experiment_name + \"_model.pt\")\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed09202",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Train EfficientNetB0 from Scratch**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0FromScratch(nn.Module):\n",
    "    \"\"\"EfficientNet-B0 trained from scratch (Random weights).\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load architecture with NO pretrained weights\n",
    "        self.backbone = torchvision.models.efficientnet_b0(weights=None)\n",
    "\n",
    "        # Re-build classifier head\n",
    "        # EfficientNet classifier is a Sequential; the Linear layer is the last one [-1]\n",
    "        in_features = self.backbone.classifier[-1].in_features\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate, inplace=True),\n",
    "            nn.Linear(in_features, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "scratch_model = EfficientNetB0FromScratch(len(LABEL_MAP), DROPOUT_RATE).to(device)\n",
    "\n",
    "# Visualize structure\n",
    "summary(scratch_model, input_size=input_shape)\n",
    "try:\n",
    "    from torchview import draw_graph\n",
    "\n",
    "    model_graph = draw_graph(\n",
    "        scratch_model,\n",
    "        input_size=(BATCH_SIZE,) + input_shape,\n",
    "        expand_nested=True,\n",
    "        depth=6,\n",
    "    )\n",
    "    model_graph.visual_graph\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize model graph: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc450ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Training\n",
    "experiment_name = \"from_scratch\"\n",
    "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + experiment_name)\n",
    "optimizer = torch.optim.Adam(\n",
    "    scratch_model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model from scratch\n",
    "scratch_model, history = fit(\n",
    "    model=scratch_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    verbose=5,\n",
    "    patience=PATIENCE,\n",
    "    experiment_name=experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plot History\n",
    "# Create a figure with two side-by-side subplots\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "# Plot of training and validation loss on the first axis\n",
    "ax1.plot(\n",
    "    history[\"train_loss\"],\n",
    "    label=\"Training loss\",\n",
    "    alpha=0.3,\n",
    "    color=\"#ff7f0e\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax1.plot(history[\"val_loss\"], label=\"Validation loss\", alpha=0.9, color=\"#ff7f0e\")\n",
    "ax1.set_title(\"Categorical Crossentropy\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot of training and validation Accuracy on the second axis\n",
    "ax2.plot(\n",
    "    history[\"train_acc\"],\n",
    "    label=\"Training Accuracy\",\n",
    "    alpha=0.3,\n",
    "    color=\"#ff7f0e\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax2.plot(history[\"val_acc\"], label=\"Validation Accuracy\", alpha=0.9, color=\"#ff7f0e\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769add2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
