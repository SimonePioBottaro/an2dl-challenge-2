{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a75bb6ad",
      "metadata": {},
      "source": [
        "# **Soft voting**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900b0225",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enviroment\n",
        "isColab = False\n",
        "colab_dir = \"/gdrive/My Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "\n",
        "isKaggle = False\n",
        "isWsl = True\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "591ec1c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loader parameters\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\n",
        "NORMALIZATION_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "IMG_RESIZE = (224, 224)\n",
        "INPUT_SHAPE = (3, *IMG_RESIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "e38d561c",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"efficientNetV2_S_fra\"\n",
        "NET_NAME = \"efficientnet\"\n",
        "\n",
        "# Fine tuning parameters\n",
        "FT_DROPOUT_RATE = 0.3\n",
        "K_FOLD = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4cb00e",
      "metadata": {},
      "source": [
        "## **Loading Enviroment**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "e09b8c7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esecuzione su WSL. Directory corrente impostata a: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
            "Changed directory to: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\n",
            "Dataset directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset\n",
            "Train set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_data\n",
            "Test set directory: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/test_data\n",
            "Label file: /mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2/dataset/train_labels.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Directory di default\n",
        "current_dir = os.getcwd()   \n",
        "\n",
        "if isColab:\n",
        "    from google.colab import drive # type: ignore\n",
        "    drive.mount(\"/gdrive\")\n",
        "    current_dir = colab_dir\n",
        "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
        "    %cd $current_dir\n",
        "elif isKaggle:\n",
        "    kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-2\"\n",
        "    os.makedirs(kaggle_work_dir, exist_ok=True)\n",
        "    current_dir = kaggle_work_dir\n",
        "    print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
        "    os.chdir(current_dir)\n",
        "elif isWsl:\n",
        "    local_pref = r\"/mnt/g/Il mio Drive/Colab Notebooks/[2025-2026] AN2DL/AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Esecuzione su WSL. Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "else:\n",
        "    print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
        "    local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-2\"\n",
        "    current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
        "    print(f\"Directory corrente impostata a: {current_dir}\")\n",
        "    os.chdir(current_dir)\n",
        "\n",
        "print(f\"Changed directory to: {current_dir}\")\n",
        "\n",
        "# Define absolute paths\n",
        "dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "train_set_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "test_set_dir = os.path.join(dataset_dir, \"test_data\")\n",
        "label_file = os.path.join(dataset_dir, \"train_labels.csv\")\n",
        "\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Train set directory: {train_set_dir}\")\n",
        "print(f\"Test set directory: {test_set_dir}\")\n",
        "print(f\"Label file: {label_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e0fdc8",
      "metadata": {},
      "source": [
        "## **Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "be49326d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be49326d",
        "outputId": "59e423d7-35ea-4fc1-933c-affb303ee9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchview in /home/berto/an2dl-challenge-2/.venv/lib/python3.12/site-packages (0.2.7)\n",
            "Requirement already satisfied: graphviz in /home/berto/an2dl-challenge-2/.venv/lib/python3.12/site-packages (from torchview) (0.21)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "PyTorch version: 2.9.1+cu130\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "%pip install torchview\n",
        "from torchview import draw_graph\n",
        "\n",
        "\n",
        "# Configurazione di TensorBoard e directory\n",
        "logs_dir = \"tensorboard\"\n",
        "if isColab or isKaggle:\n",
        "    !pkill -f tensorboard \n",
        "    !mkdir -p models\n",
        "    print(\"Killed existing TensorBoard instances and created models directory.\") \n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)  \n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import cv2\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.gridspec as gridspec\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import ndimage\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcbf293",
      "metadata": {},
      "source": [
        "### **Preparing Dataset for colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "c8f17001",
      "metadata": {},
      "outputs": [],
      "source": [
        "if isColab:\n",
        "    drive_dataset_dir = os.path.join(current_dir, \"dataset\")\n",
        "    local_dataset_dir = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(local_dataset_dir):\n",
        "        print(f\"Copying dataset from {drive_dataset_dir} to {local_dataset_dir}...\")\n",
        "        try:\n",
        "            shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
        "            print(\"Copy complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error copying dataset: {e}\")\n",
        "            print(\"Falling back to Drive dataset (slow).\")\n",
        "            # If copy fails, we stick to the original dataset_dir (which might need cleaning too if it was used directly)\n",
        "            dataset_dir = drive_dataset_dir\n",
        "    else:\n",
        "        print(\"Dataset already copied to local runtime.\")\n",
        "\n",
        "    # If copy succeeded (or already existed), use local path\n",
        "    if os.path.exists(local_dataset_dir):\n",
        "        dataset_dir = local_dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cfe95f2",
      "metadata": {},
      "source": [
        "## â³ **Data Loading**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e112e14",
      "metadata": {},
      "source": [
        "### **Definitions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "7d945105",
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_IGNORE = [\n",
        "    \"img_0001.png\",\n",
        "    \"img_0005.png\",\n",
        "    \"img_0008.png\",\n",
        "    \"img_0012.png\",\n",
        "    \"img_0018.png\",\n",
        "    \"img_0020.png\",\n",
        "    \"img_0022.png\",\n",
        "    \"img_0027.png\",\n",
        "    \"img_0028.png\",\n",
        "    \"img_0036.png\",\n",
        "    \"img_0044.png\",\n",
        "    \"img_0047.png\",\n",
        "    \"img_0048.png\",\n",
        "    \"img_0052.png\",\n",
        "    \"img_0062.png\",\n",
        "    \"img_0078.png\",\n",
        "    \"img_0085.png\",\n",
        "    \"img_0090.png\",\n",
        "    \"img_0094.png\",\n",
        "    \"img_0095.png\",\n",
        "    \"img_0126.png\",\n",
        "    \"img_0129.png\",\n",
        "    \"img_0130.png\",\n",
        "    \"img_0133.png\",\n",
        "    \"img_0136.png\",\n",
        "    \"img_0138.png\",\n",
        "    \"img_0148.png\",\n",
        "    \"img_0150.png\",\n",
        "    \"img_0155.png\",\n",
        "    \"img_0159.png\",\n",
        "    \"img_0161.png\",\n",
        "    \"img_0175.png\",\n",
        "    \"img_0178.png\",\n",
        "    \"img_0179.png\",\n",
        "    \"img_0180.png\",\n",
        "    \"img_0184.png\",\n",
        "    \"img_0187.png\",\n",
        "    \"img_0189.png\",\n",
        "    \"img_0193.png\",\n",
        "    \"img_0196.png\",\n",
        "    \"img_0222.png\",\n",
        "    \"img_0251.png\",\n",
        "    \"img_0254.png\",\n",
        "    \"img_0263.png\",\n",
        "    \"img_0268.png\",\n",
        "    \"img_0286.png\",\n",
        "    \"img_0293.png\",\n",
        "    \"img_0313.png\",\n",
        "    \"img_0319.png\",\n",
        "    \"img_0333.png\",\n",
        "    \"img_0342.png\",\n",
        "    \"img_0344.png\",\n",
        "    \"img_0346.png\",\n",
        "    \"img_0355.png\",\n",
        "    \"img_0368.png\",\n",
        "    \"img_0371.png\",\n",
        "    \"img_0376.png\",\n",
        "    \"img_0380.png\",\n",
        "    \"img_0390.png\",\n",
        "    \"img_0393.png\",\n",
        "    \"img_0407.png\",\n",
        "    \"img_0410.png\",\n",
        "    \"img_0415.png\",\n",
        "    \"img_0424.png\",\n",
        "    \"img_0443.png\",\n",
        "    \"img_0453.png\",\n",
        "    \"img_0459.png\",\n",
        "    \"img_0463.png\",\n",
        "    \"img_0486.png\",\n",
        "    \"img_0497.png\",\n",
        "    \"img_0498.png\",\n",
        "    \"img_0499.png\",\n",
        "    \"img_0509.png\",\n",
        "    \"img_0521.png\",\n",
        "    \"img_0530.png\",\n",
        "    \"img_0531.png\",\n",
        "    \"img_0533.png\",\n",
        "    \"img_0537.png\",\n",
        "    \"img_0540.png\",\n",
        "    \"img_0544.png\",\n",
        "    \"img_0547.png\",\n",
        "    \"img_0557.png\",\n",
        "    \"img_0558.png\",\n",
        "    \"img_0560.png\",\n",
        "    \"img_0565.png\",\n",
        "    \"img_0567.png\",\n",
        "    \"img_0572.png\",\n",
        "    \"img_0578.png\",\n",
        "    \"img_0580.png\",\n",
        "    \"img_0586.png\",\n",
        "    \"img_0602.png\",\n",
        "    \"img_0603.png\",\n",
        "    \"img_0607.png\",\n",
        "    \"img_0609.png\",\n",
        "    \"img_0614.png\",\n",
        "    \"img_0620.png\",\n",
        "    \"img_0623.png\",\n",
        "    \"img_0629.png\",\n",
        "    \"img_0635.png\",\n",
        "    \"img_0639.png\",\n",
        "    \"img_0643.png\",\n",
        "    \"img_0644.png\",\n",
        "    \"img_0645.png\",\n",
        "    \"img_0646.png\",\n",
        "    \"img_0656.png\",\n",
        "    \"img_0657.png\",\n",
        "    \"img_0658.png\",\n",
        "    \"img_0670.png\",\n",
        "    \"img_0673.png\",\n",
        "    \"img_0675.png\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "7afdb353",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 4\n"
          ]
        }
      ],
      "source": [
        "# Load the full dataframe\n",
        "full_df = pd.read_csv(label_file)\n",
        "\n",
        "# Remove cursed images\n",
        "full_df = full_df[~full_df[\"sample_index\"].isin(SAMPLES_TO_IGNORE)].reset_index(\n",
        "    drop=True\n",
        ")\n",
        "\n",
        "# Label mapping\n",
        "class_names = sorted(full_df[\"label\"].unique())\n",
        "label_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
        "full_df[\"label_index\"] = full_df[\"label\"].map(label_to_index)\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "9e13a32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last=False):\n",
        "    \"\"\"Create a PyTorch DataLoader with optimized settings.\"\"\"\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(6, cpu_cores))\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,\n",
        "        persistent_workers=isWsl,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "9c536e21",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import ndimage\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "\n",
        "class MaskedFixedTileDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A Dataset class that extracts fixed-size patches from the center of tissue masks\n",
        "    to preserve biological scale (magnification), rather than resizing variable crops.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataframe, img_dir, transforms=None, target_size=(224, 224), debug_max=None\n",
        "    ):\n",
        "        self.samples = []\n",
        "        self.transforms = transforms\n",
        "        self.img_dir = img_dir\n",
        "        self.target_size = target_size\n",
        "\n",
        "        # Handling inference mode (no labels) vs training mode\n",
        "        self.is_inference_mode = False\n",
        "        if dataframe is None or \"label_index\" not in dataframe.columns:\n",
        "            self.is_inference_mode = True\n",
        "            if dataframe is None:\n",
        "                # If just a directory, list images\n",
        "                img_names = sorted(\n",
        "                    [f for f in os.listdir(img_dir) if f.startswith(\"img_\")]\n",
        "                )\n",
        "            else:\n",
        "                img_names = dataframe[\"sample_index\"].tolist()\n",
        "            iterator = zip(img_names, [-1] * len(img_names))\n",
        "            total_items = len(img_names)\n",
        "        else:\n",
        "            iterator = zip(dataframe[\"sample_index\"], dataframe[\"label_index\"])\n",
        "            total_items = len(dataframe)\n",
        "\n",
        "        print(\n",
        "            f\"Processing {total_items} images to extract fixed-size {target_size} tiles...\"\n",
        "        )\n",
        "\n",
        "        count = 0\n",
        "        for img_name, label in tqdm(iterator, total=total_items):\n",
        "            if debug_max and count >= debug_max:\n",
        "                break\n",
        "            self._process_and_extract(img_name, label)\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Extraction complete. Total patches: {len(self.samples)}\")\n",
        "\n",
        "    def _process_and_extract(self, img_name, label):\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.img_dir, img_name.replace(\"img_\", \"mask_\"))\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            mask = Image.open(mask_path).convert(\"L\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load {img_name}: {e}\")\n",
        "            return\n",
        "\n",
        "        img_w, img_h = image.size\n",
        "        # Create binary mask for component labeling\n",
        "        mask_arr = np.array(mask) > 0\n",
        "\n",
        "        # Label connected components (blobs) in the mask\n",
        "        labeled_mask, n_components = ndimage.label(mask_arr) # type: ignore\n",
        "\n",
        "        for cid in range(1, n_components + 1):\n",
        "            # Extract coordinates of the current blob\n",
        "            ys, xs = np.where(labeled_mask == cid)\n",
        "\n",
        "            # Filter out very small noise artifacts (< 50 pixels)\n",
        "            if len(xs) < 50:\n",
        "                continue\n",
        "\n",
        "            # Calculate the centroid (center of mass) of the blob\n",
        "            cy, cx = int(np.mean(ys)), int(np.mean(xs))\n",
        "\n",
        "            # Define the fixed-size crop window around the centroid\n",
        "            th, tw = self.target_size\n",
        "            half_h, half_w = th // 2, tw // 2\n",
        "\n",
        "            y1 = cy - half_h\n",
        "            y2 = cy + half_h\n",
        "            x1 = cx - half_w\n",
        "            x2 = cx + half_w\n",
        "\n",
        "            # Handle Edge Cases: Calculate intersection with the actual image\n",
        "            img_y1, img_y2 = max(0, y1), min(img_h, y2)\n",
        "            img_x1, img_x2 = max(0, x1), min(img_w, x2)\n",
        "\n",
        "            # Extract the valid region from the image\n",
        "            patch_crop = image.crop((img_x1, img_y1, img_x2, img_y2))\n",
        "\n",
        "            # Calculate required padding if the crop extended beyond image bounds\n",
        "            pad_left = max(0, -x1)\n",
        "            pad_top = max(0, -y1)\n",
        "            pad_right = max(0, x2 - img_w)\n",
        "            pad_bottom = max(0, y2 - img_h)\n",
        "\n",
        "            # If padding is needed, pad with white (255) which is standard background in histology\n",
        "            if pad_left > 0 or pad_top > 0 or pad_right > 0 or pad_bottom > 0:\n",
        "                patch = ImageOps.expand(\n",
        "                    patch_crop,\n",
        "                    border=(pad_left, pad_top, pad_right, pad_bottom),\n",
        "                    fill=255,\n",
        "                )\n",
        "            else:\n",
        "                patch = patch_crop\n",
        "\n",
        "            # Ensure precise size match (e.g., if rounding errors occurred)\n",
        "            if patch.size != self.target_size:\n",
        "                patch = patch.resize(self.target_size, Image.BICUBIC) # type: ignore\n",
        "\n",
        "            # Store in RAM (Efficient for ~2k images yielding ~10k-20k patches)\n",
        "            self.samples.append(\n",
        "                {\"patch\": np.array(patch), \"label\": label, \"parent\": img_name}\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        img = Image.fromarray(item[\"patch\"])\n",
        "        label = item[\"label\"]\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, label, item[\"parent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "147c9151",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_dataset_stats(dataset_class, dataframe, img_dir):\n",
        "    \"\"\"\n",
        "    Computes channel-wise Mean and Std on the dataset without any normalization applied.\n",
        "    \"\"\"\n",
        "    print(\"Computing dataset Mean and Std (this may take a moment)...\")\n",
        "\n",
        "    # define a simple transform that only converts to tensor\n",
        "    basic_transforms = transforms.Compose(\n",
        "        [transforms.Resize(IMG_RESIZE), transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    # Instantiate dataset temporarily\n",
        "    temp_ds = dataset_class(dataframe, img_dir, transforms=basic_transforms)\n",
        "    loader = make_loader(temp_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    nb_samples = 0.0\n",
        "\n",
        "    for data, _, _ in tqdm(loader):\n",
        "        batch_samples = data.size(0)\n",
        "        # Flatten H and W to calculate stats per channel\n",
        "        data = data.view(batch_samples, data.size(1), -1)\n",
        "        mean += data.mean(2).sum(0)\n",
        "        std += data.std(2).sum(0)\n",
        "        nb_samples += batch_samples\n",
        "\n",
        "    mean /= nb_samples\n",
        "    std /= nb_samples\n",
        "\n",
        "    print(f\"\\nDONE. Copy these values into your config:\")\n",
        "    print(f\"NEW_MEAN = {mean.tolist()}\") # type: ignore\n",
        "    print(f\"NEW_STD = {std.tolist()}\") # type: ignore\n",
        "    return mean.tolist(), std.tolist()  # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "f6cadb5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating stats on Training Data...\n",
            "Computing dataset Mean and Std (this may take a moment)...\n",
            "Processing 581 images to extract fixed-size (224, 224) tiles...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac16e0ffe0d94bdbbf89c75df0a3481d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/581 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 4955\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d3d2e5ec1f44f548b4a79cf7c01a12a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/78 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DONE. Copy these values into your config:\n",
            "NEW_MEAN = [0.6673933267593384, 0.6174469590187073, 0.6541770100593567]\n",
            "NEW_STD = [0.08422686159610748, 0.11849821358919144, 0.0835428312420845]\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating stats on Training Data...\")\n",
        "    \n",
        "# We use the class we just defined\n",
        "custom_mean, custom_std = compute_dataset_stats(\n",
        "        dataset_class=MaskedFixedTileDataset, \n",
        "        dataframe=full_df, \n",
        "        img_dir=train_set_dir\n",
        "    )\n",
        "\n",
        "NORMALIZATION_MEAN = custom_mean\n",
        "NORMALIZATION_STD = custom_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06c1b624",
      "metadata": {},
      "source": [
        "### **Transforms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "1a69ba8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "\n",
        "# ADVICE 3\n",
        "train_transform_tl = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_transform_ft = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(\n",
        "            IMG_RESIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2), antialias=True\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=180),  # type: ignore\n",
        "        transforms.RandomApply(\n",
        "            [transforms.ElasticTransform(alpha=50.0, sigma=5.0)], p=0.25\n",
        "        ),\n",
        "        transforms.RandomApply(\n",
        "            [transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 2.0))], p=0.2\n",
        "        ),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "        transforms.RandomErasing(\n",
        "            p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=\"random\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMG_RESIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=NORMALIZATION_MEAN, std=NORMALIZATION_STD),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f6b750",
      "metadata": {},
      "source": [
        "## ðŸ§® **Network Parameters**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d25c92c",
      "metadata": {},
      "source": [
        "### **Custom Nets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "3f17a8ac",
      "metadata": {
        "id": "3f17a8ac"
      },
      "outputs": [],
      "source": [
        "class EfficientNetCustom(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiates EfficientNet-B0 with ImageNet weights.\n",
        "    Replaces the classifier head with a high-dropout dense layer to prevent overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.EfficientNet_V2_S_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.efficientnet_v2_s(weights=self.weights)\n",
        "        \n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(in_features, self.num_classes),  # type: ignore\n",
        "        )\n",
        "        self.freeze_backbone()\n",
        "    \n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all layers except the classifier head\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not name.startswith(\"classifier\"):\n",
        "                param.requires_grad = False\n",
        "        # Ensure classifier params are trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "        # Unfreeze the last n_layers of the backbone (excluding classifier which is already trainable)\n",
        "        child_counter = 0\n",
        "        for child in reversed(list(self.backbone.children())):\n",
        "            child_counter += 1\n",
        "            if child_counter <= n_layers:\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "701e496f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DenseNetCustom(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.weights = torchvision.models.DenseNet121_Weights.DEFAULT\n",
        "        self.backbone = torchvision.models.densenet121(weights=self.weights)\n",
        "\n",
        "        # DenseNet classifier is stored in .classifier\n",
        "        in_features = self.backbone.classifier.in_features\n",
        "\n",
        "        # Replace Classifier\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate), nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "        self.freeze_backbone()\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Freeze all feature layers\n",
        "        for param in self.backbone.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Unfreeze classifier\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self, n_blocks, all=False):\n",
        "        if all:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = True\n",
        "            return\n",
        "\n",
        "        # Keep classifier trainable\n",
        "        for param in self.backbone.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Unfreeze the last n_blocks within features\n",
        "        children = list(self.backbone.features.children())\n",
        "        total_children = len(children)\n",
        "        if n_blocks <= 0:\n",
        "            return\n",
        "\n",
        "        start = max(0, total_children - n_blocks)\n",
        "        for i in range(start, total_children):\n",
        "            for param in children[i].parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "41f44462",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper that can interchange between DenseNetCustom and EfficientNetCustom.\n",
        "    Keeps the same constructor signature used in the notebook:\n",
        "    CustomNet(num_classes, dropout_rate, backbone=...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dropout_rate=0.4, backbone=\"densenet121\"):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.backbone_name = backbone.lower()\n",
        "\n",
        "        if self.backbone_name in (\"densenet\", \"densenet121\"):\n",
        "            self.backbone = DenseNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        elif self.backbone_name in (\n",
        "            \"efficientnet\",\n",
        "            \"efficientnet_v2s\",\n",
        "            \"efficientnetv2s\",\n",
        "        ):\n",
        "            self.backbone = EfficientNetCustom(\n",
        "                num_classes=num_classes, dropout_rate=dropout_rate\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported backbone '{backbone}'. Use 'densenet' or 'efficientnet'.\"\n",
        "            )\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"freeze_backbone\"):\n",
        "            self.backbone.freeze_backbone()\n",
        "\n",
        "    def unfreeze_backbone(self, n_layers, all=False):\n",
        "        # Delegate to underlying implementation\n",
        "        if hasattr(self.backbone, \"unfreeze_backbone\"):\n",
        "            self.backbone.unfreeze_backbone(n_layers, all=all)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365f9814",
      "metadata": {
        "id": "365f9814"
      },
      "source": [
        "## **XGboost**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "b2e44904",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_k_mean_aggregation(prob_matrix, k_percent=0.3):\n",
        "    \"\"\"\n",
        "    Aggregates patch probabilities into a slide prediction by averaging\n",
        "    only the most confident patches (Top-K%).\n",
        "\n",
        "    Args:\n",
        "        prob_matrix: Numpy array of shape [N_patches, N_classes]\n",
        "        k_percent: Float (0.0 to 1.0). Percentage of patches to keep.\n",
        "                   0.3 means we only average the top 30% scores.\n",
        "    \"\"\"\n",
        "    n_patches = prob_matrix.shape[0]\n",
        "\n",
        "    # Safety check: if slide has very few patches, keep at least 1\n",
        "    k = max(1, int(n_patches * k_percent))\n",
        "\n",
        "    # Sort probabilities for each class INDEPENDENTLY (Axis 0 = patches)\n",
        "    # We want the highest probabilities for Class 0, Class 1, etc.\n",
        "    sorted_probs = np.sort(prob_matrix, axis=0)\n",
        "\n",
        "    # Take the top K (the last K elements in the sorted array)\n",
        "    top_k_probs = sorted_probs[-k:, :]\n",
        "\n",
        "    # Average them\n",
        "    slide_score = np.mean(top_k_probs, axis=0)\n",
        "\n",
        "    return slide_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "e7a663f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features_from_probs(prob_matrix):\n",
        "    \"\"\"\n",
        "    Turns a (N_patches, 4) probability matrix into a single feature vector (1, N_features).\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # 1. Mean Probabilities (Standard Soft Voting) - 4 features\n",
        "    mean_probs = np.mean(prob_matrix, axis=0)\n",
        "    features.extend(mean_probs)\n",
        "\n",
        "    # 2. Max Probabilities (Detect strong tumor signal) - 4 features\n",
        "    max_probs = np.max(prob_matrix, axis=0)\n",
        "    features.extend(max_probs)\n",
        "\n",
        "    # --- NEW INSERTION HERE ---\n",
        "    # 3. Top-K Means (The \"Clean\" Signal)\n",
        "    # Top 30%: Averages the \"surest\" 1/3 of the slide.\n",
        "    # Filters out background but keeps the tumor chunks.\n",
        "    features.extend(top_k_mean_aggregation(prob_matrix, k_percent=0.3))\n",
        "\n",
        "    # Top 10%: Very aggressive. Focuses only on the absolute peak regions.\n",
        "    features.extend(top_k_mean_aggregation(prob_matrix, k_percent=0.1))\n",
        "\n",
        "    # 3. Standard Deviation (Detect tissue heterogeneity) - 4 features\n",
        "    std_probs = np.std(prob_matrix, axis=0)\n",
        "    features.extend(std_probs)\n",
        "\n",
        "    # 4. Percentiles (Robust Max) - 4 features\n",
        "    p90_probs = np.percentile(prob_matrix, 90, axis=0)\n",
        "    features.extend(p90_probs)\n",
        "\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb1386a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_oof_and_train_xgboost(full_df):\n",
        "    print(\">>> Generating Out-Of-Fold (OOF) Predictions for XGBoost...\")\n",
        "\n",
        "    # We must recreate the exact same split\n",
        "    skf = StratifiedKFold(n_splits=K_FOLD, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # Storage for the Stacking Dataset\n",
        "    X_stack = []  # Features\n",
        "    y_stack = []  # True Labels\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(\n",
        "        skf.split(full_df, full_df[\"label_index\"])\n",
        "    ):\n",
        "        print(f\"Processing OOF for Fold {fold+1}/{K_FOLD}...\")\n",
        "\n",
        "        # Get Validation Data for this fold\n",
        "        fold_val_df = full_df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "        # Load the Model trained on the OTHER data\n",
        "        model_path = f\"models/{EXPERIMENT_NAME}_fold{fold+1}_ft_model.pt\"\n",
        "        try:\n",
        "            # Initialize appropriate model architecture (EfficientNet or ConvNeXt)\n",
        "            # Ensure this matches what you trained!\n",
        "            model = CustomNet(num_classes, FT_DROPOUT_RATE, backbone=NET_NAME).to(\n",
        "                device\n",
        "            )\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Skipping Fold {fold+1} (Model not found)\")\n",
        "            continue\n",
        "\n",
        "        # Create Dataset/Loader\n",
        "        val_ds = MaskedFixedTileDataset(\n",
        "            fold_val_df,\n",
        "            train_set_dir,\n",
        "            transforms=data_transforms,\n",
        "            target_size=IMG_RESIZE,\n",
        "        )\n",
        "        val_loader = make_loader(val_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Run Inference\n",
        "        current_probs = {}  # { 'img_name': [ [p0..p3], [p0..p3] ] }\n",
        "        img_labels = {}  # { 'img_name': label_idx }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, parent_ids in tqdm(\n",
        "                val_loader, desc=\"Predicting\", leave=False\n",
        "            ):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # TTA (Optional but recommended for consistency)\n",
        "                out1 = F.softmax(model(inputs), dim=1)\n",
        "                out2 = F.softmax(model(torch.flip(inputs, dims=[3])), dim=1)\n",
        "                probs = (out1 + out2) / 2.0\n",
        "                probs = probs.cpu().numpy()\n",
        "                labels = labels.cpu().numpy()\n",
        "\n",
        "                for i, pid in enumerate(parent_ids):\n",
        "                    if pid not in current_probs:\n",
        "                        current_probs[pid] = []\n",
        "                        img_labels[pid] = labels[i]\n",
        "                    current_probs[pid].append(probs[i])\n",
        "\n",
        "        # Feature Engineering per Slide\n",
        "        for pid, prob_list in current_probs.items():\n",
        "            prob_matrix = np.array(prob_list)\n",
        "\n",
        "            # Extract statistical features\n",
        "            feats = extract_features_from_probs(prob_matrix)\n",
        "\n",
        "            X_stack.append(feats)\n",
        "            y_stack.append(img_labels[pid])\n",
        "\n",
        "    # Convert to Arrays\n",
        "    X_stack = np.array(X_stack)\n",
        "    y_stack = np.array(y_stack)\n",
        "\n",
        "    print(f\"OOF Dataset Created. Shape: {X_stack.shape}\")\n",
        "\n",
        "    # Train XGBoost\n",
        "    print(\">>> Training XGBoost Stacker...\")\n",
        "    # Use multi:softmax for multiclass classification\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective=\"multi:softmax\",\n",
        "        num_class=num_classes,\n",
        "        n_estimators=200,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(X_stack, y_stack)\n",
        "\n",
        "    # Evaluate Stacker on OOF data (Optimistic estimate)\n",
        "    preds = xgb_model.predict(X_stack)\n",
        "    score = f1_score(y_stack, preds, average=\"macro\")\n",
        "    print(f\"XGBoost Stacking OOF F1-Score: {score:.4f}\")\n",
        "\n",
        "    return xgb_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "17a61bf3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Generating Out-Of-Fold (OOF) Predictions for XGBoost...\n",
            "Processing OOF for Fold 1/2...\n",
            "Processing 291 images to extract fixed-size (224, 224) tiles...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1eeb1980c1b4d9db7eabe0e01b697df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/291 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 2421\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc7903b86e90477192d3059d70af6b43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Predicting:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing OOF for Fold 2/2...\n",
            "Skipping Fold 2 (Model not found)\n",
            "OOF Dataset Created. Shape: (291, 24)\n",
            ">>> Training XGBoost Stacker...\n",
            "XGBoost Stacking OOF F1-Score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# EXECUTE\n",
        "xgb_stacker = generate_oof_and_train_xgboost(full_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4578c00c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_xgboost_submission(\n",
        "    test_dir, cnn_exp_name, xgb_model, output_file=\"submission_xgb.csv\"\n",
        "):\n",
        "    print(\">>> Running Inference with XGBoost Stacking...\")\n",
        "\n",
        "    # 1. Setup Data\n",
        "    test_ds = MaskedFixedTileDataset(\n",
        "        None, test_dir, transforms=data_transforms, target_size=IMG_RESIZE\n",
        "    )\n",
        "    test_loader = make_loader(test_ds, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Storage for FINAL XGBoost predictions per slide\n",
        "    # { 'slide_id': [xgb_prob_fold1, xgb_prob_fold2, ...] }\n",
        "    slide_final_preds = {}\n",
        "\n",
        "    # 2. Iterate Folds: Run CNN -> Extract Features -> Predict XGB -> Store\n",
        "    for fold in range(1, K_FOLD + 1):\n",
        "        model_path = f\"models/{cnn_exp_name}_fold{fold}_ft_model.pt\"\n",
        "        print(f\"\\n--- Processing Fold {fold}/{K_FOLD} ---\")\n",
        "\n",
        "        # Load CNN\n",
        "        try:\n",
        "            model = CustomNet(num_classes, FT_DROPOUT_RATE, backbone=NET_NAME).to(\n",
        "                device\n",
        "            )\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            model.eval()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Skipping Fold {fold} (Model not found)\")\n",
        "            continue\n",
        "\n",
        "        # Dictionary to collect patches for this SPECIFIC fold\n",
        "        current_fold_probs = {}  # { 'pid': [ [p0..p3], [p0..p3] ] }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, _, parent_ids in tqdm(\n",
        "                test_loader, desc=f\"Inference Fold {fold}\", leave=False\n",
        "            ):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # TTA\n",
        "                out1 = F.softmax(model(inputs), dim=1)\n",
        "                out2 = F.softmax(\n",
        "                    model(torch.flip(inputs, [3])), dim=1\n",
        "                )  # Horizontal Flip\n",
        "                out3 = F.softmax(model(torch.flip(inputs, [2])), dim=1)  # Vertical Flip\n",
        "                probs = (out1 + out2 + out3) / 3.0\n",
        "                probs = probs.cpu().numpy()\n",
        "\n",
        "                # Collect patches for this fold\n",
        "                for i, pid in enumerate(parent_ids):\n",
        "                    if pid not in current_fold_probs:\n",
        "                        current_fold_probs[pid] = []\n",
        "                    current_fold_probs[pid].append(probs[i])\n",
        "\n",
        "        # 3. Feature Engineer & Predict with XGBoost for THIS fold\n",
        "        for pid, prob_list in current_fold_probs.items():\n",
        "            prob_matrix = np.array(prob_list)\n",
        "\n",
        "            # Extract features (stats) from this fold's CNN predictions\n",
        "            feats = extract_features_from_probs(prob_matrix)\n",
        "\n",
        "            # Predict using XGBoost\n",
        "            # We use predict_proba to get soft voting capability for the stacker\n",
        "            # Shape: (1, n_classes)\n",
        "            xgb_prob = xgb_model.predict_proba(feats.reshape(1, -1))[0]\n",
        "\n",
        "            if pid not in slide_final_preds:\n",
        "                slide_final_preds[pid] = []\n",
        "            slide_final_preds[pid].append(xgb_prob)\n",
        "\n",
        "    # 4. Average XGBoost Predictions (Soft Voting of Stackers)\n",
        "    final_rows = []\n",
        "    print(\"\\nAggregating Ensemble Predictions...\")\n",
        "\n",
        "    for pid, preds_list in slide_final_preds.items():\n",
        "        # preds_list is a list of arrays (one per fold)\n",
        "        # Average them\n",
        "        avg_xgb_probs = np.mean(preds_list, axis=0)\n",
        "\n",
        "        # Final Argmax\n",
        "        pred_idx = np.argmax(avg_xgb_probs)\n",
        "        pred_label = class_names[pred_idx]\n",
        "\n",
        "        final_rows.append({\"sample_index\": pid, \"label\": pred_label})\n",
        "\n",
        "    # Save\n",
        "    sub = pd.DataFrame(final_rows).sort_values(\"sample_index\")\n",
        "\n",
        "    return sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "d6163117",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Running Inference with XGBoost Stacking...\n",
            "Processing 477 images to extract fixed-size (224, 224) tiles...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2178766b0164c499d3b1f08156dfeba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/477 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction complete. Total patches: 3886\n",
            "\n",
            "--- Processing Fold 1/2 ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3c18c0e52ee44a486664c651c860552",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Inference Fold 1:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing Fold 2/2 ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76e8f0d3e54e498e9dab2dd1f59bf849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Inference Fold 2:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Aggregating Ensemble Predictions...\n",
            "âœ… Saved Robust Stacking Submission: submission/efficientNetV2_S_fra_submission_xgb.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "sample_index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "label",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "706a7d9d-8741-48db-aec7-f9a7a76fedf7",
              "rows": [
                [
                  "0",
                  "img_0000.png",
                  "Luminal A"
                ],
                [
                  "1",
                  "img_0001.png",
                  "Luminal A"
                ],
                [
                  "2",
                  "img_0002.png",
                  "Luminal A"
                ],
                [
                  "3",
                  "img_0003.png",
                  "Luminal B"
                ],
                [
                  "4",
                  "img_0004.png",
                  "Luminal B"
                ],
                [
                  "5",
                  "img_0005.png",
                  "Triple negative"
                ],
                [
                  "6",
                  "img_0006.png",
                  "Luminal A"
                ],
                [
                  "7",
                  "img_0007.png",
                  "Luminal A"
                ],
                [
                  "8",
                  "img_0008.png",
                  "Luminal A"
                ],
                [
                  "9",
                  "img_0009.png",
                  "Luminal A"
                ],
                [
                  "10",
                  "img_0010.png",
                  "Luminal B"
                ],
                [
                  "11",
                  "img_0011.png",
                  "Luminal A"
                ],
                [
                  "12",
                  "img_0012.png",
                  "Luminal B"
                ],
                [
                  "13",
                  "img_0013.png",
                  "Luminal A"
                ],
                [
                  "14",
                  "img_0014.png",
                  "Luminal B"
                ],
                [
                  "15",
                  "img_0015.png",
                  "HER2(+)"
                ],
                [
                  "16",
                  "img_0016.png",
                  "Luminal A"
                ],
                [
                  "17",
                  "img_0017.png",
                  "Luminal B"
                ],
                [
                  "18",
                  "img_0018.png",
                  "Luminal A"
                ],
                [
                  "19",
                  "img_0019.png",
                  "Luminal A"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 20
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_index</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>img_0000.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>img_0001.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>img_0002.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>img_0003.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>img_0004.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>img_0005.png</td>\n",
              "      <td>Triple negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>img_0006.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>img_0007.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>img_0008.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>img_0009.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>img_0010.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>img_0011.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>img_0012.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>img_0013.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>img_0014.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>img_0015.png</td>\n",
              "      <td>HER2(+)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>img_0016.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>img_0017.png</td>\n",
              "      <td>Luminal B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>img_0018.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>img_0019.png</td>\n",
              "      <td>Luminal A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sample_index            label\n",
              "0   img_0000.png        Luminal A\n",
              "1   img_0001.png        Luminal A\n",
              "2   img_0002.png        Luminal A\n",
              "3   img_0003.png        Luminal B\n",
              "4   img_0004.png        Luminal B\n",
              "5   img_0005.png  Triple negative\n",
              "6   img_0006.png        Luminal A\n",
              "7   img_0007.png        Luminal A\n",
              "8   img_0008.png        Luminal A\n",
              "9   img_0009.png        Luminal A\n",
              "10  img_0010.png        Luminal B\n",
              "11  img_0011.png        Luminal A\n",
              "12  img_0012.png        Luminal B\n",
              "13  img_0013.png        Luminal A\n",
              "14  img_0014.png        Luminal B\n",
              "15  img_0015.png          HER2(+)\n",
              "16  img_0016.png        Luminal A\n",
              "17  img_0017.png        Luminal B\n",
              "18  img_0018.png        Luminal A\n",
              "19  img_0019.png        Luminal A"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# EXECUTE\n",
        "submission_df = generate_xgboost_submission(test_dir=test_set_dir, cnn_exp_name=EXPERIMENT_NAME, xgb_model=xgb_stacker, output_file=f\"{EXPERIMENT_NAME}_submission_xgb.csv\")\n",
        "\n",
        "submission_df.to_csv(f\"submission/{EXPERIMENT_NAME}_submission_xgb.csv\", index=False)\n",
        "print(f\"âœ… Saved Robust Stacking Submission: submission/{EXPERIMENT_NAME}_submission_xgb.csv\")\n",
        "\n",
        "submission_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890c1349",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
